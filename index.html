<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="utf-8" />
  <link rel="stylesheet" href="tablestyle.css">
  <title>Web of Things (WoT): Use Cases and Requirements</title>
  <script class="remove" src="https://www.w3.org/Tools/respec/respec-w3c"></script>
  <script class="remove">
    var respecConfig = {
      lint: {
        "no-headingless-sections": false,
      },
      specStatus: "ED",
      noRecTrack: "true",
      maxTocLevel: 6,
      processVersion: 2019,
      shortName: "wot-use-cases-requirements",
      copyrightStart: 2020,
      group: "ig/wot",
      edDraftURI: "https://w3c.github.io/wot-usecases/",
      githubAPI: "https://api.github.com/repos/w3c/wot-usecases/",
      issueBase: "https://www.github.com/w3c/wot-usecases/issues",
      editors: [{
        name: "Michael Lagally",
        w3cid: "47166",
        company: "Oracle Corp.",
        companyURL: "https://www.oracle.com/"
      },
      {
        name: "Michael McCool",
        w3cid: "93137",
        company: "Intel Corp.",
        companyURL: "https://www.intel.com/"
      },
      {
        name: "Ryuichi Matsukura",
        w3cid: "64284",
        company: "Fujitsu Ltd.",
        companyURL: "https://www.fujitsu.com/"
      },
      {
        name: "Tomoaki Mizushima",
        w3cid: "98915",
        company: "Internet Research Institute, Inc.",
        companyURL: "https://www.iri.co.jp/"
      }
      ],

      otherLinks: [
        {
          key: "Contributors",
          data: [{
            value: "In the GitHub repository",
            href: "https://github.com/w3c/wot-usecases/graphs/contributors"
          }]
        }, {
          key: "Repository",
          data: [{
            value: "We are on GitHub",
            href: "https://github.com/w3c/wot-usecases/"
          }, {
            value: "File a bug",
            href: "https://github.com/w3c/wot-usecases/issues"
          }, {
            value: "Contribute",
            href: "https://github.com/w3c/wot-usecases/pulls"
          }]
        }],
      localBiblio: {
        "JSON-SCHEMA": {
          title: "JSON Schema Validation: A Vocabulary for Structural Validation of JSON",
          href: "https://tools.ietf.org/html/draft-handrews-json-schema-validation-01",
          authors: ["Austin Wright", "Henry Andrews", "Geraint Luff"],
          status: "Internet-Draft",
          date: "19 March 2018",
          publisher: "IETF"
        },
        "ISO-6709": {
          title: "ISO-6709:2008 : Standard representation of geographic point location by coordinates",
          href: "https://www.iso.org/standard/39242.html",
          status: "Published",
          date: "2008-07",
          publisher: "ISO"
        },
        "Hybridcast": {
          title: "Hybridcast",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "..",
          publisher: "..."
        },
        "NMEA": {
          title: "National Marine Electronics Association",
          href: "https://www.nmea.org",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "WGS84": {
          title: "WGS84",
          href: "https://en.wikipedia.org/wiki/World_Geodetic_System",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "Basic Geo Vocabulary": {
          title: "W3C Semantic Web Interest Group",
          href: "https://www.w3.org/2003/01/geo/",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "W3C Geolocalization API": {
          title: "Geolocation API Specification 2nd Edition",
          href: "https://www.w3.org/TR/geolocation-API/",
          authors: ["Andrei Popescu"],
          status: "Published",
          date: "8 Nov 2016",
          publisher: "W3C"
        },
        "Open Geospatial Consortium": {
          title: "Open Geospatial Consortium",
          href: "http://docs.opengeospatial.org/as/18-005r4/18-005r4.html",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "ISO19111": {
          title: "ISO19111",
          href: "https://www.iso.org/standard/74039.html",
          authors: ["..."],
          status: "Published",
          date: "Jan 2019",
          publisher: "ISO"
        },
        "SSN": {
          title: "Semantic Sensor Network Ontology",
          href: "https://www.w3.org/TR/vocab-ssn/",
          authors: [
            "Armin Haller",
            "Krzysztof Janowicz",
            "Simon Cox",
            "Danh Le Phuoc",
            "Kerry Taylor",
            "Maxime Lefran√ßois"
          ],
          status: "Published",
          date: "19 Oct 2017",
          publisher: "W3C"
        },
        "Timestamps": {
          title: "Timestamps",
          href: "https://w3c.github.io/hr-time/#dom-domhighrestimestamp",
          authors: ["Ilya Grigorik"],
          status: "...",
          date: "06 Oct 2020",
          publisher: "W3C"
        },
        "MMI UC3.1": {
          title: "MMI UC3.1",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MMI UC3.2": {
          title: "MMI UC3.2",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "ICE F2761-09(2013)": {
          title: "ICE F2761-09(2013)",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "OpenICE": {
          title: "OpenICE",
          href: "https://www.openice.info",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MDIRA": {
          title: "MDIRA",
          href: "https://secwww.jhuapl.edu/mdira/documents",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MQTT": {
          title: "MQTT Version 3.1.1 Plus Errata 01",
          href: "http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html",
          authors: ["Andrew Banks", "Rahul Gupta"],
          status: "...",
          date: "December 2015",
          publisher: "OASIS Standard"
        },
        "OPC UA": {
          title: "OPC Unified Architecture",
          href: "https://opcfoundation.org/about/opc-technologies/opc-ua/",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "OPC"
        },
        "BACnet": {
          title: "BACnet",
          href: "http://www.bacnet.org",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "CoAP": {
          title: "The Constrained Application Protocol (CoAP)",
          href: "https://tools.ietf.org/html/rfc7252",
          authors: [
            "Z. Shelby",
            "K. Hartke",
            "C. Bormann"
          ],
          status: "Published",
          date: "June 2014",
          publisher: "IETF"
        },
        "MMI UC5.1": {
          title: "MMI UC5.1",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MMI UC5.2": {
          title: "MMI UC5.2",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        ".MMI UC1.1": {
          title: "MMI UC1.1",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MMI UC1.2": {
          title: "MMI UC1.2",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "MMI UC2.1": {
          title: "MMI UC2.1",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "IEC 61850": {
          title: "IEC 61850",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "IEEE 1574": {
          title: "IEEE 1574",
          href: "...",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "KNX": {
          title: "KNX",
          href: "https://www.knx.org/knx-en/for-professionals/index.php",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "Modbus": {
          title: "Modbus",
          href: "https://modbus.org",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "OGC Sensor Things": {
          title: "OGC Sensor Things API",
          href: "https://www.ogc.org/standards/sensorthings",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "OneM2M": {
          title: "OneM2M",
          href: "https://www.onem2m.org",
          authors: ["..."],
          status: "...",
          date: "...",
          publisher: "..."
        },
        "LWM2M": {
          title: "Lightweight Machine to Machine Technical Specification: Core",
          href: "http://openmobilealliance.org/release/LightweightM2M/V1_1-20180710-A/OMA-TS-LightweightM2M_Core-V1_1-20180710-A.pdf",
          authors: ["..."],
          status: "...",
          date: "Aug 2018",
          publisher: "OMA SpecWorks."
        },
        "OCF": {
          title: "OCF Core Specification",
          href: "https://openconnectivity.org/developer/specifications",
          authors: ["..."],
          status: "...",
          date: "April 2019",
          publisher: "Open Connectivity Foundation"
        },
      }
    };
  </script>
</head>

<body>
  <section id="abstract">
    <p>
      The Web of Things is applicable to multiple IoT domains,
      including Smart Home, Industrial, Smart City, Retail, and Health
      applications, where usage of the W3C WoT standards can simplify the
      development of IoT systems that combine devices from multiple vendors
      and ecosystems.
      During the last charter period of the WoT Working Group several
      specifications were developed to address requirements for
      these domains.
    </p>
    <p>
      This Use Case and Requirements Document is created
      to collect new IoT use cases from various domains
      that have been contributed by various stakeholders.
      These serve as a baseline for identifying requirements
      for the standardisation work in the W3C WoT groups.
    </p>
  </section>
  <section id="sotd">
  </section>
  <section id="intro">
    <h2>Introduction</h2>
    <p>
      The World Wide Web Consortium (W3C) has published the Web of Things
      (WoT) Architecture and Web of Things (WoT) Thing Description (TD) as
      official W3C Recommendations in May 2020. These specifications enable
      easy integration across Internet of Things platforms and applications.
    </p>
    <p>The W3C Web of Thing Architecture [[wot-architecture]] defines an
      abstract architecture, the WoT Thing Description [[wot-thing-description]]
      defines a format to describes a broad spectrum of very different devices,
      which may be connected over various protocols.
    </p>
    <p>
      During the inception phase of the WoT 1.0 specifications in 2017-2018
      the WoT IG collected use cases and requirements to enable
      interoperability of Internet of Things (IoT)
      services on a worldwide basis.
      Thes released specifications have been created to address the
      use cases and requirements for the first version of the WoT specifications,
      which are documented in <a href="https://w3c.github.io/wot/ucr-doc/">
        https://w3c.github.io/wot/ucr-doc/</a>

    <p>
      The present document gathers and describes new use cases and requirements
      for future standardisation work in the WoT standard.
    </p>


    <section id="document-status" class="ednote" Disclaimer:>
      <p> This document is an early work in progress and is currently under significant editorial and content rework.
        It is currently an aggregation of use case descriptions that were contributed in a different
        file format (Markdown)</p>
      <p>Before it can be published as a IG note,
        it will undergo major restructuring and cleanup in due course.
    </section>


    <section id="structure" class="ednote">
      TODO: Describe document structure, template headings, ...
    </section>

    <section id="domains" class="ednote">
      TODO: Give a domain overview, explain different horizontals and verticals.
    </section>

  </section>

  <section id="conformance"></section>

  <section id="definitions">
    <h2>Definitions: Terminology, Stakeholders and Actors</h2>
    <section id="terminology">
      <h2>Terminology</h2>
      The present document uses the terminology as the WoT Architecture [[wot-architecture]].
      <section id="new-terminology" class="ednote">TODO: Define additional terminology.</section>
    </section>

    <section id="stakeholders">
      <h2>Stakeholders</h2>
      <section id="stakeholders" class="ednote">TODO: Describe stakeholder roles and tasks.
      </section>
      <ul>
        <li>device owners</li>
        <li>device user</li>
        <li>cloud provider</li>
        <li>service provider</li>
        <li>device manufacturer</li>
        <li>gateway manufacturer</li>
        <li>identity provider</li>
        <li>directory service operator?</li>
      </ul>
    </section>

  </section>
  <section id="use-cases">
    <h2>Application Domains and Use Cases</h2>

    <!-- Categories of new use cases -->
    <section id="retail">
      <h2>Retail</h2>
      <dl>

        <dt>Submitter(s)</dt>
        <dd>

          David Ezell, Michael Lagally, Michael McCool

        </dd>
        <dt>Reviewer(s)</dt>
        <dd>


        </dd>
        <dt>Tracker Issue ID</dt>
        <dd>


        </dd>
        <dt>Category</dt>
        <dd>



        </dd>
        <dt>Class</dt>
        <dd>



        </dd>
        <dt>Status</dt>
        <dd>



        </dd>
        <dt>Target Users</dt>
        <dd>

          Retailers, customers, suppliers.

        </dd>
        <dt>Motivation</dt>
        <dd>

          Integrating and interconnecting multiple devices into the common retail workflow
          (i.e., transaction log) drastically improves retail business operations at multiple levels.
          It brings operational visibility,including consumer behavior and environmental information,
          that was not previously possible or viable in a meaningful way.

          It drastically speeds up the process of root cause analysis of operational issues and
          simplifies the work of retailers.

        </dd>
        <dt>Expected Devices</dt>
        <dd>

          Connected sensors, such as people counters, presence sensors, air quality, room ocupancy, door sensors.
          Cloud services. Video analytics edge services.

        </dd>
        <dt>Expected Data</dt>
        <dd>

          Inventory data, supply chain status information, discrete sensor data or data streams.

        </dd>
        <dt>Dependencies</dt>
        <dd>


          tbd

        </dd>
        <dt>Description</dt>
        <dd>

          Falling costs of sensors, communications, and handling of very large volumes of data combined with cloud
          computing enable retail business operations with increased operational efficiency, better customer
          service,
          and even increased revenue growth and return on investment.

          Accurate forecasts allow retailers to coordinate demand-driven outcomes that deliver connected customer
          interactions.
          They drive optimal strategies in planning, increasing inventory productivity in retail supply chains,
          decreasing operational costs and driving customer satisfaction from engagement, to sale, to fulfilment.

          Understanding of store activity juxtaposed with traditional information streams can boost worker and
          consumer safety,
          comply better with work safety regulations, enhance system and site security, and improve worker
          efficiency
          by providing real-time visibility into worker status, location, and work environment.

        </dd>
        <dt>Variants</dt>
        <dd>

          <ul>
            <li>Use edge computing, in particular video analytics, in combination with IoT devices to deliver an
              enhanced
              customer experience, better manage inventory, or otherwise improve the store workflow.</li>

        </dd>
        <dt>Security Considerations</dt>
        <dd>

          <ul>
            <li>In retail, replay attacks can cause monetary loss, customers may be incorrectly charged or
              over-charged.</li>
            <li>To avoid replay attacks, "Things" should implement a sequence number for each message and digital
              signature.</li>
            <li>Servers ("Things" or "Cloud") should verify the signature and disallow for duplicated messages.</li>
            <li>For "Things" relying on electronic payments, "Things" must comply with PCI-DSS requirements.</li>
            <li>"Things" must never store credit card information.</li>
            <li>Customer satisfaction and trust depends on availability, so attacks such as Denial-of-Service (DoS)
              need to be prevented or mitigated.</li>
            <li>To prevent DoS, implement "Things" with early DoS detection.</li>
            <li>Have an automated DoS system that will notify the controlling unit of the problem.</li>
            <li>Implement IP white list, that could be part of the DoS early detection system.</li>
            <li>Make sure your network perimeter is defended with up to date firewall software.</li>
          </ul>

        </dd>
        <dt>Privacy Considerations</dt>
        <dd>

          As a general rule, personal consumer information should not be stored.
          That is especially true in the retail industry where a security breach could cause financial, reputation,
          and brand damage.
          If personal or information that can identify a consumer is to be stored,
          it should be to conduct business and with the explicit acknowledgment of the consumer.
          WoT vendors and integrators should always have a privacy policy and make it easily available.
          By default, devices should adopt an opt-out policy.
          That means, unless the consumer explicitly allowed for the data capture and storage, avoid doing it.

        </dd>
        <dt>Gaps</dt>
        <dd>
          <section id="todo-gaps-x" class="ednote">
            TODO: Describe any gaps that are not addressed in the current WoT work
            items.
          </section>

        </dd>
        <dt>Existing standards</dt>
        <dd>

          <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that
            are relevant for this use case</section>
    </section>

    <section id="audio_video">
      <h2>Audio/Video</h2>
      <section id="media-information-references">
        <h2>Media Use Case Information Bucket</h2>
        <dl>

          This section is not a full use case description. It is rather a collection of
          thoughts and ideas to capture information and provide references and have a common discussion basis.

          The intention is to trigger new ideas and collect them in a single document at this point.

          The document is work in progress.

          <dt>Submitter(s)</dt>
          <dd>



          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>



          </dd>
          <dt>Motivation</dt>
          <dd>


          </dd>
          <dt>Expected Devices</dt>
          <dd>


          </dd>
          <dt>Expected Data</dt>
          <dd>


          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>


          </dd>
          <dt>Description</dt>
          <dd>

            <ul>
              <li>Sync of media across different devices:</li>
              <li>people in different homes watch the same content at the same time. Conversation about content.</li>
              <li>Multi-room sync playback</li>
              <li>Multi-camera angles </li>
              <li>Voice control of a media playback device (integration of smart speakers from multiple vendors)
                Describe proprietary (vendor specific) device control interfaces to control media playback on TV set.
                (proprietary implementations exist, open protocol is proposed?)</li>
            </ul>
          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>

            <section id="todo-gaps-1" class="ednote">
              TODO: Provide links to relevant standards that are relevant for this use case</section>

          </dd>
          <dt>Existing standards & related information</dt>
          <dd>

            <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that are
              relevant for this use case.
              <p>There are MANY TV standards and this would be a long list. Rather leave blank unless a very specific
                standard provides more insight.</p>
            </section>


          </dd>
          <dt>Comments</dt>
          <dd>

            Further information and resources:

            <ul>
              <li><a
                  href="https://www.w3.org/2011/webtv/wiki/images/d/d1/MediaTimedEventsInHybridcast_TPAC20190916.pdf">NHK
                  Hybridcast updates</a></li>
              <li><a
                  href="https://www.w3.org/2011/webtv/wiki/images/d/d1/MediaTimedEventsInHybridcast_TPAC20190916.pdf">MediaTimedEvents
                  in Hybridcast</a></li>
              <li><a href="https://2immerse.eu/motogp-at-home/">BC Moto GP at Home</a></li>
            </ul>
          </dd>
        </dl>
      </section>


      <section id="nhk-device-tv-sync">
        <h2>Home WoT devices work according to TV programs</h2>
        <dl>


          <dt>Submitter(s)</dt>
          <dd>

            Hiroki Endo,
            Masaya Ikeo,
            Shinya Abe,
            Hiroshi Fujisawa


          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>


          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            Person watching TV, Broadcasters

          </dd>
          <dt>Motivation</dt>
          <dd>

            A lot of home devices, such as TV, cleaner, and home lighting, connect to an IP network.
            When you watch a content program, these devices should coorperate for enhancing your expereence.

            If the cleaning robot makes a loud noise while watching the TV program, it will hinder viewing.
            Also, even if you set up the theater environment with smart lights, it is troublesome to operate it
            yourself each time the TV program switches.

            Therefore, by WoT device to operate in accordance with the TV program being viewed, thereby improving the
            user experience.

            WoT devices work according to TV programs:

            <ul>
              <li>Cleaning robot stops at an important situation,</li>
              <li>Color of smart lights are changed according to TV programs,</li>
              <li>Smart Mirror is notified that favorite TV show will start.</li>
            </ul>

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>Hybridcast TV</li>
              <li>Hybridcast Connect application (in a smartdevice such as smartphone)</li>
              <li>Cleaning Robot</li>
              <li>Smart Light (such as Philips Hue)</li>
              <li>Smart Mirror</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            The trigger value of the scene of the TV program.
            Hybridcast connect application know the Thing Description of the devices in home. (Discovery?)

          </dd>
          <dt>Dependencies</dt>
          <dd>



          </dd>
          <dt>Description</dt>
          <dd>

            Home smart devices behave according to TV programs.

            Hybridcast applications in TV emit information about TV programs for smart home devices.
            (Hybridcast is a Japanese Integrated Broadcast-Broadband system. Hybridcast applications are HTML5
            applications that work on Hybridcast TV.)

            Hybridcast Contact application receives the information and controlls smart home devices.
            <br>
            <br>
            <img src="images/scenario_nhk.png" width="100%" height=100%>
            <br>
          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>


          </dd>
          <dt>Existing standards</dt>
          <dd>

            Hybridcast and Hybridcast Connect: a Japanese Integrated Broadcast-Broadband system (<a
              href="http://www.iptvforum.jp/download/input.html">IPTVFJ STD-0013 "Hybridcast Operational Guideline
              Version 2.8" (Application Forms)</a>, <a href="https://github.com/nhkrd">Reference Implementations</a>),
            HbbTV,
            ATSC 3.0,
            ...etc.

          </dd>
          <dt>Comments</dt>
          <dd>


          </dd>
        </dl>
      </section>

    </section>

    <section id="agriculture">
      <h2>Agriculture</h2>
      <section id="smart-agriculture">
        <h2>Smart Agriculture (Greenhouse Horticulture)</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Ryuichi Matsukura, Takuki Kamiya

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

          </dd>
          <dt>Class</dt>
          <dd>

          </dd>
          <dt>Status</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>

            Agricultural corporation, Farmer, Manufacturers (Sensor, other facilities), Cloud provider

          </dd>
          <dt>Motivation</dt>
          <dd>

            Greenhouse Horticulture controlled by computers can create an optimal environment for growing plants. This
            enables to improve productivity and ensure stable vegetable production throughout the year, independent of
            the weather. This is the result of research on the growth of plants in the 1980s. For example, in
            tomatoes, switching to hydroponics and optimizing the temperature, humidity and CO2 concentration required
            for photosynthesis resulted in a five times increase in yield. The growth conditions for other vegetables
            also have been investigated, and this control system is applied now.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Sensors ( temperature, humidity, brightness, UV brightness, air pressure, and CO2)
            Heating, CO2 generator, open and close sunlight shielding sheet.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Sensors‚Äô values to clarify the gaps between conditions for maximizing photosynthesis and the current
            environment.
            Following sensors values at one or some points in the greenhouse: temperature, humidity, brightness, and
            CO2.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            WoT Architecture„ÄÅWoT Thing Description

          </dd>
          <dt>Description</dt>
          <dd>

            Sensors and some facilities like heater, CO2 generator, sheet controller are connected to the gateway via
            wired or wireless networks. The gateway is connected to the cloud via the Internet. All sensors and
            facilities can be accessed and controlled from the cloud.
            To maximize photosynthesis, the temperature, CO2 concentration, and humidity in the greenhouse are mainly
            controlled. When the sunlight comes in the morning and CO2 concentration inside decreases, the application
            turns on the CO2 generator to keep over 400 ppm, the same as the air outside. The temperature in the
            greenhouse is adjusted by controlling the heater and the sunlight shielding sheet.
            The cloud gathers all sensor data and the status of the facilities. The application makes the best
            configuration for the region of the greenhouse located.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>

            In the case of the wireless connection to the sensors, the gateway should keep the latest value of the
            sensors since the wireless connection is sometimes broken. The gateway can create a virtual entity
            corresponding to the sensor and allow the application to access this virtual entity having the actual
            sensor status like sleeping.

          </dd>
          <dt>Existing standards</dt>
          <dd>

          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>
      <section id="smart-agriculture-openfield">
        <h2>Smart Agriculture: open-field agriculture</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Cristiano Aguzzi

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

            Smart Agriculture

          </dd>
          <dt>Class</dt>
          <dd>
            Open field agriculture

          </dd>
          <dt>Status: WIP</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>

            Agricultural corporation, Farmer, Manufacturers (Sensor, other facilities), Cloud provider, Middleware
            provider, Network providers, service provider.

          </dd>
          <dt>Motivation</dt>
          <dd>
            Water is vital for ensuring food security to the world‚Äôs population, and agriculture is the biggest consumer
            amounting for 70% of freshwater. Field irrigation application methods are one of the main causes of water
            wastage. The most common technique, surface irrigation, wastes a high percentage of the water by wetting
            areas where no plants benefit from it. On the other hand, localized irrigation can use water more
            efficiently and effectively, avoiding both under-irrigation and over-irrigation. However, in an attempt to
            avoid under-irrigation, farmers feed more water than is needed resulting not only to productivity losses,
            but also water wastages.

            Therefore, technology should be developed and deployed for sensing water needs and automatically manage
            water supply to crops. However, open field agriculture is characterized by a quite dynamic range of
            requirements. Usually, solutions developed for one particular crop type cannot be reused in other
            cultivations. Moreover, the same field can have different crop types or different sizes/shapes during the
            years, meaning that technology to monitor the state of crop growth should be highly configurable and
            adaptive. Even agriculture and irrigation methods can change and also they are very different depending on
            the size of the field and its clime type.

            Consequently, silos applications are deployed leveraging on IoT technologies to gather data about the crop
            growth state and irrigation needs. The Web of Things may help to create a single platform where
            cost-effective applications could adapt seamlessly between different scenarios, breaking the silos and
            giving value both to the environment and the market.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <p>Sensors:</p>
            <ul>
              <li>Weather sensors (maybe collected together inside a <a
                  href="https://en.wikipedia.org/wiki/Weather_station)">weather station</a>)</li>
              <ul>
                <li>temperature</li>
                <li>air humidity</li>
                <li>air pressure</li>
                <li>pluviometer</li>
                <li>global solar radiation</li>
                <li>anemometer (wind speed)</li>
                <li> wind direction</li>
                <li>global solar radiation and photosynthetically active radiation</li>
                <li> gas/air quality sensor (i.e. CO2)</li>
              </ul>
              <li>Soil sensors (usually packed together in soil probes)</li>
              <ul>
                <li>soil temperature</li>
                <li>soil moisture/water content</li>
                <li>soil conductivity (detecting salt levels in the soil)</li>
                <li> water table sensor</li>
              </ul>
              <li>Drone sensors</li>
              <ul>
                <li>camera</li>
                <li>temperature sensitive camera</li>
                <li>multispectral camera</li>
              </ul>
            </ul>
            <p>Actuators:</p>
            <ul>
              <li>drones: used for data collection or pesticed/impollination</li>
              <li>sprinklers</li>
              <li>pumps</li>
              <li>central pivot sprinklers</li>
              <li>hose-reel irrigation machine</li>
            </ul>
            <p>Additional devices:</p>
            <ul>
              <li>Solar panels</li>
              <li>Loggers: units that collect data from close sensors. </li>
              <li>Gateways</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>
            Sensor data plays a central role in Smart Agriculture. In particular, it is critical that the information
            sensed is associated with a timestamp. Common algorithms use *time series* to calculate the water needs of a
            crop.

            Furthermore, soil sensors usually are calibrated over a specific soil type (which may differ even in the
            same geographic region). For example, the calibration data for a soil moisture sensor is represented by a
            function that maps sensor output to soil water content. In literature, this function is knowns as a
            *calibration curve*. Commercial sensors are precalibrated with a "standard" curve but on most occasions, it
            fails to accurately measure the water content. Therefore, it can be configured during the installation phase
            (which may happen every time the soil is plowed).

            Finally, a crucial aspect is forecasting. Farmers use this information to actively change their management
            procedures. Services exploit it to suggest irrigation schedule or change device settings to behave
            accordingly to environmental changes.

            To summarize here it is a list of most important expected data from Open field agriculture:

            <ul>
              <li>Calibration curve</li>
              <li>Time series</li>
              <li>Forecast data</li>
              <li>Geolocations: sensor data must be contextualized in geolocation. Also, geolocation is critical in
                massive open fields to localize instrument position.</li>
              <li>Weather data</li>
              <li>Unit of measure: commercial soli sensor may output their value in a different unit of measures (i.e.
                volts or % water in an m^3 of soil)</li>
              <li>Relative values</li>
              <li>Depth position: geolocation is not sufficient to describe the parameters of the soil. Depth is an
                additional context that should be added to an observed value. </li>
              <li>Device owner information</li>
              <li>Battery level and energy consumption</li>
            </ul>

          </dd>
          <dt>Dependencies</dt>
          <dd>

            WoT Architecture, WoT Thing Description

          </dd>
          <dt>Description</dt>
          <dd>
            In open-field agriculture, the IoT solutions leverage on different radio protocols and devices. Usually,
            radio protocols should cover long distances (even kilometers) and be energy efficient. Devices too need to
            be energy saving as they are deployed for months and sometimes even years in harsh environments. A
            sleeping-cycle is one mechanism they use to save energy usually coordinated by *loggers/gateways* or
            preprogrammed. *Loggers* are deployed closed to sensor devices and have more storage space. They serve as
            buffers between sensors and higher services. Often *loggers* and sensors are embedded in the same board,
            otherwise, they are connected using cables or close-ranged radio protocols. On the other hand, *gateways*
            serve as a collection point for data of an entire field or farm. They are much more capable devices and
            usually are more energy-consuming. In some deployment scenarios, they host a full operating system with
            multiple software facilities installed. Otherwise, gateways only serve as relays of data sent from the
            loggers and sensors to cloud services and vice-versa. The cloud services may be partially hosted in edge
            servers to preserve data privacy and responsiveness of the whole IoT solution. Possible cloud services are:
            <ul>
              <li>Weather forecasting/local weather forecasting</li>
              <li>Soil digital twin to simulate and predict water content</li>
              <li>Plant digital twin (growth and water needs prediction)</li>
              <li>Irrigation advice service: combining the previous services and knowing the irrigation system topology
                is possible to advise farms with the best times to irrigate a crop. </li>
              <li>Pesticide and fertilize planning</li>
            </ul>

            The complete deployment topology of an open field agriculture solution is described in the diagram below:
            <br>
            <br>
            <img src="./images/Agriculture.svg" width="100%" height="100%">
            <br>
            <br>
          </dd>
          <dt>Variants</dt>
          <dd>
            Open-field agriculture varies a lot between geographical location and methods. For example in the <a
              href="http://swamp-project.org/">SWAMP project</a> there three different pilots with different
            requirement/constraints:
            <ul>
              <li><a href="http://swamp-project.org/cbec/) (Reggio Emilia region">Italian pilot</a>:</li>
              <ul>
                <li>Relative small field size</li>
                <li>Multiple connectivity solutions available: 4G, LPWAN, and WiFi</li>
                <li>Variance in crop types, sometimes even inside the same farm</li>
                <li>Small soil type variance</li>
                <li>Precise model soil behavior</li>
                <li>A great influence of the water table</li>
                <li>Variance in the irrigation system</li>
                <li>Channel-based water distribution</li>
                <li>The main goal is to optimize water consumption</li>
              </ul>
              <li><a href="http://swamp-project.org/matopiba/) (Matopiba and Guaspari location">Brazilian pilot</a>:
              </li>
              <ul>
                <li>Huge field size</li>
                <li>Centra pivot irrigation systems: need to optimize each sprinkler output</li>
                <li>Soil type variance within the same field</li>
                <li>A low number of connectivity options: no 4G, only radio communication base on LPWAN</li>
                <li>Low crop type variance</li>
                <li>the main goal is to optimize energy consumption</li>
              </ul>
              <li><a href="http://swamp-project.org/intercrop/">Spain pilot</a></li>
              <ul>
                <li>Efficient localized irrigation and application of the right amount of water to the crop</li>
                <li>arid location</li>
                <li>The goal is to minimize water consumption but maintaining a good field yield.</li>
              </ul>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>
            Currently, there is no specification on how to model device status (i.e. connected/disconnected)
            Examples of how to handle a device calibration phase may help developers to use a standardized approach.
            Possibly define standard links types to define the relation between loggers and sensors
            Handle both geographical position and depth information.
            Ontology class for battery and energy consumption
            Model historical and forecast data

          </dd>
          <dt>Existing standards</dt>
          <dd>
            <ul>
              <li><a href="https://tools.ietf.org/html/rfc8376">LPWAN</a></li>
              <li><a href="http://www.sdi-12.org/current_specification/SDI-12_version-1_4-Jan-10-2019.pdf">SDI 12</a>
              </li>
              <li><a href="https://tools.ietf.org/html/rfc7252">CoAP</a></li>
              <li><a href="https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html">MQTT</a></li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>

            This use case is designed using the experience gained in the European-Brazil Horizon 2020 SWAMP project.
            Please follow the
            <a href="http://swamp-project.org/">link</a> for further information. Since SWAMP is heavily oriented to
            optimize water consumption,
            this document just mentioned issues like plant feeding, fertilizing, pollination, yield prediction, crop
            quality measurement, etc.
            Nevertheless, WoT technologies may be employed also in these scenarios.
          </dd>
        </dl>
      </section>
      <section id="Agricultural-use-case2">
        <h2>Irrigation in outdoor environment</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            <ul>
              <li>Catherine Roussey, INRAE [1], France</li>
              <li>Jean-Pierre Chanet, INRAE [1], France</li>
            </ul>

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

          </dd>
          <dt>Class</dt>
          <dd>

          </dd>
          <dt>Status</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li>device users: farmers</li>
              <li>service provider</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            Depending on the type of crops (e.g. maize), cultivated plots may need specific irrigation processes
            in outdoor environments. Depending on the country there exist some specific pedo-climatic conditions
            and some water consumption restrictions. Thus an irrigation system is installed on the plot. It is used
            on a several days basis (e.g. every 7 days), for each plot. The goal is to optimize the irrigation decision
            based on the crop development stage and the quantity of rain that has already fallen down on the plot.
            For example an important rain may postpone the irrigation decision.
            <br>
            <br>
            This use case aims to evaluate the number of days to delay the irrigation system, in addition to the basis
            irrigation frequency (e.g. 2 delay days means 9 days between two irrigations).

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>6 tensiometers in the plot (soil moisture):</li>
              <ul>
                <li> 3 tensiometers at 30 cm depth</li>
                <li> 3 tensiometer at 60 cm depth</li>
              </ul>
              <li>1 weather station:</li>
              <ul>
                <li>thermometer (outdoor temperature)</li>
                <li>pluviometer (rain quantity)</li>
              </ul>
              <li>1 mobile pluviometer (quantity of water provided by the watering system)</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            To decide when to water a cultivated plot, we evaluate the crop growth stage, the root zone moisture level
            and the number of delay days:

            <ul>
              <li>To evaluate the <b>Crop growth stage</b>, we need:</li>
              <ul>
                <li>Min and max temperature per day: the <b>min temperature per day</b> is evaluated on the period [d-1
                  18:00, d 18:00[. The <b>*max temperature per day</b> is evaluated on the period [d 06:00:00, d+1
                  06:00:00[.i</li>
                <li><b><a href="https://en.wikipedia.org/wiki/Growing_degree-day">Growing degree day</a></b> values uses
                  min and max temperature per day, the sowing day and the type of seed. The Growing degree day is
                  compared to some thresholds to evaluate the crop growth stage</li>
              </ul>
              <li>To evaluate the <b>Root zone moisture level</b>, we need: </li>
              <ul>
                <li>Mean moisture per day per probe: in order to get reliable values, each tensiometer sends several
                  measurements of soil moisture, at fixed hours of the day (usually in the morning), that are
                  aggregated; their mean value is considered</li>
                <li>For the set of 3 tensiometers localised at the same level of depth, the median value is evaluated
                  from their mean per day moisture measurements. One tensiometer may not provide accurate values (the
                  soil around the probe is too dry and the soil matter is not connected to the probe). The median value
                  of three different tensiometers at the same depth will improve the accuracy of the moisture
                  measurement.</li>
                <li>Then the sum of the two median values at two different depths is evaluated, to take into account the
                  quantity of water available in the root zone volume. This aggregated value estimates the root zone
                  moisture level. </li>
                <li>The root zone moisture level is compared to some thresholds (dependent on the crop growth stage) to
                  evaluate if the crop needs water or not at the end of the basis irrigation period.</li>
              </ul>
              <li>To determine the <b>number of delay days</b>, we need:</li>
              <ul>
                <li>The time period between two waterings of the same plot is dependent on the farm and known by the
                  farmer. When a watering is launched, no new watering should be planned during the basic irrigation
                  frequency. The quantity of rain that falls down on the plot may postpone the watering plan. The total
                  quantity of rain per day is compared to some thresholds to determine the number of delay days.</li>
              </ul>
            </ul>

            The mobile pluviometer is used to validate that the quantity of water received by the crop actually
            corresponds to the quantity of water provided by the watering system.
            <br>
            <br>
            At the end, the farmer may decide if he follows the irrigation recommendations or not. He could force the
            watering for one of the next days.
            <br>
          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            <ul>
              <li>WoT Architecture: wireless communication in outdoor environments presents some issues: communication
                consumes lots of energy, sensor nodes have limited energy, weather conditions impact communication
                quality</li>
              <li>WoT Thing Description: the affordance should be precise enough to describe the soil at a specific
                depth or the root zone volume or the min temperature per day</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            To avoid Property right and consent management issues between farmers and cloud service providers on these
            computed data, sensors are connected to the farm infrastructure and the services that evaluate aggregated
            data are executed locally on this infrastructure.
            <br>
            <br>
            The weather station may be located outside of the farm.
            <br>
            <br>
            The tensiometers are located inside the farm. The tensiometers and the mobile pluviometer are connected
            using wireless communication to the gateway. The gateway sends the measurements to the farm infrastructure.

          </dd>
          <dt>Variants:</dt>
          <dd>
            The crop growth stage may be observed by the farmer. In this case, he can force this value to update the
            service inputs.

          </dd>
          <dt>Security Considerations</dt>
          <dd>

            The 6 tensiometers and 1 pluviometer are installed on the plot, but only the farmer should be able to change
            their configurations (frequency of communication). Wireless communication should be used but the measurement
            data should only be accessible through the farm network infrastructure.

          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            Data concerning quantity of water, type of seed, sowing day should be protected.

          </dd>
          <dt>Gaps</dt>
          <dd>

            The main potential issues come from tensiometers located in the plot, as they are known to be cheap and easy
            to use probes but not always reliable. They can face multiple issues: if the soil gets too dry or the probe
            is improperly installed, there may be air between the probe and the soil, therefore preventing the probe
            from providing accurate conductivity measurements.
            <br>
            <br>
            To be sure of the quality of those measurements each tensiometer sends its measurements several times (3 to
            5) per day. The tensiometer may send an inappropriate value due to the bad connexion between the soil and
            the probe, that is the reason why three tensiometers are used and the median value is computed. If the
            gateway does not receive the value of one sensor during a whole day, an alert should be sent. To take an
            irrigation decision, at least one measurement per sensor and per day should be provided.
            <br>
            <br>
            The gateway can create a virtual entity corresponding to the sensor and allow the application to access this
            virtual entity having the actual sensor status like sleeping.
            <br>
            <br>
            Sensor nodes deployed in outdoor environments may take into account that their energy supply device
            (battery, solar panel) constrains the lifetime of the device. Thus they should be able to alert that they
            may not be able to provide a service due to lack of energy or they should be able to change their
            configuration and switch communication protocols to save as much energy as possible.
            <br>
            <br>
            Moreover wireless communication can be impacted by weather conditions or any outdoor conditions. For example
            a tractor that comes too close to the sensor node may move the communication device and destroy some
            components. Some kind of network supervision must be achieved (for instance by the gateway) to check node
            availability.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <ul>
              <li><a href="https://www.w3.org/TR/vocab-ssn/">Semantic Sensor Network</a></li>
              <li><a href="https://saref.etsi.org/saref4agri/v1.1.2/">SAREF4Agri</a></li>
              <li><a href="https://www.w3.org/TR/prov-o/">PROV-O</a></li>
              <li><a
                  href="https://irstea.github.io/caso/OnToology/ontology/caso.owl/documentation/index-en.html">CASO</a>
              </li>
              <li><a href="http://www.w3id.org/def/irrig">IRRIG</a></li>
            </ul>

            The CASO and IRRIG ontologies extend SSN, PROV-O and SAREF4AGRI to implement an irrigation expert system.
            <br>
            <br>
            A thesaurus climate and forecast that describes the weather properties and associated phenomenon is
            available at <a href="http://vocab.nerc.ac.uk/collection/P07/">http://vocab.nerc.ac.uk/collection/P07/</a>.
            <br>
            <br>
            The weather measurements provided by the agricultural weather station of Agrotechnopole is available at <a
              href="http://ontology.irstea.fr/weather/snorql/">http://ontology.irstea.fr/weather/snorql/</a>. [5]

          </dd>
          <dt>Comments</dt>
          <dd>

            This use case has been implemented in France, following local conditions and regulations. There exists an
            open manual irrigation decision method called <a
              href="http://www.irrinov.arvalisinstitutduvegetal.fr/irrinov.asp">IRRINOV¬Æ</a> developed by Arvalis [2]
            and INRAE dedicated to France and some specific crops: maize, wheat and cereals, potatoes and beans.
            <br>
            <br>
            IRRINOV¬Æ can be automated using wireless sensor networks and semantic web technologies. The considered
            network is of star type: all sensors can communicate with a common gateway, which is connected to the
            Internet. The IRRINOV¬Æ implementation was developed in [3]. This work presents an expert system for maize
            using drools. It automates the irrigation decision for maize based on sensor measurements.
            <br>
            <br>
            To measure weather properties, we use the recommendation provided by the French National Weather Institute:
            M√©t√©o France[4]. Its web site defines how to evaluate the min and max temperatures per day in <a
              href="http://www.meteofrance.fr/publications/glossaire/154123-temperature-minimale">http://www.meteofrance.fr/publications/glossaire/154123-temperature-minimale</a>
            (in French, we found no equivalent description in English).

          </dd>
          <dt>References</dt>
          <dd>

            [1] <a href="https://www.inrae.fr/">https://www.inrae.fr/</a>
            <br>
            [2] <a href="https://www.arvalisinstitutduvegetal.fr/">https://www.arvalisinstitutduvegetal.fr/</a>
            <br>
            [3] Q-D. Nguyen, C. ROUSSEY, M. Poveda-Villal√≥n, C. de Vaulx , J-P. Chanet. Development Experience of a
            Context-Aware System for Smart Irrigation Using CASO and IRRIG Ontologies. Applied Science 2020, 10(5),
            1803; <a href="https://doi.org/10.3390/app10051803">https://doi.org/10.3390/app10051803</a>
            <br>
            [4] <a href="http://www.meteofrance.fr/">http://www.meteofrance.fr/</a>
            <br>
            [5] C. ROUSSEY,S. BERNARD, G. ANDR√â, D. BOFFETY. Weather Data Publication on the LOD using SOSA/SSN
            Ontology.Semantic Web Journal, 2019 <a
              href="http://www.semantic-web-journal.net/content/weather-data-publication-lod-using-sosassn-ontology-0">http://www.semantic-web-journal.net/content/weather-data-publication-lod-using-sosassn-ontology-0</a>
          </dd>
        </dl>
      </section>
      <section id="smart-agriculture-openfield">
        <h2>Smart Agriculture: open-field agriculture</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Cristiano Aguzzi

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

            Smart Agriculture

          </dd>
          <dt>Class</dt>
          <dd>
            Open field agriculture

          </dd>
          <dt>Status: WIP</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>

            Agricultural corporation, Farmer, Manufacturers (Sensor, other facilities), Cloud provider, Middleware
            provider, Network providers, service provider.

          </dd>
          <dt>Motivation</dt>
          <dd>
            Water is vital for ensuring food security to the world‚Äôs population, and agriculture is the biggest consumer
            amounting for 70% of freshwater. Field irrigation application methods are one of the main causes of water
            wastage. The most common technique, surface irrigation, wastes a high percentage of the water by wetting
            areas where no plants benefit from it. On the other hand, localized irrigation can use water more
            efficiently and effectively, avoiding both under-irrigation and over-irrigation. However, in an attempt to
            avoid under-irrigation, farmers feed more water than is needed resulting not only to productivity losses,
            but also water wastages.

            Therefore, technology should be developed and deployed for sensing water needs and automatically manage
            water supply to crops. However, open field agriculture is characterized by a quite dynamic range of
            requirements. Usually, solutions developed for one particular crop type cannot be reused in other
            cultivations. Moreover, the same field can have different crop types or different sizes/shapes during the
            years, meaning that technology to monitor the state of crop growth should be highly configurable and
            adaptive. Even agriculture and irrigation methods can change and also they are very different depending on
            the size of the field and its clime type.

            Consequently, silos applications are deployed leveraging on IoT technologies to gather data about the crop
            growth state and irrigation needs. The Web of Things may help to create a single platform where
            cost-effective applications could adapt seamlessly between different scenarios, breaking the silos and
            giving value both to the environment and the market.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <p>Sensors:</p>
            <ul>
              <li>Weather sensors (maybe collected together inside a <a
                  href="https://en.wikipedia.org/wiki/Weather_station)">weather station</a>)</li>
              <ul>
                <li>temperature</li>
                <li>air humidity</li>
                <li>air pressure</li>
                <li>pluviometer</li>
                <li>global solar radiation</li>
                <li>anemometer (wind speed)</li>
                <li> wind direction</li>
                <li>global solar radiation and photosynthetically active radiation</li>
                <li> gas/air quality sensor (i.e. CO2)</li>
              </ul>
              <li>Soil sensors (usually packed together in soil probes)</li>
              <ul>
                <li>soil temperature</li>
                <li>soil moisture/water content</li>
                <li>soil conductivity (detecting salt levels in the soil)</li>
                <li> water table sensor</li>
              </ul>
              <li>Drone sensors</li>
              <ul>
                <li>camera</li>
                <li>temperature sensitive camera</li>
                <li>multispectral camera</li>
              </ul>
            </ul>
            <p>Actuators:</p>
            <ul>
              <li>drones: used for data collection or pesticed/impollination</li>
              <li>sprinklers</li>
              <li>pumps</li>
              <li>central pivot sprinklers</li>
              <li>hose-reel irrigation machine</li>
            </ul>
            <p>Additional devices:</p>
            <ul>
              <li>Solar panels</li>
              <li>Loggers: units that collect data from close sensors. </li>
              <li>Gateways</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>
            Sensor data plays a central role in Smart Agriculture. In particular, it is critical that the information
            sensed is associated with a timestamp. Common algorithms use *time series* to calculate the water needs of a
            crop.

            Furthermore, soil sensors usually are calibrated over a specific soil type (which may differ even in the
            same geographic region). For example, the calibration data for a soil moisture sensor is represented by a
            function that maps sensor output to soil water content. In literature, this function is knowns as a
            *calibration curve*. Commercial sensors are precalibrated with a "standard" curve but on most occasions, it
            fails to accurately measure the water content. Therefore, it can be configured during the installation phase
            (which may happen every time the soil is plowed).

            Finally, a crucial aspect is forecasting. Farmers use this information to actively change their management
            procedures. Services exploit it to suggest irrigation schedule or change device settings to behave
            accordingly to environmental changes.

            To summarize here it is a list of most important expected data from Open field agriculture:

            <ul>
              <li>Calibration curve</li>
              <li>Time series</li>
              <li>Forecast data</li>
              <li>Geolocations: sensor data must be contextualized in geolocation. Also, geolocation is critical in
                massive open fields to localize instrument position.</li>
              <li>Weather data</li>
              <li>Unit of measure: commercial soli sensor may output their value in a different unit of measures (i.e.
                volts or % water in an m^3 of soil)</li>
              <li>Relative values</li>
              <li>Depth position: geolocation is not sufficient to describe the parameters of the soil. Depth is an
                additional context that should be added to an observed value. </li>
              <li>Device owner information</li>
              <li>Battery level and energy consumption</li>
            </ul>

          </dd>
          <dt>Dependencies</dt>
          <dd>

            WoT Architecture, WoT Thing Description

          </dd>
          <dt>Description</dt>
          <dd>
            In open-field agriculture, the IoT solutions leverage on different radio protocols and devices. Usually,
            radio protocols should cover long distances (even kilometers) and be energy efficient. Devices too need to
            be energy saving as they are deployed for months and sometimes even years in harsh environments. A
            sleeping-cycle is one mechanism they use to save energy usually coordinated by *loggers/gateways* or
            preprogrammed. *Loggers* are deployed closed to sensor devices and have more storage space. They serve as
            buffers between sensors and higher services. Often *loggers* and sensors are embedded in the same board,
            otherwise, they are connected using cables or close-ranged radio protocols. On the other hand, *gateways*
            serve as a collection point for data of an entire field or farm. They are much more capable devices and
            usually are more energy-consuming. In some deployment scenarios, they host a full operating system with
            multiple software facilities installed. Otherwise, gateways only serve as relays of data sent from the
            loggers and sensors to cloud services and vice-versa. The cloud services may be partially hosted in edge
            servers to preserve data privacy and responsiveness of the whole IoT solution. Possible cloud services are:
            <ul>
              <li>Weather forecasting/local weather forecasting</li>
              <li>Soil digital twin to simulate and predict water content</li>
              <li>Plant digital twin (growth and water needs prediction)</li>
              <li>Irrigation advice service: combining the previous services and knowing the irrigation system topology
                is possible to advise farms with the best times to irrigate a crop. </li>
              <li>Pesticide and fertilize planning</li>
            </ul>

            The complete deployment topology of an open field agriculture solution is described in the diagram below:
            <br>
            <br>
            <img src="./images/Agriculture.svg" width="100%" height="100%">
            <br>
            <br>
          </dd>
          <dt>Variants</dt>
          <dd>
            Open-field agriculture varies a lot between geographical location and methods. For example in the <a
              href="http://swamp-project.org/">SWAMP project</a> there three different pilots with different
            requirement/constraints:
            <ul>
              <li><a href="http://swamp-project.org/cbec/) (Reggio Emilia region">Italian pilot</a>:</li>
              <ul>
                <li>Relative small field size</li>
                <li>Multiple connectivity solutions available: 4G, LPWAN, and WiFi</li>
                <li>Variance in crop types, sometimes even inside the same farm</li>
                <li>Small soil type variance</li>
                <li>Precise model soil behavior</li>
                <li>A great influence of the water table</li>
                <li>Variance in the irrigation system</li>
                <li>Channel-based water distribution</li>
                <li>The main goal is to optimize water consumption</li>
              </ul>
              <li><a href="http://swamp-project.org/matopiba/) (Matopiba and Guaspari location">Brazilian pilot</a>:
              </li>
              <ul>
                <li>Huge field size</li>
                <li>Centra pivot irrigation systems: need to optimize each sprinkler output</li>
                <li>Soil type variance within the same field</li>
                <li>A low number of connectivity options: no 4G, only radio communication base on LPWAN</li>
                <li>Low crop type variance</li>
                <li>the main goal is to optimize energy consumption</li>
              </ul>
              <li><a href="http://swamp-project.org/intercrop/">Spain pilot</a></li>
              <ul>
                <li>Efficient localized irrigation and application of the right amount of water to the crop</li>
                <li>arid location</li>
                <li>The goal is to minimize water consumption but maintaining a good field yield.</li>
              </ul>
            </ul>
          </dd>
          <dt>Gaps</dt>
          <dd>
            Currently, there is no specification on how to model device status (i.e. connected/disconnected)
            Examples of how to handle a device calibration phase may help developers to use a standardized approach.
            Possibly define standard links types to define the relation between loggers and sensors
            Handle both geographical position and depth information.
            Ontology class for battery and energy consumption
            Model historical and forecast data

          </dd>
          <dt>Existing standards</dt>
          <dd>
            <ul>
              <li><a href="https://tools.ietf.org/html/rfc8376">LPWAN</a></li>
              <li><a href="http://www.sdi-12.org/current_specification/SDI-12_version-1_4-Jan-10-2019.pdf">SDI 12</a>
              </li>
              <li><a href="https://tools.ietf.org/html/rfc7252">CoAP</a></li>
              <li><a href="https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html">MQTT</a></li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>

            This use case is designed using the experience gained in the European-Brazil Horizon 2020 SWAMP project.
            Please follow the <a href="http://swamp-project.org/">link</a> for further information. Since SWAMP is
            heavily oriented to optimize water consumption, this document just mentioned issues like plant feeding,
            fertilizing, pollination, yield prediction, crop quality measurement, etc. Nevertheless, WoT technologies
            may be employed also in these scenarios.
          </dd>
        </dl>
      </section>
    </section>

    <section id="smart-city">
      <h2>Smart City</h2>
      <section id="smartcity-geolocation">
        <h2>Smart City Geolocation</h2>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>
            Jennifer Lin (GovTech Singapore), Michael McCool
          </dd>
          <dt>Reviewer(s)</dt>
          <dd>
            Michael Lagally
          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

          </dd>
          <dt>Class</dt>
          <dd>

          </dd>
          <dt>Status</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>
            <p>
              A Smart City managing mobile devices and sensors,
              including passively mobile sensor packs, packages,
              vehicles, and autonomous robots, where their location needs to
              be determined dynamically.
            </p>
            <section id="todo-target-users-smartcity-geolocation" class="ednote">
              <p>
                TODO: Stakeholders/Users need to be further clarified.
                They include the city government,
                people counting
                service providers, police, network providers, ...
              </p>
            </section>
          </dd>
          <dt>Motivation</dt>
          <dd>
            <p>
              Smart Cities need to track a large number of mobile devices and sensors.
              Location information may be integrated with a logistics or fleet management
              system.
              A reusable geolocation module is needed with a common network interface to
              include in these various applications.
              For outdoor applications, GPS
              could be used, but indoors other geolocation technologies might be
              used, such as WiFi triangulation or vision-based navigation (SLAM).
              Therefore the geolocation information should be technology-agnostic.
            </p>
            <p>
              NOTE: we prefer the term "geolocation", even indoors, over "localization" to
              avoid confusion with language localization.
            </p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <p>
              One of the following:
            <ul>
              <li>A geolocation system on a personal device, such as a smart phone.</li>
              <li>A geolocation system to be attached to some other portable device.</li>
              <li>A geolocation system attached to a mobile vehicle.</li>
              <li>A geolocation system on a payload transported by a vehicle.</li>
              <li>A geolocation system on an indoor mobile robot.</li>
            </ul>
            </p>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <p>
            <ul>
              <li>Sensor ID</li>
              <li>Timestamp of last geolocation</li>
              <li>2D location</li>
              <ul>
                <li>typically latitude and longitude</li>
                <li>May also be semantic, i.e. room in a building, exit</li>
              </ul>
            </ul>
            </p>
            <p>
              Optional:
            <ul>
              <li>Semantic location</li>
              <ul>
                <li>Possibly in addition to numerical lat/long location.</li>
              </ul>
              <li>Altitude</li>
              <ul>
                <li>May also be semantic, i.e. floor of a building</li>
              </ul>
              <li>Heading</li>
              <li>Speed</li>
              <li>Accuracy information</li>
              <ul>
                <li>Confidence interval, e.g. distance that true location will be within some probability.</li>
                <li>Gaussian covariance matrix</li>
                <li>For each measurement</li>
                <li>For lat/long, may be a single value (see web browser API; radius?)</li>
              </ul>
              <li>Geolocation technology (GPS, SLAM, etc). </li>
              <ul>
                <li>Note that multiple technologies might be used together.</li>
                <li>Include parameters such as sample interval, accuracy</li>
              </ul>
              <li>For each geolocation technology, data specific to that technology:</li>
              <ul>
                <li>GPS: NMEA type</li>
              </ul>
              <li>Historical data</li>
            </ul>
            </p>
            <p>
              Note: the system should be capable of notifying consumers
              of changes in location.
              This may be used to implement geofencing by some other system.
              This may require additional parameters, such as the
              maximum distance that the device may be moved before a notification is
              sent, or the maximum amount of time between updates.
              Notifications may be sent by a variety of means, some of which may
              not be traditional push mechanisms (for example, email might be used).
              For geofencing applications, it is not necessary that the device be aware
              of the fence boundaries; these can be managed by a separate system.
            <p>
          </dd>
          <dt>Dependencies</dt>
          <dd>
            node-wot
          </dd>
          <dt>Description</dt>
          <dd>
            <p>
              Smart Cities have the need to observe the physical locations of
              large number of mobile devices
              in use in the context of a Fleet or Logistics Management System, or
              to place sensor data on a map in a Dashboard application.
              These systems may also include geofencing notifications and mapping
              (visual tracking) capabilities.
            </p>
          </dd>
          <dt>Variants</dt>
          <dd>
            <p>
            <ul>
              <li>A version of the system may log historical data so the past</li>
              locations of the devices can be recovered.
              <li>Geolocation technologies other than GPS may be used. The payload</li>
              may contain additional information specific to the geolocation
              technology used. In particular, in indoor situations technologies such
              as WiFi triangulation or (V)SLAM may be more appropriate.
              <li>Geofencing may be implemented using event notifications and
                will require setting of additional parameters such as maximum distance.</li>
            </ul>
            </p>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <p>
              High-resolution timestamps can be used in conjunction with cache manipulation to
              access protected regions of memory, as with the SPECTRE exploit. Certain
              geolocation APIs and technologies can return high-resolution timestamps which
              can be a potential problem. Eventually these issues will be addressed in cache
              architecture but in the meantime a workaround is to artificially limit the
              resolution of timestamps.
            </p>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <p>
              Location is generally considered private information when it is used with a device
              that may be associated with a specific person, such as a phone or vehicle, as it
              can be used to track that person and infer their activities or who they associate
              with (if multiple people are being tracked at once). Therefore APIs to access geographic location
              in sensitive contexts are often restricted, and access is allowed only after confirming
              permission from the user.
            </p>
          </dd>
          <dt>Gaps</dt>
          <dd>
            <p>
              There is no single standardized semantic vocabulary for representing location data.
              Location data can be point data, a path, an area or a volumetric object.
              Location information can be expressed using multiple standards,
              but the reader of location data in a TD or in data returned by an IoT device
              must be able to unambiguously describe location information.
            </p>
            <p>
              There are both dynamic (data returned by a mobile sensor) and static (fixed installation
              location) applications for geolocation data. For dynamic location data, some recommended vocabulary
              to annotate data schemas would be useful. For static location data, a standard format
              for metadata to be included in a TD itself would be useful.
            </p>
          </dd>
          <dt>Existing standards</dt>
          <dd>
            <p>
            <ul>
              <li>NMEA: defines sentences from GPS devices</li>
              <li><a href="https://en.wikipedia.org/wiki/World_Geodetic_System">WGS84</a>:</li>
              <ul>
                <li>World Geodetic System</li>
                <li>Defines lat/long/alt coordinate system used by most other geolocation standards</li>
                <li>More complicated than you would think (need to deal with deviations of Earth from
                  a true sphere, gravitational irregularities, position of centroid, etc. etc.)</li>
              </ul>
              <li><a href="https://www.w3.org/2003/01/geo/">Basic Geo Vocabulary</a>:</li>
              <ul>
                <li>Very basic RDF definitions for lat, long, and alt</li>
                <li>Does not define heading or speed</li>
                <li>Does not define accuracy</li>
                <li>Does not define timestamps</li>
                <li>Uses string as a data model (rather than a number)</li>
              </ul>
              <li><a href="https://www.w3.org/TR/geolocation-API/">W3C Geolocalization API</a>:</li>
              <ul>
                <li>W3C Devices and Sensors WG is now handling</li>
                <li>There is an updated proposal: <a
                    href="https://w3c.github.io/geolocation-sensor/#geolocationsensor-interface">https://w3c.github.io/geolocation-sensor/#geolocationsensor-interface</a>
                </li>
                <li>Data schema of updated proposal is similar to existing API, but all elements are now optional</li>
                <li>Data includes latitude, longitude, altitude, heading, and speed</li>
                <li>Accuracy is included for latitude/longitude (single number in meters, 95% confidence, interpretation
                  a little ambiguous, but probably intended to be a radius) and altitude, but not for heading or speed.
                </li>
              </ul>
              <li><a href="https://www.ogc.org/">Open Geospatial Consortium</a>:</li>
              <ul>
                <li>See <a
                    href="http://docs.opengeospatial.org/as/18-005r4/18-005r4.html">http://docs.opengeospatial.org/as/18-005r4/18-005r4.html</a>
                </li>
                <li>Referring to locations by coordinates</li>
                <li>Has standards defining semantics for identifying locations</li>
                <li>Useful for mapping</li>
              </ul>
              <li>ISO</li>
              <ul>
                <li><a href="https://www.iso.org/standard/74039.html">ISO19111</a>:</li>
                <ul>
                  <li>Standard for referring to locations by coordinates</li>
                  <li>Related to OGS standard above and WGS84</li>
                </ul>
                <li>Various other standards that relate to remote sensing, geolocation, etc.</li>
                <li>Here is an example (see references): <a
                    href="https://www.iso.org/obp/ui/fr/#iso:std:iso:ts:19159:-2:ed-1:v1:en">https://www.iso.org/obp/ui/fr/#iso:std:iso:ts:19159:-2:ed-1:v1:en</a>
                </li>
              </ul>
              <li><a href="https://www.w3.org/TR/vocab-ssn/">SSN</a>:</li>
              <ul>
                <li>Defines "accuracy": <a
                    href="https://www.w3.org/TR/vocab-ssn/#SSNSYSTEMAccuracy">https://www.w3.org/TR/vocab-ssn/#SSNSYSTEMAccuracy</a>
                </li>
                <li>Definition of accuracy is consistent with how it is used in Web Geolocation API</li>
                <li>Also defines related terms Precision, Resolution, Latency, Drift, etc.</li>
              </ul>
              <li>Timestamps:</li>
              <ul>
                <li>W3C standard in proposed new web geolocation API: <a
                    href="https://w3c.github.io/hr-time/#dom-domhighrestimestamp">https://w3c.github.io/hr-time/#dom-domhighrestimestamp</a>
                </li>
                <li>See also related issues such as latency defined in SSN</li>
              </ul>
              </p>
              <p>
                Note that accuracy and time are issues that apply to all kinds of sensors, not just
                geolocation. However, the specific geolocation technology of GPS
                is special since it is also a source of accurate time.
              </p>
          </dd>
          <dt>Comments</dt>
          <dd>

          </dd>
        </dl>
      </section>
      <section id="smartcity-dashboard">
        <h2>Smart City Dashboard</h2>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>
            Michael McCool
          </dd>
          <dt>Reviewer(s)</dt>
          <dd>
            Jennifer Lin, Michael Lagally
          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

          </dd>
          <dt>Class</dt>
          <dd>

          </dd>
          <dt>Status</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>
            <p>
              A Smart City managing a large number of devices whose data
              needs to be visualized and understood in context.
            </p>
            <p>
              Stakeholders include:
            <ul>
              <li>device owners: need to make data from devices available to dashboard system.</li>
              <li>device user: users of the dashboard system, such as members of city management,
                are indirectly "using" the devices by accessing their data, and
                in one variant, sending commands to actuators.</li>
              <li>cloud provider: the dashboard system itself or components of it (such as a database or data ingestion
                system) may
                be hosted in the cloud./li>
            </ul>
            </p>
            <section id="todo-target-users-smartcity-dashboard" class="ednote">
              <p>
                TODO: Stakeholders/Users need to be further clarified.
              </p>
            </section>
          </dd>
          <dt>Motivation</dt>
          <dd>
            <p>
              In order to facilitate Smart City planning and decision-making, a Smart City
              dashboard interface makes it possible for city management to
              view and visualize all sensor data through the entire city in real time,
              with data identified as to geographic source location.
            </p>
          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <p>
              Actuators can include robots; for these, commands might be given to robots to move to new locations,
              drop off or pick up sensor packages, etc.
              However, it could also include other kinds of actuators, such as flood gates, traffic signals, lights,
              signs, etc.
              For example, posting a public message on an electronic billboard might be one task possible through the
              dashboard.
            </p>
            <p>
              Sensors can include those for the environment and for people and traffic management
              (density counts, thermal cameras, car speeds, etc).
              status of robots, other actuators, and sensors, data visualization,
              and (optionally) historical comparisons.
            </p>
            <p>
              Dashboard would include mapping functionality.
              Mapping implies a need for location data for every actuator and sensor, which could be
              acquired through geolocation sensors (e.g. GPS) or assigned statically during installation.
            </p>
            <p>
              This use case also includes images from cameras and real-time image and data streaming.
            </p>
          </dd>
          <dt>Expected Data</dt>
          <dd>
            <p>
            <ul>
              <li>Environmental data for temperature, humidity, UV levels, pollution levels, etc.</li>
              <li>Infrastructure status (water flow, electrical grid, road integrity, etc)</li>
              <li>Emergency sensing (flooding, earthquake, fire, etc.)</li>
              <li>Traffic (both people and vehicles)</li>
              <li>Health monitoring (eg fever trackingi, mask detection, social distancing)</li>
              <li>Safety monitoring (eg wearing construction helmets on a construction site)</li>
              <li>Reports from non-IoT sources (for example, police reports of crimes, hospital emergency case reports)
              </li>
              <li>Images and data derived from images (people traffic and density can be derived from image analysis)
              </li>
              All data would need an associated geolocation and timestamp so it can be placed in time and space.
            </ul>
            </p>
          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>Thing description - support for data ingestion and normalization, geolocation and timestamp standards.
              </li>
              <li>Discovery - directories capable of tracking and managing a large number of devices on a large and
                possibly segmented network</li>
            </ul>
            </p>
          </dd>
          <dt>Description</dt>
          <dd>
            <p>
              Data from a large number and wide variety of sensors needs to be integrated
              into a single database and normalized, then placed in time and space, and
              finally visualized.
            </p>
            <p>
              The user, a member of city management responsible for making planning decisions,
              sees data visualized on a map suitable for planning decisions.
            </p>
            <p>
              Variants:
            <ul>
              <li>Historical data may also be available (allowing an analysis of trends over time).</li>
              <li>It may be possible to also issue commands to actuators through the interface.</li>
              <li>The system may be used for emergency response (for instance, closing floodgates in response to an
                expected tsunami)</li>
              <li>A subset of the data visualization capabilties may be made available to the public (for example,
                traffic)</li>
              <li>Filtering based on parameters such as location (area, state, county, country, zip code, etc), sensor
                type, subject matter, etc.</li>
              <li>Ability to generate alerts off of various parameters</li>
              <li>Ability to produce logs off historical data</li>
            </ul>
            </p>
          </dd>
          <dt>Security Considerations</dt>
          <dd>
            <p>
            <ul>
              <li>Access to data should only be provided to authorized users, although some may be made available
                publically</li>
              <li>Access to actuators should only be provided to authorized users, and commands should be recorded for
                auditing.</li>
            </ul>
            </p>
          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <p>
            <ul>
              <li>Management of privacy-sensitive information, for example images of people, </li>
              should be controlled and ideally not associated with specific individuals
              <li>Data that can be used to track movements of particular invididuals should be controlled or eliminated.
              </li>
              <li>Data purge functions should be supported to allow the permanent deletion of private data.</li>
            </ul>
            </p>
          </dd>
          <dt>Gaps</dt>
          <dd>
            <p>
            <ul>
              <li>Geolocation data standards</li>
              <li>Timestamp data standards</li>
              <li>Scalable Discovery</li>
            </ul>
            </p>
          </dd>
          <dt>Existing standards</dt>
          <dd>

          </dd>
          <dt>Comments</dt>
          <dd>

          </dd>
        </dl>
      </section>
      <section id="mmi-3-1_interactive-public-spaces">
        <h2>Interactive Public Spaces</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            Public spaces provide many opportunities for engaging, social and fun interaction.
            At the same time,
            preserving privacy while sharing tasks and activities with other people is a major issue
            in ambient systems.
            These systems may also deliver personalized information in combination
            with more general services presented publicly.

            A trustful discovery of the services and devices available in such environments
            is a necessity to guarantee personalization and privacy in public-space applications.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Public spaces supporting personalizable services and device access.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the personal mobile device
            application and the public space's services and devices.
            <br>
            <br>
            Profile data for user preferences.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                in IoT orchestration services in the public space.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            Interactive installations such as touch-sensitive or gesture-tracking billboards
            may be set up in public places.
            Objects that present public information (e.g. a map of a shopping mall)
            can use a multimodal interface (built-in or in tandem with the user's mobile devices)
            to simplify user interaction and provide faster access.
            Other setups can stimulate social activities,
            allowing multiple people to enter an interaction simultaneously to work together
            towards a certain goal (for a prize)
            or just for fun (e.g. play a musical instrument or control a lighting exhibition).
            In a context where privacy is an issue
            (for example, with targeted/personalized alerts or advertisements),
            the user's mobile device acts as a mediator for the services
            running on the public network.
            This allows the user to receive relevant information in the way she sees fit.
            Notifications can serve as triggers for interaction with public devices and services
            if the user chooses to do so.

          </dd>
          <dt>Variants</dt>
          <dd>

            The user may have additional mobile devices they want to incorporate into
            an interaction, for example a headset acting as an auditory aid or personal speech output
            device.

          </dd>
          <dt>Gaps</dt>
          <dd>

            Data format describing user interface preferences.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 3.1.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>
      <section id="mmi-3-2_meeting-room-event-assistance">
        <h2>Meeting Room Event Assistance</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>


          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Meeting space supporting personalizable services and device access.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the personal mobile device
            application and the meeting space's services and devices.

            Profile data for user preferences.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                in IoT orchestration services in the meeting space.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            A conference room where a series of meetings will take place.
            People can go in and out of the room before,
            after and during the meeting.
            The door is "touched" by a badge.
            An application on the user's mobile device can
            activate any available display in the room and the room and can access and
            receive notification from devices and services in the room.
            The chair of the meeting is notified by a dynamically composed graphic animation,
            audio notification or a mobile phone notification,
            about available devices and services, and
            can install applications indicated by links.

            The chair of the meeting selects a setup procedure by text amongst the provided links.
            These options could be, for example:
            photo step-by-step instructions (smartphone, HDTV display, Web site),
            audio instructions (MP3 audio guide, room speakers reproduction, HDTV audio)
            or RFID enhanced instructions (mobile SmartTag Reader, RFID Reader for smartphone).
            The chair of the meeting chooses the room speakers reproduction,
            then the guiding Service is activated and he starts to set the video projector.
            After some attendees arrive,
            the chair of the meeting changes to the slide show option and continues to
            follow the instructions at the same step it was paused but with another more
            private modality for example, a smartphone slideshow.

          </dd>
          <dt>Variants</dt>
          <dd>


            The user may have additional mobile devices they want to incorporate into
            an interaction, for example a headset acting as an auditory aid or personal speech output
            device.

          </dd>
          <dt>Gaps</dt>
          <dd>

            Data format describing user interface preferences.

            Ability to install applications based on links that can access IoT services.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 3.2.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>

      <section id="smart-campus">
        <h2>Cross-Domain Discovery in a Smart Campus</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>
            Andrea Cimmino and Ra√∫l Garc√≠a Castro

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>

          </dd>
          <dt>Class</dt>
          <dd>

          </dd>
          <dt>Status</dt>
          <dd>

          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li> device owners</li>
              <li> service provider</li>
              <li> network operator (potentially transparent for WoT use cases)</li>
              <li> directory service operator</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            In this use case a network full of IoT devices is presented, in which these devices are registered in
            several Middle-Nodes. The challenge presented in this scenario is to been able do discover the different
            sensors, by issues a SPARQL query, and without having prior knowledge of where those devices are allocated.
            Therefore, the discovery SPARQL query must start from a specific Middle-Node and reach all those
            Middle-Nodes that are relevant to answer the query.

            This scenario requires that discovery does not only happen locally when a Middle-Node receives the query and
            checks if some Thing Description registered is suitable to answer the query. Instead, the scenario requires
            also that the Middle-Node forwards the query through the network (topology conformed by the middle-nodes) in
            order to find those Middle-Nodes that actually contain relevant Thing Descriptions. Notice from the
            following example that the query is not broadcasted in the network to prevent flooding, instead the
            Middle-Nodes follow some discovery heuristic to know where the query should be forwarded. Also, notice that
            in this scenario not all the Middle-Nodes have the IoT devices registered directly within, they are
            Middle-Nodes collectors, such as Middle-Node C, I, G, and D.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Any device from the energy context (e.g., solar panels, smart plugs, or smart energy meters), devices from
            the building context (e.g., light bulbs, light switches, occupancy sensors, or thermostats), devices from
            the environmental context (e.g., soil moisture, flood detection, or air humidity), devices from the
            wearables context (e.g., smart bands), and/or devices from the water context (e.g., water valves, or water
            quality sensors)

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Data coming from different contexts, such as Energy, Building, Environmental Wearables and Water.

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            Current WoT-Discovery approach

          </dd>
          <dt> Description</dt>
          <dd>
            A campus has a wide range of IoT devices distributed across their grounds. These IoT devices belong to very
            different domains in a smart city, such as, energy, buildings, environment, water, wearable, etc. The IoT
            devices are distributed across the campus and belong to different infrastructures or even to individuals. A
            sample topology of this scenario could be the following:

            <br>
            <br>

            <img src="images/smart-campus-topology.svg" width="100%" height="100%">

            <br>
            <br>

            In this scenario, energy-related IoT devices monitor the energy use and income in the campus, among other
            things. From these measurements, an Energy Management System may predict a negative peak of incoming energy
            that would entail the failure of the whole system. In this case, a Service or a User needs to discover all
            those IoT devices that are not critical for the normal functioning of the campus (such as indoor or outdoor
            illumination, HVAC systems, or water heaters) and interact with them in order to save energy, by switching
            them off or reducing their consumption. Besides, the same Service or User will look for those IoT devices
            that are critical for the well-functioning of the campus (such as magnetic locks, water distribution system,
            or fire/smoke sensors) and ensure that they are up and running. Additionally, the Service or the User, will
            discover relevant people's wearable to warn them about the situation.

            <p>Sample flow:</p> A service, or a user, sends a (SPARQL) query to the discovery endpoint of a known
            Middle-Node (which can be wrapped by a GUI). The Middle-Node will try to answer the query first checking the
            Thing Descriptions of the IoT devices registered in such Middle-Node. Then, if the query requires further
            discovery, or it was not successfully answered the Middle-Node will forward the query to its *known*
            Middle-Nodes. Recursively, the Middle-Nodes will try to answer the query and/or forward the query to their
            known Middle-Nodes. When one Middle-Node is able to answer the query it will forward back to the former
            Middle-Node the partial query answer. Finally, when the discovery task finishes, the former Middle-Node will
            join all the partial query answers producing an unified view (which could be synchronous or asynchronous).

            <br>
            <br>


            For instance, assuming Middle-Node F receives a query that asks about all the discoverable Building IoT
            devices in the campus. First, the Middle-Node F will try to answer the query with the Thing Descriptions of
            the IoT registered within. Since Middle-Node F contains some Building IoT devices a partial query answer is
            achieved. However, since they query asked about all the discoverable Building IoT devices Middle-Node F
            should forward the query to its other known Middle-Nodes, i.e., Middle-Node G. This process will be repeated
            by the Middle-Nodes until the query reaches the Middle-Nodes H and B which are the ones that have registered
            Thing Descriptions about IoT buildings. Therefore, the query will travel through the topology as follows:
            <br>
            <br>
            <img src="images/smart-campus-sample.svg" width="100%" height="100%">
            <br>
            <br>
            Finally, when Middle Nodes B and H compute two partial query answers, those answers will be forwarded back
            to Middle-Node F which will join them with its former partial query answer obtained from its registered
            Thing Descriptions. Finally, a global query answer will be provided.

          </dd>

          <dt>Variants</dt>
          <dd>

          </dd>
          <dt>Security Considerations</dt>
          <dd>
            None, in this case an underneath infrastructure that handles security is assumed


          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            None, although relevant in this case the core of the use case relies on the feature of finding across the
            network different IoT devices. It is assumed that there is an underneath infrastructure that handles privacy

          </dd>
          <dt>Gaps</dt>
          <dd>

            Been able to find suitable Middle-Nodes that are relevant to answer the query, with no prior knowledge

          </dd>
          <dt>Existing standards</dt>
          <dd>
            None
          </dd>
          <dt>Comments</dt>
          <dd>
            None
          </dd>
        </dl>
      </section>

    </section>

    <section id="health">
      <h2>Health</h2>

      <section id="public-health">
        <h2>Public Health</h2>
        <section id="smartcity-health-monitoring">
          <h2>Public Health Monitoring</h2>
          <dl>

            <dt>Submitter(s)</dt>
            <dd>

              Jennifer Lin (GovTech Singapore)

            </dd>
            <dt>Reviewer(s)</dt>
            <dd>

              Michael McCool, Michael Lagally

            </dd>
            <dt>Tracker Issue ID</dt>
            <dd>



            </dd>
            <dt>Category</dt>
            <dd>



            </dd>
            <dt>Class</dt>
            <dd>



            </dd>
            <dt>Status</dt>
            <dd>



            </dd>
            <dt>Target Users</dt>
            <dd>

              Agencies, companies and other organizations in a Smart City with
              significant pedestrian traffic in a pandemic situation.

            </dd>
            <dt>Motivation</dt>
            <dd>

              A system to monitor the health of people in public places is useful to
              control the spread of infectious diseases. In particular, we would like
              to identify individuals with temperatures outside the norm (i.e. running
              a fever) and then take appropriate action. Actions can include sending
              a notification or actuating a security device, such as a gate.

              This mechanism should be non-invasive and non-contact since the solution
              should not itself contribute to the spread of infectious diseases.

              Data may also be aggregated for statistics purposes, for example, to
              identify the number of people in an area with elevated temperatures.
              This has additional requirements to avoid double-counting individuals.

            </dd>
            <dt>Expected Devices</dt>
            <dd>

              One of the following:
              <ul>
                <li>A thermal camera.</li>
                <li>Face detection (AI) service</li>
                <ul>
                  <li>May be on device or be an edge or cloud service.</li>
                </ul>
              </ul>

              Optional:
              <ul>
                <li>RGB and/or depth camera registered with the thermal camera</li>
                <li>Cloud service for data aggregation and analytics.</li>
                <li>Some way to identify location (optional)
                  Note that location might be static and configured during installation,
                  but might also be based on a localization technology if the device needs to be
                  portable (for example, if it needs to be set up quickly for an event).</li>
              </ul>

            </dd>
            <dt>Expected Data</dt>
            <dd>

              <ul>
                <li>Sensor ID</li>
                <li>Timestamp</li>
                <li>Number of people identified with a fever in image</li>
                <li>Estimated temperature for each person</li>
                <ul>
                  <li>May be coarse, low/normal/high</li>
                </ul>
                <li>Location</li>
                <ul>
                  <li>Latitude, Longitude, Altitude, Accuracy</li>
                  <li>Semantic (eg a particular building entrance)</li>
                </ul>
                <li>Thermal image</li>
              </ul>

              Optional:
              <ul>
                <li>RGB image</li>
                <li>Depth image</li>
                <li>Localization technology (see localization use case)</li>
                <li>Integration with local IoT devices: gates, lights, or people (guards)</li>
                <li>Bounding boxes around faces of identified people in image(s)</li>
                <li>Data that can be used to uniquely identify a face (distinguish it from others)</li>
                <ul>
                  <li>Aggregation system may output the total number of unique faces with fever</li>
                </ul>
              </ul>

              Note 1: the system should be capable of notifying consumers (such as security personnel),
              of fever detections.
              This may be email, SMS, or some other mechanism, such as MQTT publication.
              <br>
              <br>
              Note 2: In all cases where images are captured, privacy considerations apply.
              <br>
              <br>
              It would also be useful to count unique individuals for statistics purposes,
              but not necessarily based on identifying particular people. This is to
              avoid counting the same person multiple times.

            </dd>
            <dt>Dependencies</dt>
            <dd>

              node-wot

            </dd>
            <dt>Description</dt>
            <dd>

              A thermal camera image is taken of a group of people
              and an AI service is used to identify faces in the image.
              The temperature of each person is then estimated from the registered face;
              for greater accuracy, a consistent location for sampling should be used, such
              as the forehead.
              The estimated temperature is compared to high (and optionally, low)
              thresholds and a notification (or other action) is taken if the
              temperature is outside the norm.
              Additional features may be extracted to identify unique individuals.

            </dd>
            <dt>Variants</dt>
            <dd>


              <ul>
                <li>Enough information is included in the notification that the specific
                  person that raised the alarm can be identified. For example, if an RGB
                  camera is also registered with the thermal camera, then a bounding box may
                  be indicated via JSON and the RGB image included; or the bounding box could
                  be actually drawn into the sent image, or the face could be cropped out.
                  This is useful if, for example, a notification needs to be sent to health
                  or security workers who need to identify the person in a crowd.</li>
                <li>Instead of simply a notification, an action may be taken, such as closing
                  or refusing to open a gate at the entrance to a building, to prevent sick
                  employees from entering the building.</li>
                <li>To generate statistics, for example to count the number of people with
                  fevers, then unique individuals need to be indentified to avoid counting
                  the same person more than once.
                <li>The same sensors might be used to determine the number of people in
                  an area and send a notification if crowded conditions are detected, in
                  order to support social distancing behaviour (for instance, supporting
                  an app that notifies users when a destination is crowded) in a pandemic situation.</li>
                <li>Cameras that provide video streams rather than still images.</li>
              </ul>

            </dd>
            <dt>Security Considerations</dt>
            <dd>

              <ul>
                <li>Because PII is involved (see below) access should be controlled (only provided to authorized
                  users) and communications protected (encrypted).</li>
              </ul>

            </dd>
            <dt>Privacy Considerations</dt>
            <dd>
              <ul>
                <li>Images of people and their health status is involved. </li>
                <ul>
                  <li>If later these are made public then the health information of a particular person would be
                    released
                    publically.</li>
                  <li>There is also the possibility that the camera data could be in error, and should be confirmed with
                    a
                    more accurate sensor.</li>
                  <li>This information needs to be treated as PII and protected: only distributed to authorized users,
                    and
                    deleted when no longer needed.</li>
                  <li>However, derived aggregate information can be kept and published.</li>
                </ul>
              </ul>
            </dd>
            <dt>Gaps</dt>
            <dd>

              <ul>
                <li>Onboarding mechanism for rapidly deploying a large number of devices</li>
                <li>Standard vocabulary for geolocation information</li>
                <li>Implementations able to handle image payload formats, possibly in combination with non-image data
                  (eg images and JSON in a single response)</li>
                <li>Video streaming support (if we wish to serve video stream from the camera instead of still images)
                </li>
                <li>Standard ways to specify notification mechanisms and data payloads for things like SMS and email
                  (in addition to the expected MQTT, CoAP, and HTTP event mechanisms)</li>
              </ul>

            </dd>
            <dt>Existing standards</dt>
            <dd>


            </dd>
            <dt>Comments</dt>
            <dd>

              <ul>
                <li>May be additional requirements for privacy since images of people and their health status is
                  involved.</li>
                <li>Different sub-use cases: immediate alerts or actions vs. aggregate data gathering</li>
              </ul>

            </dd>
          </dl>
        </section>
        <section id="MedicalDevices">
          <h2>Interconnected medical devices in a hospital ICU</h2>
          <dl>

            <dt>Submitter(s)</dt>
            <dd>

              Taki Kamiya

            </dd>
            <dt>Reviewer(s)</dt>
            <dd>



            </dd>
            <dt>Tracker Issue ID</dt>
            <dd>



            </dd>
            <dt>Category</dt>
            <dd>



            </dd>
            <dt>Class</dt>
            <dd>



            </dd>
            <dt>Status</dt>
            <dd>



            </dd>
            <dt>Target Users</dt>
            <dd>

              <ul>
                <li>device owners</li>
                <li>device user</li>
                <li>cloud provider</li>
                <li>service provider</li>
                <li>device manufacturer</li>
                <li>gateway manufacturer</li>
                <li>identity provider</li>
              </ul>

            </dd>
            <dt>Motivation</dt>
            <dd>

              Preventable medical errors may account for more than 100,000 deaths per year in U.S. alone. These errors
              are mainly caused by failures of communication such as a chart misread or the wrong data passed along to
              machines or staffs. Part of the problem could be solved if the machines could speak to one another.
              Manufacturers have little incentive to make their proprietary code and data easily to accessible and
              process able by their competitors‚Äô machines. So the task of middleman falls to the hospital staffs. In
              addition to saving lives, a common framework could result in collecting and recording more clinical data
              on patients, making it easier to deliver precision medicine.

            </dd>
            <dt>Expected Devices</dt>
            <dd>


            </dd>
            <dt>Expected Data</dt>
            <dd>


            </dd>
            <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
            <dd>


            </dd>
            <dt>Description</dt>
            <dd>

              Physiological Closed-Loop Control (PCLC) devices are a group of emerging technologies, which use
              feedback from physiological sensor(s) to autonomously manipulate physiological variable(s) through
              delivery of therapies conventionally delivered by clinician(s).

              Clinical scenario without PCLC. An elderly female with end-stage renal failure was given a standard
              insulin infusion protocol to manage her blood glucose, but no glucose was provided. Her blood glucose
              dropped to 33, then rebounded to over 200 after glucose was given. This scenario has not changed for
              decades.

              The desired state with PCLC implemented in an ICU. A patient is receiving an IV insulin infusion and is
              having the blood glucose continuously monitored. The infusion pump rate is automatically adjusted
              according to the real-time blood glucose levels being measured, to maintain blood glucose values in a
              target range. If the patient‚Äôs glucose level does not respond appropriately to the changes in insulin
              administration, the clinical staff is alerted.

              Medical devices do not interact with each other autonomously (monitors, ventilator, IV pumps, etc.)
              Contextually rich data is difficult to acquire. Technologies and standards to reduce medical errors and
              improve efficiency have not been implemented in theater or at home.

              In recent years, researchers have made progress developing PCLC devices for mechanical ventilation,
              anesthetic delivery applications, and so on. Despite these promises and potential benefits, there has
              been limited success in the translation of PCLC devices from <a
                href="https://today.duke.edu/2014/07/benchbedside">bench to bedside</a>. A key challenge to bringing
              PCLC devices to a level required for a clinical trials in humans is risk management to ensure device
              reliability and safety.

              The United States Food and Drug Administration (FDA) classifies new hazards that might be introduced by
              PCLC devices into three categories. Besides clinical factors (e.g. sensor validity and reliability,
              inter- and intra-patient physiological variability) and usability/human factors (e.g. loss of
              situational awareness, errors, and lapses in operation), there are also engineering challenges including
              robustness, availability, and integration issues.

            </dd>
            <dt>Variants</dt>
            <dd>


              US military developed ONR SBIR (Automated Critical Care System Prototype), and found those issues.

              <ul>
                <li>No plug and play, i.e. cannot swap O2 Sat with another manufacturer.</li>
                <li>No standardization of data outputs for devices to interoperate.</li>
                <li>Must have the exact make/model to replace a faulty device or system will not work.</li>
              </ul>

            </dd>
            <dt>Security Considerations</dt>
            <dd>

              Security considerations for interconnected and dynamically composable medical systems are critical not
              only because laws such as <a href="https://www.hhs.gov/hipaa/index.html">HIPAA</a> mandate it, but also
              because security attacks can have serious safety consequences for patients. The systems need to support
              automatic verification that the system components are being used as intended in the clinical context,
              that the components are authentic and authorized for use in that environment, that they have been
              approved by the hospital‚Äôs biomedical engineering staff and that they meet regulatory safety and
              effectiveness requirements.

              For security and safety reasons, <a href="https://www.astm.org/Standards/F2761.htm">ICE
                F2761-09(2013)</a> compliant medical devices never interact directly each other. All interaction is
              coordinated and controlled via the applications.

              While transport-level security such as TLS provides reasonable protection against external attackers,
              they do not provide mechanisms for granular access control for data streams happening within the same
              protected link. Transport-level security is also not sufficiently flexible to balance between security
              and performance. Another issue with widely used transport-level security solutions is the lack of
              support for multicast.

            </dd>
            <dt>Privacy Considerations</dt>
            <dd>

              <section id="todo-privacy-2" class="ednote">TODO: Describe any issues related to privacy; if there are
                none,
                say "none" and justify</section>

            </dd>
            <dt>References</dt>
            <dd>
              <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that are relevant
                for
                this use case</section>

            </dd>
            <dt>Gaps</dt>
            <dd>

              Multicast support. It has proven useful for efficient and scalable discovery and information exchange in
              industrial systems.

            </dd>
            <dt>Existing standards</dt>
            <dd>

              <p><a href="https://www.astm.org/Standards/F2761.htm">F2761-09 (2013)</a> </p>

              Medical Devices and Medical Systems - Essential safety requirements for equipment comprising the
              patient-centric integrated clinical environment (ICE) - Part 1: General requirements and conceptual
              model.

              The idea behind ICE is to allow medical devices that conform to the ICE standard, either natively or
              using an adapter, to interoperate with other ICE-compliant devices regardless of manufacturer.

              <p><a href="https://www.openice.info/">OpenICE</a></p>

              OpenICE is an initiative to create a community implementation of F2761-09 (ICE - Integrated Clinical
              Environment) based on <a href="https://www.omg.org/spec/DDS/About-DDS/">DDS</a>.

              <p><a href="https://secwww.jhuapl.edu/mdira/documents">MDIRA Specification Document Version
                  1.0</a>. </p>

              MDIRA Version 1.0 provides requirements and implementation guidance for MDIRA-compliant systems focused
              on trauma and critical care in austere environments.

              Johns Hopkins University Applied Physics Laboratory (JHU-APL) lead a research project in collaboration
              with US military to develop a framework of autonomous / closed loop prototypes for military health care
              which are dual use for the civilian healthcare system.

            </dd>
            <dt>Comments</dt>
            <dd>

            </dd>
          </dl>
        </section>
      </section>

      <section id="private-health">
        <h2>Private Health</h2>
        <section id="mmi-4-1_health-notifiers">
          <h2>Health Notifiers</h2>
          <dl>

            <dt>Submitter(s)</dt>
            <dd>

              Michael McCool

            </dd>
            <dt>Reviewer(s)</dt>
            <dd>



            </dd>
            <dt>Tracker Issue ID</dt>
            <dd>



            </dd>
            <dt>Category</dt>
            <dd>

              Accessibility

            </dd>
            <dt>Class</dt>
            <dd>



            </dd>
            <dt>Status</dt>
            <dd>



            </dd>
            <dt>Target Users</dt>
            <dd>

              End user with a health problem they wish to monitor.

              Health services provider (doctor, nurse, paramedic, etc).

            </dd>
            <dt>Motivation</dt>
            <dd>

              In critical situations regarding health,
              like a medical emergency,
              media multimodality may be the most effective way to communicate alerts,
              When the goal is to monitor the health evolution of a
              person in both emergency and non-emergency contexts,
              access via networked devices may be the most effective way to collect data and
              monitor a patient's status.

            </dd>
            <dt>Expected Devices</dt>
            <dd>

              Medical facilities supporting device and service access.

            </dd>
            <dt>Expected Data</dt>
            <dd>

              Command and status information transferred between the personal mobile device
              application and the meeting space's services and devices.

              Profile data for user preferences.

            </dd>
            <dt>Dependencies</dt>
            <dd>

              <ul>
                <li>WoT Thing Description</li>
                <li>WoT Discovery</li>
                <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                  in IoT orchestration services.</li>
              </ul>

            </dd>
            <dt>Description</dt>
            <dd>

              In medical facilities,
              a system may provide multiple options to control sensor operations
              by voice or gesture ("start reading my blood pressure now").
              These interactions may be mediated by an application installed into a smartphone.
              The system integrates information from multiple sensors
              (for example, blood pressure and heart rate);
              reports medical sensor readings periodically (for example, to a remote medical facility)
              and sends alerts when unusual readings/events are detected.

            </dd>
            <dt>Variants</dt>
            <dd>


              The user may have additional mobile devices they want to incorporate into
              an interaction, for example a headset acting as an auditory aid or personal speech output
              device.

            </dd>
            <dt>Gaps</dt>
            <dd>

              Data format describing user interface preferences.

              Ability to install applications based on links that can access IoT services.

            </dd>
            <dt>Existing standards</dt>
            <dd>

              This use case is based on MMI UC 3.2.

            </dd>
            <dt>Comments</dt>
            <dd>

              Does not include Requirements section from original MMI use case.
            </dd>
          </dl>
        </section>
      </section>
    </section>

    <section id="manufacturing">
      <h2>Manufacturing</h2>
      <section id="big-data">
        <h2>Big Data for Manufacturing</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            Device owners, cloud provider.

          </dd>
          <dt>Motivation</dt>
          <dd>

            Production lines for industrial manufacturing consist of multiple machines, where each machine
            incorporates sensors for various values.
            A failure of a single machine can cause defective products or a stop of the entire production.

            Big data analysis enables to identify behavioral patterns across multiple production lines of the entire
            production plant and across multiple plants.

            The results of this analysis can be used for optimizing consumption of raw materials, checking the status
            of production lines and plants and predicting and preventing fault conditions.


          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Various sensors, e.g. temperature, light, humidity, vibration, noise, air quality.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Discrete sensor values, such as temperature, light, humidity, vibration, noise, air quality readings.
            The data can be delivered periodically or on demand.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            Thing Description: groups of devices, aggregation / composition mechanism, thing models
            Discovery/Onboarding: Onboarding of groups of devices

          </dd>
          <dt>Description</dt>
          <dd>

            A company owns multiple factories which contain multiple production lines.
            Examples are production lines, environment sensors,
            These devices collect data from multiple sensors and transmit this information to the cloud. Sensor data
            is stored in the cloud, can be visualized and analyzed using machine learning / AI.

            The cloud service allows to manage single and groups of devices.
            Combining the data streams from multiple devices allows to get an easy overview of the state of all
            connected devices in the user's realm.

            In many cases there are groups of devices of the same kind, so the aggregation of data across devices can
            serve to identify anomalies or to predict impending outages.

            The cloud service allows to manage single and groups of devices and can help to identify abonormal
            conditions.
            For this purpose a set of rules can be defined by the user, which raises alerts towards the user or
            triggers actions on devices based on these rules.

            This enables the early detection of pending problems and reduces the risk of machine outages, quality
            problems or threats to the environment or life of humans.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>


          </dd>
          <dt>Existing standards</dt>
          <dd>

            <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that
              are relevant for this use case</section>

          </dd>
          <dt>Comments</dt>
          <dd>

            See also Digital Twin use case.

          </dd>
        </dl>
      </section>

      <section id="multi-vendor">
        <h2>Multi-Vendor System Integration</h2>
        <section id="wot-profile">
          <h2>Out of the box interoperability</h2>
          <dl>

            <dt>Submitter(s)</dt>
            <dd>

              Michael Lagally

            </dd>
            <dt>Reviewer(s)</dt>
            <dd>

              All WoT members, specifically Sebastian Kaebisch, Michael McCool, Ege Korkan, Zoltan Kis, Takuki Kamiya,
              Ryuichi Matsukura, Kunihiko Toumura, Michael Koster.

            </dd>
            <dt>Tracker Issue ID</dt>
            <dd>



            </dd>
            <dt>Category</dt>
            <dd>



            </dd>
            <dt>Class</dt>
            <dd>



            </dd>
            <dt>Status</dt>
            <dd>



            </dd>
            <dt>Target Users</dt>
            <dd>

              <ul>
                <li>device owner</li>
                <li>service provider</li>
                <li>cloud provider</li>
                <li>device manufacturer</li>
                <li>gateway manufacturer</li>
              </ul>

            </dd>
            <dt>Motivation</dt>
            <dd>

              <ul>
                <li>As an device owner, I want to know whether a device will work with my system before I purchase it to
                  avoid wasting money.</li>
                <ul>
                  <li>Installers of IoT devices want to be able to determine if a given device will be compatible with
                    the
                    rest of their installed systems and whether they will have access to its data and affordances.</li>
                </ul>
              </ul>

              <ul>
                <li>As a developer, I want TDs to be as simple as possible so that I can efficiently develop them.</li>
                <ul>
                  <li>Here "simple" should relate to the end goal, "efficiently develop"; that is, TDs should be
                    straightforward for the average developer to complete and validate.</li>
                </ul>
              </ul>

              <ul>
                <li>As a developer, I want to be able to validate that a Thing will be compatible with a Consumer
                  without having to test against every possible consumer.</li>
              </ul>

              <ul>
                <li>As a cloud provider I want to onboard, manage and communicate with as many devices as possible out
                  of the box.</li>
                This should be possible without device specific customization.
              </ul>

            </dd>
            <dt>Expected Devices</dt>
            <dd>

              sensors, actuators, gateways, cloud, directory service.

            </dd>
            <dt>Expected Data</dt>
            <dd>

              discrete or streaming data.

            </dd>
            <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
            <dd>

              WoT Profile, WoT Thing Description

            </dd>
            <dt>Description</dt>
            <dd>

              As a consumer of devices I want to be able to process data from any device that conforms to a class of
              devices.

              I want to have a guarantee that I'm able to correctly interact with all affordances of the Thing that
              complies with this class of devices.
              Behavioral ambiguities between different implementations of the same description should not be possible.

              I want to integrate it into my existing scenarios out of the box, i.e. with close to zero configuration
              tasks.

            </dd>
            <dt>Variants</dt>
            <dd>

              N/A

            </dd>
            <dt>Gaps</dt>
            <dd>

              <section id="todo-gaps-x" class="ednote">
                TODO: Describe any gaps that are not addressed in the current WoT standards and building blocks
              </section>

            </dd>
            <dt>References</dt>
            <dd>

              <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that are relevant
                for
                this use case</section>

        </section>

        </dd>
        <dt>Existing standards</dt>
        <dd>

          various.

        </dd>
        <dt>Comments</dt>
        <dd>

          A strawman proposal for a profile specification has been submitted at: <a
            href="https://github.com/w3c/wot-profile">https://github.com/w3c/wot-profile</a>
          <br>
          <br>
          An initial set of requirements is available here:<a
            href="https://github.com/w3c/wot-profile/edit/master/REQUIREMENTS/requirements.md">https://github.com/w3c/wot-profile/edit/master/REQUIREMENTS/requirements.md</a>
          <br>
          <br>
          Recommendations for commonalities and interoperability profiles of IoT platforms:<a
            href="https://european-iot-pilots.eu/wp-content/uploads/2018/11/D06_02_WP06_H2020_CREATE-IoT_Final.pdf">https://european-iot-pilots.eu/wp-content/uploads/2018/11/D06_02_WP06_H2020_CREATE-IoT_Final.pdf</a>
        </dd>
        </dl>
      </section>
      <section id="digital-twin">
        <h2>Digital Twin</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>


          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            Device owners, cloud provider.

          </dd>
          <dt>Motivation</dt>
          <dd>

            A digital twin is the virtual representation of a physical asset such as a machine, a vehicle, robot,
            sensor.
            Using a digital twin allows businesses to analyze their physical assets to troubleshoot in real time,
            predict future problems, minimize downtime, and perform simulations to create new business opportunities.

            A digital twin may also be called a twin or a shadow. Digital twin technology may be referred to as device
            virtualization.

            Digital twins can be located in the edge or in the cloud.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Various devices such as sensors, machines, vehicles, production lines, industry robots.

            Digital twin platforms at the edge or in the cloud.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Machine status information, discrete sensor data or data streams.

          </dd>
          <dt>Dependencies</dt>
          <dd>


            <ul>
              <li>WoT Architecture </li>
              <li>WoT Thing Description </li>
              <li>WoT Profile </li>
              <li>WoT Scripting? </li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            The user benefits from using digital twins with the following scenarios:

            <ul>
              <li>Better visibility: Continually view the operations of the machines or devices, and the status of
                their interconnected systems.</li>
            </ul>

            <ul>
              <li>Accurate prediction: Retrieve the future state of the machines from the digital twin model by using
                modeling.</li>
            </ul>

            <ul>
              <li>What-if analysis: Easily interact with the model to simulate unique machine conditions and perform
                what-if analysis using well-designed interfaces.</li>
            </ul>

            <ul>
              <li>Documentation and communication: Use of the digital twin model helps to understand, document, and
                explain the behavior of a specific machine or a collection of machines.</li>
            </ul>

            <ul>
              <li>Integration of disparate systems: Connect with back-end applications related to supply chain
                operations such as manufacturing, procurement, warehousing, transportation, or logistics.</li>
            </ul>


          </dd>
          <dt>Variants</dt>
          <dd>

            <p>Virtual Twin</p>

            The virtual twin is a representation of a physical device or an asset. A virtual twin uses a model that
            contains observed and desired attribute values and also uses a semantic model of the behavior of the
            device.

            Intermittent connectivity: An application may not be able to connect to the physical asset. In such a
            scenario, the application must be able to retrieve the last known status and to control the operation
            states of other assets.

            Protocol abstraction: Typically, devices use a variety of protocols and methods to connect to the IoT
            network. From a users perspective this complexity should not affect other business applications such as an
            enterprise resource planning (ERP) application.

            Business rules: The user can specify the normal operating range of a property in a semantic model.
            Business rules can be declaratively defined and actions can be automatically invoked in the edge or on the
            device.

            Example: In a fleet of connected vehicles, the user monitors a collection of operating parameters, such as
            fuel level, location, speed and others. The semantics-based virtual twin model enables the user to decide
            whether the operating parameters are in normal range. In out of range conditions the user can take
            appropriate actions.


            <p>Predictive Twin</p>

            In a predictive twin, the digital twin implementation builds an analytical or statistical model for
            prediction by using a machine-learning technique. It need not involve the original designers of the
            machine. It is different from the physics-based models that are static, complex, do not adapt to a
            constantly changing environment, and can be created only by the original designers of the machine.

            A data analyst can easily create a model based on external observation of a machine and can develop
            multiple models based on the user‚Äôs needs.
            The model considers the entire business scenario and generates contextual data for analysis and
            prediction.

            When the model detects a future problem or a future state of a machine, the user can prevent or prepare
            for them.
            The user can use the predictive twin model to determine trends and patterns from the contextual machine
            data. The model helps to address business problems.


            <p>Twin Projections</p>

            In twin projections, the predictions and the insights integrate with back-end business applications,
            making IoT an integral part of business processes.
            When projections are integrated with a business process, they can trigger a remedial business workflow.

            Prediction data offers insights into the operations of machines. Projecting these insights into the
            back-end applications infrastructure enables business applications to interact with the IoT system and
            transform into intelligent systems.


          </dd>
          <dt>Gaps</dt>
          <dd>

            WoT does not define a way to describe the behavior of a thing to use for a simulation.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <section id="todo-references-x" class="ednote">
              TODO: Provide links to relevant standards that are
              relevant for this use case</section>

          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>
      <section id="X-Protocol-Interworking">
        <h2>Cross Protocol Interworking</h2>
        <dl>
          <dt>Submitter(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Ege Korkan, Michael McCool, Michael Koster, Sebastian K√§bisch

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>


          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            Device owners, cloud providers.

          </dd>
          <dt>Motivation</dt>
          <dd>

            In smart city, home and industrial scenarios various devices are connected to a common network. These
            devices implement different protocols. To enable interoperability, an "agent" needs to communicate across
            different protocols. Platforms for this agent can be edge devices, gateways or cloud services.

            Interoperability across protocols is a must for all user scenarios that integrate devices from more than
            one protocol.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Various sensors, e.g. temperature, light, humidity, vibration, noise, air quality, edge devices, gateways,
            cloud servers and services.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Discrete sensor values, such as temperature, light, humidity, vibration, noise, air quality readings.
            A/V streams.
            The data can be delivered periodically or on demand.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            WoT Profiles.

          </dd>
          <dt>Description</dt>
          <dd>

            There are multiple user scenarios that are addressed by this use case.

            An example in the smart home environment is an automatic control lamps, air conditioners, heating, window
            blinds in a household
            based on sensor data, e.g. sunlight, human presence, calendar and clock, etc.

            In an industrial environment individual actuators and production devices use different protocols.
            Examples include MQTT, OPC-UA, Modbus, Fieldbus, and others.
            Gathering data from these devices, e.g. to support digital twins or big data use cases requires an "Agent"
            to bridge across these protocols.
            To provide interoperability and to reduce implementation complexity of this agent a common set of (minimum
            and maximum)
            requirements need to be supported by all interoperating devices.

            A smart city environment is similar to the industrial scenario in terms of device interoperability.
            Devices differ however,
            they include smart traffic lights, traffic monitoring, people counters, cameras.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>

            A common profile across protocols is required to address this use case.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            MQTT, OPC-UA, BACNet, CoAP, various other home and industrial protocols.
          </dd>
          <dt>Comments</dt>
          <dd>


          </dd>
        </dl>
      </section>
    </section>

    <section id="multimodal">
      <h2>Multimodal System Integration</h2>
      <section id="mmi-5-1_multimodal-recognition-support">
        <h2>Multimodal Recognition Support</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            Recognizer system development has arrived at a point of maturity where
            if we want to dramatically enhance recognition performance,
            sensor fusion from multiple modalities is needed.
            In order to achieve this,
            an image recognizer should incorporate results coming from other
            kinds of recognizers (e.g. audio recognizer) within the network
            engaged in the same interaction cycle.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Audio sensing device (microphone).

            Video sensing device (camera).

            Audio recognition service.

            Video recognition service.

            Devices capabale of presenting alerts in various modalities.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the sensing devices,
            the recognition services, and the alert devices.

            Profile data for user preferences.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                in IoT orchestration services.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            An audio recognizer has been trained with the more common sounds in the house,
            in order to provide alerts in case of an emergency.
            In the same house a security system uses a video recognizer to identify people
            at the front door.
            These two systems need to cooperate with a remote home management system
            to provide integrated services.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>

            Support for video and audio recognition services.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 5.1.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>
      <section id="mmi-5-2_enhancement-of-synergistic-interactions">
        <h2>Enhancement of Synergistic Interactions</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            One of the main indicators concerning the usability of a system
            is the corresponding level of accessibility provided by it.
            The opportunity for all the users to receive and to deliver all kinds of information,
            regardless of the information format or the type of user profile,
            state or impairment is a recurrent need in web applications.
            One of the means to achieve accessibility is the design of a more
            synergic interaction based on the discovery of multimodal Modality Components.

            Synergy is two or more entities functioning together to produce a result
            that is not obtainable independently.
            It means "working together".
            For example,
            how to avoid disruptive interactions
            in nomadic systems (always affected by the changing context)
            is an important issue.
            In these applications,
            user interaction is difficult,
            distracted and less precise.
            Discovery and use of alternative input and output devices
            can increase synergic interaction offering new possibilities
            more adapted to the current context.
            Such a system can also enhance the fusion process for target groups of
            users experiencing permanent or temporary learning difficulties or with sensorial,
            emotional or social impairments.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            A normal client computer with I/O devices that need to be emulated.

            Alternative I/O devices that need to be interfaced to the client system.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the client computer
            and the alternative I/O devices.

            Profile data for user preferences.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                in IoT orchestration services.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            A person working mostly with a PC is having a problem with his right arm and hands.
            He is unable to use a mouse or a keyboard for a few months.
            He can point at things, sketch, clap, make gestures, but he can not make any precise movements.
            A generic interface allows this person to perform his most important tasks in his
            personal devices:
            to call someone, open a mailbox, access his agenda or navigate over some Web pages.
            The generic interface can propose child-oriented intuitive interfaces like a
            clapping-based interface,
            a very articulated TTS component, or reduced gesture input widgets.
            Other specialized devices might include phones with very big numbers,
            very simple remote controls,
            screens displaying text at high resolution,
            or voice command devices.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Gaps</dt>
          <dd>


          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 5.2.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>
    </section>

    <section id="accessibility">
      <h2>Accessibility</h2>
      <section id="mmi-1-1_audiovisual-devices-as-smartphone-extensions">
        <h2>Audiovisual Devices Acting as Smartphone Extensions</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            Many of today's home IoT-enabled devices can provide similar functionality
            (e.g. audio/video playback),
            differing only in certain aspects of the user interface.
            This use case would allow continuous interaction with a specific
            application as the user moves from room to room,
            with the user interface switched automatically to the set of
            devices available in the user's present location.

            On the other hand,
            some devices can have specific capabilities
            and user interfaces that can be used to add information to a larger context
            that can be reused by other applications and devices.
            This drives the need to spread an application across different devices
            to achieve a more user-adapted and meaningful interaction according to the
            context of use.
            Both aspects provide arguments for exploring use cases where
            applications use distributed multimodal interfaces.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Mobile phone or other client running an application requiring a extended and
            more accessible user interface.

            IoT-enabled audio-visual devices providing audio and visual information
            display capabilities that can be used to augment the user interface of the
            application.

            Possible edge computation services providing speech-to-text or described video
            (e.g. object detection) capabilities.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Visual display information mapping information from audio to visual modalities,
            for example text generated from voice recognition.

            Text from an application that needs to be displayed at a larger size.

            Visual alerts correspondig to audio stimuli, eg sound effects in a game mapped
            to visual icons.

            Visual information mapped to audio information, for example,
            described video based on an AI service providing object recognition.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from application for interacting
                with devices.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            A home entertainment system is adapted by a mobile device
            as a set of user interface components.

            In addition to media rendering and playback,
            these Devices also act as input or output modalities for
            an application, for example an application running on a smartphone.
            The native user interface on the application
            does not have to be manipulated directly at all.
            A wall-mounted touch-sensitive TV could be used to navigate applications,
            and a wide-range microphone can handle speech input.
            Spatial (Kinect-style) gestures may also be used to control
            application behavior.

            Accessibility support software on the smartphone
            discovers available modalities and arranges them to best
            serve the user's purpose.
            One display can be used to show photos and movies,
            another for navigation.
            As the user walks into another room,
            this configuration is adapted dynamically to the new location.
            User intervention may be sometimes required to decide on
            the most convenient modality configuration.
            The state of the interaction is maintained
            while switching between modality sets.
            For example,
            if the user was navigating a GUI menu in the living room,
            it is carried over to another screen when she switches rooms,
            or replaced with a different modality such as voice
            if there are no displays in the new location.

          </dd>
          <dt>Variants</dt>
          <dd>

            Modalities may be translated from one form to another to accomodate
            accessibility issues, for example, visual cues into audio cues and
            vice-versa, as appropriate.

          </dd>
          <dt>Gaps</dt>
          <dd>

            An AI service may be require to perform modality mapping, for example,
            object recognition.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 1.1.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
            Variant supporting
            modality conversion is not included in the original MMI use case.
          </dd>
        </dl>
      </section>
      <section id="mmi-1-2_unified-smart-home-control-and-status">
        <h2>Unified Smart Home Control and Status Interface</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            The increase in the number of controllable devices in an
            intelligent home creates a problem with controlling all available services
            in a coherent and useful manner.
            Having a shared context,
            built from information collected through sensors and direct user input,
            would improve recognition of user intent, and thus simplify interactions.

            In addition,
            multiple input mechanisms could be selected by the user based on device type,
            level of trust and the type of interaction required for a particular task.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Mobile phone or other client running an application providing command
            mediation capabilities.

            IoT-enabled smart home devices supporting
            remote sensing and actuation functionality.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the command mediation
            application and one or more devices.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from application for interacting
                with devices.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            Smart home functionality (window blinds, lights, air conditioning etc.)
            is controlled through a multimodal interface,
            composed from modalities built into the house itself
            (e.g. speech and gesture recognition)
            and those available on the user's personal devices
            (e.g. smartphone touchscreen).
            The system may automatically adapt to the preferences of a specific user,
            or enter a more complex interaction if multiple people are present.

            Sensors built into various devices around the house can act as input
            modalities that feed information to the home and affect its behavior.
            For example,
            lights and temperature in the gym room can be adapted dynamically
            as workout intensity recorded by the fitness equipment increases.
            The same data can also increase or decrease volume and tempo of music tracks
            played by the user's mobile device or the home's media system.

          </dd>
          <dt>Variants</dt>
          <dd>

            The intelligent home in tandem with the user's personal
            devices can additionally monitor user behavior for emotional patterns
            such as 'tired' or 'busy' and adapt further.

          </dd>
          <dt>Gaps</dt>
          <dd>

            A service may be needed to recognize gestures and emotional states.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 1.2; original title was Intelligent Home Apparatus.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>
    </section>

    <section id="automotive">
      <h2>Automotive</h2>
      <section id="mmi-2-1_smart-car-configuration-management">
        <h2>Smart Car Configuration Management</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>

            Accessibility

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>


          </dd>
          <dt>Motivation</dt>
          <dd>

            User interface personalization is a task that most often needs to be repeated
            for all Devices a user wishes to interact with recurringly.
            With complex devices,
            this task can also be very time-consuming,
            which is problematic if the user regularly accesses similar,
            but not identical devices, as in the case of several cars rented over a month.

            A standardized set of personal information and preferences that could be used
            to configure personalizable devices automatically would be very helpful for all
            these cases in which the interaction becomes a customary practice.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Personal mobile device running an application providing command
            mediation capabilities.

            IoT-enabled smart car supporting
            remote sensing, actuation, and configuration functionality.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Command and status information transferred between the personal mobile device
            application and the car's services and devices.

            Profile data for user preferences.

          </dd>
          <dt>Dependencies</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API in application on mobile personal device and possibly
                in IoT orchestration services in the car.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            Basic in-car functionality is standardized to be managed by other devices.
            A user can control seat, radio or AC settings through a personalized multimodal interface
            shared by the car and her personal mobile device.
            User preferences are stored on the mobile Device (or in the cloud),
            and can be transferred across different car models handling a specific functionality
            (e.g. all cars with touchscreens should be able to adapt to a "high contrast" preference).

            The car can make itself available as a complex modality component that wraps around all
            functionality and supported modalities,
            or as a collection of modality components such as touchscreen, speech recognition system,
            or audio player.
            In the latter case,
            certain user preferences may be shared with other environments.

            For example,
            a user may opt to select the "high contrast" scheme at night on all of her displays,
            in the car or at home.
            A car that provides a set of modalities can be also adapted by the mobile device
            to compose an interface for its functionality,
            for example to manage playback of music tracks through the car's voice control system.
            Sensor data provided by the phone can be mixed with data recorded by the car's own sensors
            to profile user behavior which can be used as context in multimodal interaction.

          </dd>
          <dt>Variants</dt>
          <dd>

            Additional portable devices may be brought into the car and also be
            incorporated into an application, for example, a GPS navigation system.

          </dd>
          <dt>Gaps</dt>
          <dd>

            Data format describing user interface preferences.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            This use case is based on MMI UC 2.1.

          </dd>
          <dt>Comments</dt>
          <dd>

            Does not include Requirements section from original MMI use case.
          </dd>
        </dl>
      </section>
    </section>

    <section id="smart-grids">
      <h2>Energy / Smart Grids</h2>
      <section id="smart-grid">
        <h2>Smart Grids</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Christian Glomb (Siemens)

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Michael Lagally (Oracle)

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li>Grid operators on all voltage levels line Distribution System Operators (DSO), Transmission System
                Operators (TSO)</li>
              <li>Plant operators (centralized as well as de-centralized producers)</li>
              <li>Virtual Power Plant (VPP) operators</li>
              <li>Energy grid markets</li>
              <li>Cloud providers where grid backend services are hosted and where Operation Technology bridges to
                Information Technology</li>
              <li>Device manufacturers, owners, and users; devices include communication gateways, monitoring and
                control units</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>


          </dd>
          <dt>Expected Devices</dt>
          <dd>

            A smart grid integrates all players in the electricity market into one overall system through the
            interaction of generation, storage, grid management and consumption. Power and storage plants are already
            controlled today in such a way that only as much electricity is produced as is needed. Smart grids include
            consumers as well as small, decentralized energy suppliers and storage locations in this control system,
            so that on the one hand, consumption is more homogeneous in terms of time and space (see also intelligent
            electricity consumption) and on the other hand, in principle inhomogeneous producers (e.g. wind power) and
            consumers (e.g. lighting) can be better integrated.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>Weather and climate data</li>
              <li>Metering data (both production as well as consumption as well as storage, e.g. 15 min. intervals)
              </li>
              <li>Real time data from PMUs (Phasor Measurement Units)</li>
              <li>Machine and equipment monitoring data (enabling health checks)</li>
              <li>...</li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            WoT Architecture, WoT Binding Templates (covering protocol specifica)

          </dd>
          <dt>Description</dt>
          <dd>

            The term Smart Grid refers to the communicative networking and control of power generators, storage
            facilities, electrical consumers, and grid equipment in power transmission and distribution networks for
            electricity supply. This enables the optimization and monitoring of the interconnected components. The aim
            is to secure the energy supply on the basis of efficient and reliable system operation.

          </dd>
          <dt>Variants</dt>
          <dd>

            <p>Decentralized Power Generation</p>

            While electricity grids with centralized power generation have dominated up to now, the trend is moving
            towards decentralized generation plants, both for generation from fossil primary energy through small CHP
            plants and for generation from renewable sources such as photovoltaic systems, solar thermal power plants,
            wind turbines and biogas plants. This leads to a much more complex structure, primarily in the area of
            load control, voltage maintenance in the distribution grid and maintenance of grid stability. In contrast
            to medium to large power plants, smaller, decentralised generation plants also feed directly into the
            lower voltage levels such as the low-voltage grid or the medium-voltage grid. This use case variants also
            includes operation and control of energy storages like batteries.

            <p>Virtual Power Plants</p>

            A Virtual Power Plant (VPP) is an aggregation of Distributed Energy Resources (DERs) that can act as an
            entity on energy markets or as an ancillary service to grid operation.
            The individual DERs often have a primary use on their own, with electric generation/consumption being a
            side-effect resp. secondary use. This results in negotiations/collaborations between many different
            parties e.g. such as the DER owner, the VPP operator, the grid operator and others.

            <p>Smart Metering</p>
            For consumers, a major change is the installation of smart meters. Their core tasks are remote reading and
            the possibility to realize fluctuating prices within a day at short notice. All electricity meters must
            therefore be replaced by those with remote data transmission.

            <p>Other variants</p>
            Emergency response, grid synchronization, grid black start

          </dd>
          <dt>Building Blocks</dt>
          <dd>
            <ul>
              <li>Multi-Stakeholder Operation: Multiple involved parties have to find a common mode of operation</li>
              <li>Device Lifecycle Management: Since the VPP is a dynamic system of loosely coupled DERs, the
                appearance and disappearance of DERs as well as the software management on the devices itself requires
                a means to orchestrate the lifecycle of individual device's respective components.</li>
              <li>Embedded Runtime: Especially for DERs in remote locations, maintaining a close couple control loop
                can be expensive if feasible at all. Therefore, it is desirable to be able to offload control logic to
                the DER itself.</li>
              <li>Ensemble Discovery: In order to dynamically find matching DERs needed for the operational goal of a
                VPP, a registry with different options of DER discovery is needed.</li>
              <li>Content-Negotiation: The different stakeholders have to interact and therefore need a common data
                format.</li>
              <li>Resource Description: The DER has to describe itself to enable discovery of single DERs and
                ensembles, also the operational data needs to be understood by the different stakeholders without
                engineering effort.</li>
              <li>Push Services: As there is a fan-out with many devices that probably have a rate-limited connection
                connecting to one single command centre, a bidirectional communication mechanism is needed rather than
                polling for the reverse direction</li>
              <li>Object Memory: As multiple and interchangable stakeholders are involved in the application, a
                backlog of the object is beneficial for scrollkeeping</li>
            </ul>

          </dd>
          <dt>Non-Functionals</dt>
          <dd>
            <ul>
              <li>Privacy: As fine-grained metering informtion provides sensitive data about a household, the system
                should show a high digree of privacy</li>
              <li>Trust: Since the data exchange between the virtual power plant and the distributed energy resource
                leads to a physical action that invokes high currents and monetary flows, the integrity of both
                parties and the exchange's data is crucial</li>
              <li>Layered L7 Communication: Since multiple different links are used for monitoring and control,
                integration requires a clear and consistent seperation of information from the used serialization and
                application protocols to enable the exchange of homogenous information over heterogenous application
                layer protocols</li>
            </ul>

          </dd>
          <dt>Gaps</dt>
          <dd>

            <section id="<section id=" todo-references-x" class="ednote">TODO: Provide links to relevant standards that
              are relevant for this use case</section>-gaps-x" class="ednote"><section id="todo-references-x"
              class="ednote">TODO: Provide links to relevant standards that are relevant for this use case</section>:
            Describe any gaps that are not addressed in the current WoT standards and building blocks
      </section>

      </dd>
      <dt>Existing standards</dt>
      <dd>

        IEC 61850 - International standard for data models and communication protocols

        IEEE 1547 - US standard for interconnecting distributed resources with electric power systems

      </dd>
      <dt>Comments</dt>
      <dd>
      </dd>
      </dl>
    </section>

    <section id="transportation">
      <h2>Transportation</h2>
      <dl>

        <dt>Submitter(s)</dt>
        <dd>

          Zoltan Kis

        </dd>
        <dt>Reviewer(s)</dt>
        <dd>

          Jennifer Lin, Michael McCool

        </dd>
        <dt>Tracker Issue ID</dt>
        <dd>



        </dd>
        <dt>Category</dt>
        <dd>



        </dd>
        <dt>Class</dt>
        <dd>



        </dd>
        <dt>Status</dt>
        <dd>



        </dd>
        <dt>Sub-categories</dt>
        <dd>
          Transportation - Infrastructure
          Transportation - Cargo
          Transportation - People

        </dd>
        <dt>Target Users</dt>
        <dd>

          Smart Cities: managing roads, public transport and commuting, autonomous and human driven vehicles,
          transportation tracking and control systems, route information systems, commuting and public transport,
          vehicles, on-demand transportation, self driving fleets, vehicle information and control systems,
          infrastructure sharing and payment system, smart parking, smart vehicle servicing, emergency monitoring,
          etc.

          Transport companies: managing shipping, air cargo, train cargo and last mile delivery transportation
          systems including automated systems.

          Commuters: Mobility as a service, booking systems, route planning, ride sharing, self-driving,
          self-servicing infrastructure, etc.


        </dd>
        <dt>Motivation</dt>
        <dd>

          Provide common vocabulary for describing transport related services and solutions that can be reused
          across sub-categories, for easier interoperability between various systems owned by different
          stakeholders.

          Thing models could be defined in many subdomains to help integration or interworking between multiple
          systems.

          Transportation of goods can be optimized at global level by enhancing interoperability between vertical
          systems.

        </dd>
        <dt>Expected Devices</dt>
        <dd>

          Road information system (routes, conditions, navigation).
          Road control system (e.g. virtual rails).
          Traffic management services, e.g. intelligent traffic light system with localization and identification
          (by satellite, radio frequency identification, cameras etc).
          Emergency monitoring and data/location sharing.
          Airport management.
          Shipping docks and ports management.
          Train networks management.
          Public transport vehicles (train, metro, tram, bus, minibus), mobility as a service (ride sharing, bicycle
          sharing, scooters etc).
          Transportation network planning and management (hubs, backbones, sub-networks, last mile network).
          Electronic timetable management system.
          Vehicles (human driven, self-driving, isolated or part of fleet).
          Connected vehicles (cars, ships, airplanes, trains, buses etc).

        </dd>
        <dt>Expected Data</dt>
        <dd>

          Vehicle data (identification, location, speed, route, selected vehicle data).
          Weather and climate data.
          Contextual data (representing various risk factors, delays, etc).

        </dd>
        <dt>Dependencies</dt>
        <dd>

          Localization technologies.
          Automotive data.
          Contextual data.
          Cloud integration.

        </dd>
        <dt>Description</dt>
        <dd>

          Transportation system implementers will be able to use a unified data description model accoss various
          systems.

        </dd>
        <dt>Variants</dt>
        <dd>

          There will be different verticals, such as:

          <ul>
            <li>Smart City public transport</li>
            <li>Smart City traffic management</li>
            <li>Smart city vehicle management</li>
            <li>Cargo traffic management</li>
            <li>Cargo vehicle management</li>
          </ul>

        </dd>
        <dt>Gaps</dt>
        <dd>


        </dd>
        <dt>Existing standards</dt>
        <dd>

          <section id="todo-references-x" class="ednote">TODO: Provide links to relevant standards that
            are relevant for this use case</section>

        </dd>
        <dt>Comments</dt>
        <dd>


        </dd>
      </dl>
    </section>

    <section id="smart-buildings">
      <h2>Building Technologies</h2>

      <section id="smart-building-things">
        <h2>Linking Smart Building Things and BIM Models</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>
            <p><a href="https://orcid.org/0000-0002-5672-5508">Edison Chung (MINES St. Etienne)</a></br>
              <a href="https://orcid.org/0000-0001-7604-8543">Herv√© Pruvost (Fraunhofer IIS EAS)</a></br>
              <a href="https://orcid.org/0000-0002-2033-859X">Georg Ferdinand Schneider (Schaeffler Technologies
                AG)</a></p>


          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            <p>Michael Lagally (Oracle)</br>
              Sebastian Kaebisch (Siemens)</p>

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

            &lt;please leave blank&gt;

          </dd>
          <dt>Category</dt>
          <dd>

            Smart Building

          </dd>
          <dt>Class</dt>
          <dd>

            &lt;please leave blank&gt;

          </dd>
          <dt>Status</dt>
          <dd>

            &lt;please leave blank&gt;

          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>service provider</li>
              <li>device manufacturer</li>
              <li>gateway manufacturer</li>
              <li>identity provider</li>
              <li>directory service operator</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            <p>When operating smart buildings, aggregating and managing all data provided by heterogeneous devices in
              these buildings still require a lot of manual effort. Besides the hurdles of data acquisition that
              relies on multiple protocols, the acquired data generally lacks contextual information and metadata
              about its location and purpose. Usually, each service or application that consumes data from building
              things requires information about its content and its context like, e.g.:</p>
            <ul>
              <li>which thing produces the data (sensor, meter, actuator, other technical component...) in a building;
              </li>
              <li>which physical quantity or process is represented (temperature, energy supply, monitoring,
                actuation);</li>
              <li>which other building things are involved (e.g. sensor hosted by a duct or a space).</li>
            </ul>
            <p>Through the increased use of model-based data exchange over the whole life cycle of a building, often
              referred to as Building Information Modeling (BIM), a curated source for data describing the building
              itself is available including, amongst others, the topology of the building structured into e.g. sites,
              storeys and spaces.</p>
            <p>Automatically tracking down data and their related things in a building would especially ease the
              configuration and operation of Building Automation and Control Systems (BACS) and Heating
              Ventilation and Air-Conditioning (HVAC) services during commissioning, operation, maintenance and
              retrofitting. To tackle these challenges, still, building experts make use of metadata and naming
              conventions which are manually implemented in Building Management Systems (BMS) databases to annotate
              data and things. An important property of a thing is its location within the topology of a building as well
              as where its related data are produced or used. For example, this applies to the temperature sensor of a space,
              the temperature setpoint of a zone, a mixing damper flap actuator of a HVAC component, etc. In addition, other
              attributes of things are of interest, such as cost or specific manufacturer data. One difficulty is
              especially the lack of a standardized way of creating, linking and sharing this information in an
              automated manner. On the contrary, manufacturers, service providers and users introduce their own
              metadata for their own purpose. As a solution, the Web of Things (WoT) Thing Description (TD) aims at
              providing normalized and syntactic interoperability between things.</p>
            <p>To support this effort, this use case is motivated by the need to enhance semantic interoperability
              between things in smart buildings and to provide them with contextual links to building information. This
              building information is usually obtained from a BIM model. The use case builds on Web of Data technologies
              and reuses schemas available from the Linked Building Data domain. It should serve as a use case template
              for many applications in an Internet of Building Things (IoBT).</p>

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>Actuators</li>
              <li>Sensors</li>
              <li>Devices from the building context</li>
              <li>Devices from the HVAC system</li>
              <li>Smart devices</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>Sensor ID</li>
              <li>Thing Descriptions</li>
              <li>Protocol integrations</li>
              <li>Sensor readings</li>
              <li>Building topology</li>
              <li>Semantic location</li>
              <li>Geolocation</li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            <ul>
              <li><a href="https://www.w3.org/TR/wot-thing-description/">Web of Things Thing Description (WoT TD)</a>
              </li>
              <li><a href="https://w3c.github.io/wot-binding-templates/">Web of Things Binding Templates</a></li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            <p>The goal of this use case is to show the potential to automate workflows and address the heterogeneity of
              data as observed in the smart building domain. The examples show the potential benefits of combining WoT
              TD with contextual data obtained from BIM.</p>
            <p>The use cases is based on the <a
                href="https://github.com/TechnicalBuildingSystems/OpenSmartHomeData">Open
                Smart Home Dataset</a>, which introduces a BIM model for a residential flat combined with observations
              made by typical smart home sensors. We extend the dataset with Thing Descriptions of some of the items.
              The respective Thing Description of a temperature sensor in the kitchen of the considered flat is as
              follows:</p>
            <pre class="example" id="Kitchen-temp-Sensor">
                    <code class="lang-json">{
                        <span class="hljs-attr">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Kitchen-temp-Sensor"</span>,
                        <span class="hljs-attr">"@context"</span>: [
                            <span class="hljs-string">"https://www.w3.org/2019/wot/td/v1"</span>,
                            {
                                <span class="hljs-attr">"osh"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet#"</span>,
                                <span class="hljs-attr">"bot"</span>: <span class="hljs-string">"https://w3id.org/bot#"</span>,
                                <span class="hljs-attr">"sosa"</span>: <span class="hljs-string">"http://www.w3.org/ns/sosa/"</span>,
                                <span class="hljs-attr">"om"</span>: <span class="hljs-string">"http://www.ontology-of-units-of-measure.org/resource/om-2/"</span>,
                                <span class="hljs-attr">"ssns"</span>: <span class="hljs-string">"http://www.w3.org/ns/ssn/systems/"</span>,
                                <span class="hljs-attr">"brick"</span>: <span class="hljs-string">"https://brickschema.org/schema/Brick#"</span>,
                                <span class="hljs-attr">"schema"</span>: <span class="hljs-string">"http://schema.org"</span>
                            }
                        ],
                        <span class="hljs-attr">"title"</span>: <span class="hljs-string">"Kitchen-temp-Sensor"</span>,
                        <span class="hljs-attr">"description"</span>: <span class="hljs-string">"Kitchen Temperature Sensor"</span>,
                        <span class="hljs-attr">"@type"</span>: [<span class="hljs-string">"sosa:Sensor"</span>, <span class="hljs-string">"brick:Zone_Air_Temperature_Sensor"</span>, <span class="hljs-string">"bot:element"</span>],
                        <span class="hljs-attr">"@reverse"</span>: {
                            <span class="hljs-attr">"bot:containsElement"</span>: {
                                <span class="hljs-attr">"@id"</span>: <span class="hljs-string">"osh:Kitchen"</span>
                            }
                        },
                        <span class="hljs-attr">"securityDefinitions"</span>: {
                            <span class="hljs-attr">"basic_sc"</span>: {
                                <span class="hljs-attr">"scheme"</span>: <span class="hljs-string">"basic"</span>,
                                <span class="hljs-attr">"in"</span>: <span class="hljs-string">"header"</span>
                            }
                        },
                        <span class="hljs-attr">"security"</span>: [
                            <span class="hljs-string">"basic_sc"</span>
                        ],
                        <span class="hljs-attr">"properties"</span>: {
                            <span class="hljs-attr">"Kitchen-temp"</span>: {
                                <span class="hljs-attr">"type"</span>: <span class="hljs-string">"number"</span>,
                                <span class="hljs-attr">"unit"</span>: <span class="hljs-string">"om:degreeCelsius"</span>,
                                <span class="hljs-attr">"forms"</span>: [
                                    {
                                        <span class="hljs-attr">"href"</span>: <span class="hljs-string">"https://kitchen.example.com/temp"</span>,
                                        <span class="hljs-attr">"contentType"</span>: <span class="hljs-string">"application/json"</span>,
                                        <span class="hljs-attr">"op"</span>: <span class="hljs-string">"readproperty"</span>
                                    }
                                ],
                                <span class="hljs-attr">"readOnly"</span>: <span class="hljs-literal">true</span>,
                                <span class="hljs-attr">"writeOnly"</span>: <span class="hljs-literal">false</span>
                            }
                        },
                        <span class="hljs-attr">"sosa:observes"</span>: {
                            <span class="hljs-attr">"@id"</span>: <span class="hljs-string">"osh:Kitchen-temp"</span>,
                            <span class="hljs-attr">"@type"</span>: <span class="hljs-string">"sosa:ObservableProperty"</span>
                        },
                        <span class="hljs-attr">"ssns:hasSystemCapability"</span>: {
                            <span class="hljs-attr">"@id"</span>: <span class="hljs-string">"osh:Kitchen-temp-Sensor-Capa"</span>,
                            <span class="hljs-attr">"@type"</span>: <span class="hljs-string">"ssns:SystemCapability"</span>,
                            <span class="hljs-attr">"ssns:hasSystemProperty"</span>: {
                                <span class="hljs-attr">"@type"</span>: [<span class="hljs-string">"ssns:MeasurementRange"</span>],
                                <span class="hljs-attr">"schema:minValue"</span>: <span class="hljs-number">0.0</span>,
                                <span class="hljs-attr">"schema:maxValue"</span>: <span class="hljs-number">40.0</span>,
                                <span class="hljs-attr">"schema:unitCode"</span>: <span class="hljs-string">"om:degreeCelsius"</span>
                            }
                        }
                    }
                    </code>
                </pre>
            <p>Where the contextual information on the measurement range of the sensor is specified using the <a
                href="http://www.w3.org/ns/ssn/systems/">SSNS</a> schema. The location information of the thing <a
                href="#Kitchen-temp-Sensor">Kitchen-temp-Sensor</a> is
              provided based on the <a href="https://w3id.org/bot">Building Topology Ontology (BOT)</a>, a minimal
              ontology developed by the <a href="https://www.w3.org/community/lbd/">W3C Linked Building Data Community
                Group (W3C LBD CG)</a> to describe the topology of buildings in the semantic web. Additionally, the
              thing description of the corresponding actuator is given below.</p>
            <p></p>
            <pre class="example" id="Kitchen-tempS-Actuator">
                    <code class="lang-json">{
                    <span class="hljs-attr">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet#Kitchen-tempS-Actuator"</span>,
                    <span class="hljs-attr">"@context"</span>: [
                        <span class="hljs-string">"https://www.w3.org/2019/wot/td/v1"</span>,
                        {
                            <span class="hljs-attr">"osh"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet#"</span>,
                            <span class="hljs-attr">"bot"</span>: <span class="hljs-string">"https://w3id.org/bot#"</span>,
                            <span class="hljs-attr">"sosa"</span>: <span class="hljs-string">"http://www.w3.org/ns/sosa/"</span>,
                            <span class="hljs-attr">"ssn"</span>: <span class="hljs-string">"http://www.w3.org/ns/ssn/"</span>,
                            <span class="hljs-attr">"brick"</span>: <span class="hljs-string">"https://brickschema.org/schema/Brick#"</span>
                        }
                    ],
                    <span class="hljs-attr">"title"</span>: <span class="hljs-string">"Kitchen-tempS-Actuator"</span>,
                    <span class="hljs-attr">"description"</span>: <span class="hljs-string">"Kitchen Temperature Setpoint Actuator"</span>,
                    <span class="hljs-attr">"@type"</span>: [<span class="hljs-string">"sosa:Actuator"</span>, <span class="hljs-string">"brick:Zone_Air_Temperature_Setpoint"</span>, <span class="hljs-string">"bot:element"</span>],
                    <span class="hljs-attr">"@reverse"</span>: {
                        <span class="hljs-attr">"bot:containsElement"</span>: {
                            <span class="hljs-attr">"@id"</span>: <span class="hljs-string">"osh:Kitchen"</span>
                        }
                    },
                    <span class="hljs-attr">"securityDefinitions"</span>: {
                        <span class="hljs-attr">"basic_sc"</span>: {
                            <span class="hljs-attr">"scheme"</span>: <span class="hljs-string">"basic"</span>,
                            <span class="hljs-attr">"in"</span>: <span class="hljs-string">"header"</span>
                        }
                    },
                    <span class="hljs-attr">"security"</span>: [
                        <span class="hljs-string">"basic_sc"</span>
                    ],
                    <span class="hljs-attr">"actions"</span>: {
                        <span class="hljs-attr">"Kitchen-tempS"</span>: {
                            <span class="hljs-attr">"forms"</span>: [
                                {
                                    <span class="hljs-attr">"href"</span>: <span class="hljs-string">"https://kitchen.example.com/tempS"</span>
                                }
                            ]
                        }
                    },
                    <span class="hljs-attr">"ssn:forProperty"</span>: {
                        <span class="hljs-attr">"@id"</span>: <span class="hljs-string">"osh:Kitchen-tempS"</span>,
                        <span class="hljs-attr">"@type"</span>: <span class="hljs-string">"sosa:ActuatableProperty"</span>
                    }
                }
                </code>
            </pre>
          <dt><em>Combining Topological Context and Thing Descriptions</em></dt>
          <p>The scenario considered is related to the replacement of a temperature sensor in a BACS. The topological
            information localizing the things, e.g. the <a href="#Kitchen-temp-Sensor">temperature sensor</a> can
            be used to automatically commission the newly replaced sensor and link it to existing control
            algorithms. For this purpose, the identifiers of suitable sensors and actuators are needed and can be,
            for example, queried via <a href="https://www.w3.org/TR/sparql11-query/">SPARQL</a>. Here the query uses
            some additional classification of sensors from <a href="https://brickschema.org/ontology/1.1">BRICK
              schema</a>.</p>

          <pre class="example" id="sparql">
                            <code class="lang-sparql">PREFIX <span class="hljs-string">bot:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//w3id.org/bot&gt;</span>
                            PREFIX <span class="hljs-string">brick:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//brickschema.org/schema/Brick#&gt;</span>
                            PREFIX <span class="hljs-string">osh:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//w3id.org/ibp/osh/OpenSmartHomeDataSet#&gt;</span>
                            SELECT ?sensor ?actuator
                            WHERE{
                              ?space a bot:Space .
                              ?space bot:containsElement ?sensor .
                              ?space bot:containsElement ?actuator .
                              ?sensor a brick:Zone_Air_Temperature_Sensor .
                              ?actuator a brick:Zone_Air_Temperature_Setpoint .
                            }
                            </code>
                        </pre>
          <p>Similarly this data can obtained via a REST API built upon the <a
              href="https://tools.ietf.org/html/rfc7231#section-4">HTTP</a> protocol. Below is an example endpoint
            applying <a href="https://roy.gbiv.com/pubs/dissertation/top.htm">REST</a> style for getting the same
            information for a specific space name:</p>
          <pre class="example" id="RESTApiCall">
                        <code class="lang-json">GET <span class="hljs-string">"https://server.example.com/api/locations?space=osh:Kitchen&amp;sensorType=brick:Zone_Air_Temperature_Sensor&amp;actuatorType=brick:Zone_Air_Temperature_Setpoint"</span>
                        API response:
                        {
                          <span class="hljs-string">"location"</span>: {
                            <span class="hljs-string">"site"</span>: {
                              <span class="hljs-string">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Site1</span>"</span>,
                              <span class="hljs-string">"name"</span>: <span class="hljs-string">"Site1"</span>
                            },
                            <span class="hljs-string">"building"</span>: {
                              <span class="hljs-string">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Building1</span>"</span>,
                              <span class="hljs-string">"name"</span>: <span class="hljs-string">"Building1"</span>
                            },
                            <span class="hljs-string">"zone"</span>: <span class="hljs-literal">null</span>,
                            <span class="hljs-string">"storey"</span>: {
                              <span class="hljs-string">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Level2</span>"</span>,
                              <span class="hljs-string">"name"</span>: <span class="hljs-string">"Level2"</span>
                            },
                            <span class="hljs-string">"space"</span>: {
                              <span class="hljs-string">"id"</span>: <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Kitchen</span>"</span>,
                              <span class="hljs-string">"name"</span>: <span class="hljs-string">"Kitchen"</span>
                            },
                          <span class="hljs-string">"sensors"</span>: [
                            <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Kitchen-temp-Sensor</span>"</span>
                          ],
                          <span class="hljs-string">"actuators"</span>: [
                            <span class="hljs-string">"https://w3id.org/ibp/osh/OpenSmartHomeDataSet<span class="hljs-subst">#Kitchen-tempS-Actuator</span>"</span>
                          ]
                        }
                        </code></pre>
          <dt><em>Automated Update of Fault Detection Rule based on Thing Description</em></dt>
          <p>Another related use case in smart buildings, which would greatly benefit from harmonised thing
            descriptions and attached location information is related to the detection of unexpected behavior,
            errors and faults. An example for such a detection of faults is the rule-based surveillance of sensor
            values. A generic rule applicable to sensors is that the observation values stay within the measurement
            range of the sensor. Again, in the case of maintenance as described above a sensor is replaced.</p>
          <p>Some agent configuring fault detection rules, can obtain the measurement range from the sensor&#39;s TD
            (see above) to obtain the parameters to configure the mentioned rule. Again, a query or API call
            retrieving this information (schema:minValue/ schema:maxValue) can be used to update the upper and lower
            bound of the values provided by the <a href="#Kitchen-temp-Sensor">sensor</a>.</p>

          </dd>
          <dt>Security Considerations</dt>
          <dd>

            <p>Security in smart buildings is of importance. In particular, access control needs to be properly secured.
              This applies also for data access which can be secured using existing security schemes (API Keys, OAuth2...).
              Moreover, from certain observations, e.g. electricity consumption, clues can be indirectly given such as presence in a
              home. Hence, security needs must be defined and properly addressed.</p>


          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            <p>Privacy considerations can be of a concern if observations of sensors can be matched to individuals.
              It is of the responsability of building owners, managers and users to define their own privacy policies
              for their data and share necessary consents if necessary.
            </p>

          </dd>
          <dt>Accessibility Considerations</dt>
          <dd>

            <p>Accessibility is a major concern in the buildings domain. Efforts exist in also providing accessibility
              data in a
              electronic format. The W3C LBD CG is in contact with the <a href="https://www.w3.org/community/lda/">W3C
                Linked Data for Accessbility Community Group</a>.
            </p>

          </dd>
          <dt>Internationalisation (i18n) Considerations</dt>
          <dd>

            <p>Internationalisation is a concern as the Buildings industry is a global industry. This is reflected in
              some efforts, e.g. BOT used in the examples above does provide multilanguage labels in up to 16
              different languages including english, french and chinese.</p>

          </dd>
          <dt>Requirements</dt>
          <dd>

            <p>None</p>

          </dd>
          <dt>Gaps</dt>
          <dd>

            <p>None</p>

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <ul>
              <li><a href="https://saref.etsi.org/saref4bldg/">SAREF4Bldg an ETSI Standard</a></li>
              <li><a href="https://www.w3.org/TR/vocab-ssn/">SOSA/SSN a W3C Recommendation</a></li>
              <li><a href="https://standards.buildingsmart.org/IFC/DEV/IFC4/ADD2/OWL/index.html">Industry Foundation
                  Classes (IFC) an
                  ISO standard</a></li>
              <li><a href="https://w3id.org/bot">Building Topology Ontology (BOT)</a></li>
              <li><a href="https://brickschema.org">BRICK</a></li>
            </ul>


          </dd>
          <dt>Comments</dt>
          <dd>


          </dd>
        </dl>
      </section>
      <section id="smart-building">
        <h2>Smart Building</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Sebastian Kaebisch (Siemens)

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Michael Lagally (Oracle)

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

          </dd>
          <dt>Motivation and Description</dt>
          <dd>

            Buildings such as office buildings, hotels, airports, hospitals, train stations and sports stadiums
            typically consist of heterogeneous IoT systems such as lightings, elevators, security (e.g., door
            control), air-conditionings, fire warnings, heatings, pools, parking control, etc.

            Monitoring, controlling, and management of such a heterogeneous IoT landscape is quite chellanging in
            terms of engeneering and maintenance.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            All kind of sensors and actuators (e.g., HVAC).

          </dd>
          <dt>Expected Users</dt>
          <dd>

            <ul>
              <li>systems engineers</li>
              <li>system administrators</li>
              <li>third party user</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Hetrogeneos data models from different IoT systems such as BACnet, KNX, and Modbus.

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            WoT Thing Description and Thing Model, WoT Architecture, WoT Binding Templates (covering protocol
            specifica)


          </dd>
          <dt>Existing standards</dt>
          <dd>

            BACnet, KNX, OPC-UA, Modbus

          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>

      <section id="connected-building-energy-efficiency">
        <h2>Connected Building Energy Efficiency</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Farshid Tavakolizadeh (Fraunhofer)

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Michael McCool (Intel)

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>directory service operator</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            Construction and renovation companies often deal with the challenge of delivering target energy-efficient
            buildings given specific budget and time constraints. Energy efficiency, as one of the key factors for
            renovation investments, depends on the availability of various data sources to support the renovation
            design and planning. These include climate data and building material along with residential comfort and
            energy consumption profiles. The profiles are created using a combination of manual inputs and sensory
            data collected from residents.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>Gateway (e.g. Single-board computer with a Z-Wave controller)</li>
            </ul>

            Z-wave Sensors:
            <ul>
              <li>Power Meter</li>
              <li>Gas Meter</li>
              <li>Smart Plug</li>
              <li>Heavy Duty Switch</li>
              <li>Door/Window Sensors</li>
              <li>CO2 Sensor</li>
              <li>Thermostat</li>
              <li>Multi-sensors (Motion, Temperature, Light, Humidity, Vibration, UV)</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>Ambient conditions</li>
              <li>Occupancy model</li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>


          </dd>
          <dt>Description</dt>
          <dd>

            Renovation of residential buildings to improve energy efficiency depend on a wide range of sensory
            information to understand the building conditions and consumption models. As part of the pre-renovation
            activities, the renovation companies deploy various sensors to collect relevant data over a period of
            time. Such sensors become part of a wireless sensor network (WSN) and expose data endpoint with the help
            of one or more gateway devices. Depending on the protocols, the endpoints require different interaction
            flows to securely access the current and historical measurements. The renovation applications need to
            discover the sensors, their endpoints and how to interact with them based on search criteria such as the
            physical location, mapping to the building model or measurement type.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Security Considerations</dt>
          <dd>


          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            The TD may expose personal information about the building layout and residents.

          </dd>
          <dt>Gaps</dt>
          <dd>

            There is no standard vocabulary for embedding application-specific meta data inside the TD. It is possible
            to extend the TD context and add additional fields but with too much flexibility, every application may
            end up with a completely different structure, making such information more difficult to discover. In this
            use-case, the application specific data are:
            <ul>
              <li>the mapping between each thing and the space in the building model</li>
              <li>various identifiers for each thing (e.g. sensor serial number, z-wave ID, SenML name)</li>
              <li>indoor coordinates</li>
            </ul>

            There is no standard API specification for the WoT Thing Directory to maintain and query TDs.

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <ul>
              <li><a href="https://www.ogc.org/standards/sensorthings">OGC SensorThings</a> model includes a
                `properties` property for each Thing which is a non-normative JSON Object for application-specific
                information (not to be confused with TD's `properties` which is a Map of instances of PropertyAffordance
              </li>
            </ul>

          </dd>
          <dt>Comments</dt>
          <dd>


          </dd>
        </dl>
      </section>
    </section>

    <section id="shared-devices">
      <h2>Shared Devices and resources</h2>
      <section id="education">
        <h2>Shared Devices</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Ege Korkan

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>

            <a href="https://github.com/w3c/wot-architecture/issues/496">496</a>

          </dd>
          <dt>Category</dt>
          <dd>

            Education

          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            For the education category:

            <ul>
              <li>device owners : The university -&gt; Research Group -&gt; Specific Lab</li>
              <li>device user : Students and potentially anyone who participates in plugfests</li>
              <li>service provider : The university -&gt; Research Group</li>
              <li>network operator : The university</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            This use case motivates a standardized use of shared resources. One example is when a physical resource of
            the Thing should not be used by multiple Consumers at the same time like the arm of the robot but its
            position can be read my multiple Consumers.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            Concrete devices are irrelevant for this use case but devices with a physical state is required. However,
            we have currently the following devices that are connected to Raspberry Pis where the WoT stack (node-wot
            or similar) is running. Concrete device models can be given upon request.

            <ul>
              <li>Robotic arms</li>
              <li>Conveyor belts</li>
              <li>Motorized sliders where the robots or devices can be mounted on</li>
              <li>Philips Hue devices: Light bulbs, LED Strips, Motion sensors, Switch. We do not have the source code
                of these devices (brownfield) </li>
              <li>Various sensors (brightness, humidity, temperature, gyroscopic sensors)</li>
              <li>LED Screen to display messages</li>
            </ul>

            There are also IP Cameras but they are not WoT compatible and are not planned to be made compatible.

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Atmospheric data of a room, machine sensors

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            Thing Description, Scripting API, possibly security

          </dd>
          <dt>Description</dt>
          <dd>

            We are offering a practical course for the students where they can interact fully remotely with WoT
            devices and verify their physical actions via video streams. We have sensors and actuators like robots.
            Students then build mashup applications to deepen their knowledge of WoT technologies. Official page of
            the course is <a
              href="https://campus.tum.de/tumonline/wbLv.wbShowLVDetail?pStpSpNr=950504601&pSpracheNr=1">here</a>.

          </dd>
          <dt>Variants</dt>
          <dd>


          </dd>
          <dt>Security Considerations</dt>
          <dd>

            The devices are connected to the Internet and are secured behind a router and proxy.

          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            none from the WoT point of view since we want the devices to be used by anyone and the devices do not
            share any information that is related to the students or us as the provider of the devices.
            However, there are cameras which can show humans entering the room as a side effect (they are meant to
            monitor the devices). The streams are accessible only to authorized users, the room has signs on the door
            and there is a cage around the area that is filmed.

          </dd>
          <dt>Gaps</dt>
          <dd>

            <p>Thing Description</p>

            <ul>
              <li>How to give hints that a particular action should not be used by others at the same time. A new
                keyword (like `"shared":true`) would be needed for devices that do not implement a describable
                mechanism.</li>
              <li>How to describe the mechanism that the Thing implements to manage the shared resources. Does it
                happen in the security level? </li>
            </ul>

            <p>Scripting API</p>

            <ul>
              <li>How does the Consumer code change when this mechanism is used. Does it get settled in the
                implementation or scripting level. </li>
            </ul>

          </dd>
          <dt>Existing standards</dt>
          <dd>


          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>
    </section>

    <section id="oauth2-flow">
      <h2>Oauth2 Flows</h2>
      <section id="oauth">
        <h2>OAuth2 Flows</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool, Cristiano Aguzzi

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>



          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>

            WIP

          </dd>
          <dt>Target Users</dt>
          <dd>

            <ul>
              <li>device owner</li>
              <li>device user</li>
              <li>device application</li>
              <li>service provider</li>
              <li>identity provider</li>
              <li>directory service</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>


            OAuth 2.0 is an authorization protocol widely known for its usage across several web services.
            It enables third-party applications to obtain limited access to HTTP services on behalf of the resource
            owner
            or of itself.
            The protocol defines the following actors:

            <ul>
              <li>Client: an application that wants to use a resource owned by the resource owner. </li>
              <li>Authorization Server: An intermediary that authorizes the client for a particular `scope`. </li>
              <li>Resource: a web resource </li>
              <li>Resource Server: the server where the resource is stored</li>
              <li>Resource Owner: the owner of a particular web resource. If it is a human is usually referred to as an
                end-user. More specifically from the RFC:</li>
              <ul>
                <li>An entity capable of granting access to a protected resource.</li>
              </ul>
            </ul>


            These actors can be mapped to WoT entities:
            <ul>
              <li>Client is a WoT Consumer</li>
              <li>Authorization Server is a third-party service</li>
              <li>Resource is an interaction affordance</li>
              <li>Resource Server is a Thing described by a Thing Description acting as a server. </li>
              May be a device or a service.
              <li>Resource Owner might be different in each use case.
                A Thing Description may also combine resources from different owners or web server.</li>
            </ul>
            TO DO: Check the OAuth 2.0 spec to determine exactly how Resource Owner is defined.
            Is it the actual owner of the resource (eg running the web server) or simply someone
            with the rights to access that resource?
            <br>
            The OAuth 2.0 protocol specifies an authorization layer that separates the client from the resource owner.
            The basic steps of this protocol are summarized in the following diagram:
            <pre>
     +--------+                               +---------------+
     |        |--(A)- Authorization Request -&gt;|   Resource    |
     |        |                               |     Owner     |
     |        |&lt;-(B)-- Authorization Grant ---|               |
     |        |                               +---------------+
     |        |
     |        |                               +---------------+
     |        |--(C)-- Authorization Grant --&gt;| Authorization |
     | Client |                               |     Server    |
     |        |&lt;-(D)----- Access Token -------|               |
     |        |                               +---------------+
     |        |
     |        |                               +---------------+
     |        |--(E)----- Access Token ------&gt;|    Resource   |
     |        |                               |     Server    |
     |        |&lt;-(F)--- Protected Resource ---|               |
     +--------+                               +---------------+
</pre>
            Steps A and B defines what is known as authorization grant type or flow.
            What is important to realize here is that not all of these interactions
            are meant to take place over a network protocol.
            In some cases,
            interaction with with a human through a user interface may be intended.

            OAuth2.0 defines 4 basic flows plus an extension mechanism.
            The most common of which are:
            <ul>
              <li>`code`</li>
              <li>`implicit`</li>
              <li>`password` (of resource owner)</li>
              <li>`client` (credentials of the client)</li>
            </ul>

            In addition, a particular extension which is of interest to IoT is the `device` flow.

            Further information about the OAuth 2.0 protocol can be found in
            <a href="https://tools.ietf.org/html/rfc6749#section-1">IETF RFC6749</a>.
            In addition to the flows, OAuth 2.0 also supports scopes.
            Scopes are identifiers which can be attached to tokens.
            These can be used to limit authorizations to
            particular roles or actions in an API.
            Each token carries a set of scopes and these can be checked when an interaction
            is attempted and access can be denied if the token does not include a scope
            required by the interaction.

            This document describes relevant use cases for each of the OAuth 2.0 authorization flows.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            To support OAuth 2.0, all devices must have the capability of:
            <ul>
              <li>Both the producer and consumer must be able to create and participate in a TLS connection.</li>
              <li>The producer must be able to verify an access (bearer) token (i.e. have sufficient computational
                power/connectivity). </li>
            </ul>

            Comment:
            <ul>
              <li>Investigate whether DTLS can be used.</li>
              Certainly the connection needs to be encrypted; this is required in the OAuth 2.0 specification.
              <li>Investigate whether protocols other than HTTP can be used, e.g. CoAP.</li>
              <ul>
                <li>found an interesting IETF draft RFC about CoAP support(encrypted using various mechanisms like DTLS
                  or
                  CBOR Object Signing and Encryption): <a
                    href="https://tools.ietf.org/html/draft-ietf-ace-oauth-authz-35">draft-ietf-ace-oauth</a></li>
              </ul>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            Depending on the OAuth 2.0 flow specified, various URLs and elements need to be specified,
            for example, the location of an authorization token server.
            OAuth 2.0 is also based on bearer tokens and so
            needs to include the same data as those, for example, expected encryption suite.
            Finally,
            OAuth 2.0 supports scopes so these need to be defined in the security scheme and specified in
            the form.

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            Thing Description, Scripting API, Discovery, and Security.

          </dd>
          <dt>Description</dt>
          <dd>

            A general use case for OAuth 2.0 is when a WoT consumer wants to access restricted interaction
            affordances.
            In particular, when those affordances have a specific resource owner which
            may grant some temporary permissions to the consumer.

            The WoT consumer can either be hosted in a remote device or interact directly with the end-user inside an
            application.

          </dd>
          <dt>Variants</dt>
          <dd>

            For each OAuth 2.0 flow, there is a corresponding use case variant.
            We also include the experimental "device" flow for consideration.

            <br>
            <br>
            code

            A natural application of this protocol is when the end-user wants to interact directly with the consumed
            thing or to grant his authorization to a remote device. In fact from the <a
              href="https://tools.ietf.org/html/rfc6749#section-4.1">RFC6749</a>

            <ul>
              <li>
                Since this is a redirection-based flow, the client must be capable of
                interacting with the resource owner's user-agent (typically a web
                browser) and capable of receiving incoming requests (via redirection)
                from the authorization server.
              </li>
            </ul>

            This implies that the code flow can be only used when the resource owner interacts directly with the WoT
            consumer at least once. Typical scenarios are:

            <ul>
              <li>In a home automation context, a device owner uses a third party software to interact
                with/orchestrate one or more devices</li>
              <li>Similarly, in a smart farm, the device owner might delegate its authorization to third party
                services.</li>
              <li>In a smart home scenario, Thing Description Directories might be deployed using this authorization
                mechanism. In particular, the list of the registered TDs might require an explicit read authorization
                request to the device owner (i.e. an human who has bought the device and installed it). </li>
              <li>... </li>
            </ul>

            The following diagram shows the steps of the protocol adapted to WoT idioms and entities. In this
            scenario, the WoT Consumer has read the Thing Description of a Remote Device and want to access one of its
            WoT Affordances protected with OAuth 2.0 code flow.
            <pre>
                                                 +-----------+
  +----------+                                   |           |
  | Resource |                                   |  Remote   |
  |   Owner  |                                   |  Device   +&lt;-------+
  |          |                                   |           |        |
  +----+-----+                                   +-----------+        |
       ^                                                              |
       |                                                              |
      (B)                                                             |
+------------+          Client Identifier      +---------------+      |
|           ------(A)-- & Redirection URI ----&gt;+               |      |
|   User-    |                                 | Authorization |      |
|   Agent   ------(B)-- User authenticates ---&gt;+     Server    |      |
|            |                                 |               |      |
|           ------(C)-- Authorization Code ---&lt;+               |      |
+---+----+---+                                 +---+------+----+      |
    |    |                                         ^      v           |
   (A)  (C)                                        |      |           |
    |    |                                         |      |           |
    ^    v                                         |      |           |
+---+----+---+                                     |      |           |
|            |&gt;-+(D)-- Authorization Code ---------'      |           |
|    WoT     |         & Redirection URI                  |           |
|  Consumer  |                                            |           |
|            |&lt;-+(E)----- Access Token -------------------'           |
+-----+------+      (w/ Optional Refresh Token)                       |
      v                                                               |
      |                                                               |
      +-----------(F)----- Access WoT --------------------------------+
                           Affordance
</pre>
            Notice that steps (A), (B) and (C) are broken in two parts as they pass through the User-Agent.


            <p>device</p>

            The device flow (IETF <a href="https://tools.ietf.org/html/rfc8628">RFC 8628</a>) is a variant of the code
            flow for browserless and
            input-constrained devices. Similarly, to its <i>parent</i> flow, it requires a close interaction between the
            resource owner and the WoT consumer. Therefore, the use cases for this flow are the same as the code
            authorization grant but restricted to all devices that do not have a rich means to interact with the
            resource owner. However, differently from `code`, RFC 8628 states explicitly that one of the actors of
            the protocol is an <b>end-user</b> interacting with a <b>browser</b> (even if <a
              href="https://tools.ietf.org/html/rfc8628#section-6.2">section-6.2</a>
            briefly describes an authentication using a companion app and BLE), as shown in the following (slightly
            adapted) diagram:

            <pre>
+----------+
|          |
|  Remote  |
|  Device  |
|          |
+----^-----+
     |
     | (G) Access WoT Affordance
     |
+----+-----+                                +----------------+
|          +&gt;---(A)-- Client Identifier ---v+                |
|          |                                |                |
|          +&lt;---(B)-- Device Code,      ---&lt;+                |
|          |          User Code,            |                |
|   WoT    |          & Verification URI    |                |
| Consumer |                                |                |
|          |  [polling]                     |                |
|          +&gt;---(E)-- Device Code       ---&gt;+                |
|          |          & Client Identifier   |                |
|          |                                |  Authorization |
|          +&lt;---(F)-- Access Token      ---&lt;+     Server     |
+-----+----+   (& Optional Refresh Token)   |                |
      v                                     |                |
      :                                     |                |
     (C) User Code & Verification URI       |                |
      :                                     |                |
      ^                                     |                |
+-----+----+                                |                |
| End User |                                |                |
|    at    +&lt;---(D)-- End user reviews  ---&gt;+                |
|  Browser |          authorization request |                |
+----------+                                +----------------+
</pre>
            Notable mentions:
            <ul>
              <li>the protocol is heavily end-user oriented. In fact, the RFC states the following</li>
              <ul>
                <li>Due to the polling nature of this protocol (as specified in Section 3.4), care is needed to avoid
                  overloading the capacity of the token endpoint. To avoid unneeded requests on the token endpoint, the
                  client SHOULD only commence a device authorization request when <b>prompted by the user and not
                    automatically</b>, such as when the app starts or when the previous authorization session expires or
                  failAs.</li>
              </ul>
              <li>TLS is required both between WoT Consumer/Authorization Server and between Browser/Authorization
                Server</li>
              <li>Other user interactions methods may be used but are left out of scope</li>
            </ul>


            <p>client credential</p>

            The Client Credentials grant type is used by clients to obtain an access token outside of the context of
            an end-user. From <a href="https://tools.ietf.org/html/rfc6749#section-4.4">RFC6749</a>:

            <ul>
              <li>The client can request an access token using only its client
                credentials (or other supported means of authentication) when the
                the client is requesting access to the protected resources under its
                control, or <b>those of another resource owner that has been previously
                  arranged with the authorization server</b> (the method of which is beyond
                the scope of this specification).</li>
            </ul>

            Therefore the client credential grant can be used:
            <ul>
              <li>When the resource owner is a public authority. For example, in a smart city context, the authority
                provides a web service where to register an application id.</li>
              <li>Companion application</li>
              <li>Industrial IoT. Consider a smart factory where the devices or services are provisioned with client
                credentials. </li>
              <li>...</li>
            </ul>

            The Client Credentials flow is illustrated in the following diagram. Notice how the Resource Owner is not
            present.

            <pre>
+----------+
|          |
|  Remote  |
|  Device  |
|          |
+----^-----+
     |
     |  (C) Access WoT Affordance
     ^
+----+-----+                                  +---------------+
|          |                                  |               |
|          +&gt;--(A)- Client Authentication ---&gt;+ Authorization |
|   WoT    |                                  |     Server    |
| Consumer +&lt;--(B)---- Access Token ---------&lt;+               |
|          |                                  |               |
|          |                                  +---------------+
+----------+
</pre>
            Comment: Usually client credentials are distributed using an external service which is used by humans to
            register a particular application. For example, the `npm` cli has a companion dashboard where a developer
            requests the generation of a token that is then passed to the cli. The token is used to verify the
            publishing process of `npm` packages in the registry. Further examples are Docker cli and OpenId Connect
            Client Credentials.

            <p>implicit</p>
            <b>Deprecated</b>
            From <a href="https://tools.ietf.org/html/draft-ietf-oauth-security-topics-15#section-2.1.2">OAuth 2.0
              Security Best Current Practice</a>:
            <ul>
              <li>
                In order to avoid these issues, clients SHOULD NOT use the implicit
                grant (response type "token") or other response types issuing access
                tokens in the authorization response, unless access token injection
                in the authorization, response is prevented and the aforementioned
                token leakage vectors are mitigated.
              </li>
            </ul>

            The RFC above suggests using `code` flow with Proof Key for Code Exchange (PKCE) instead.
            <br>
            The implicit flow was designed for public clients typically implemented inside a browser (i.e. javascript
            clients). As the `code` is a redirection-based flow and it requires direct interaction with the resource's
            owner user-agent. However, it requires one less step to obtain a token as it is returned directly in the
            authentication request (see the diagram below).
            <br>
            Considering the WoT context this flow is not particularly different from `code` grant and it can be used
            in the same scenarios.
            <br>
            Comment: even if the `implicit` flow is deprecated existing services may still using it.

            <pre>
+----------+
| Resource |
|  Owner   |
|          |
+----+-----+
     ^
     |
    (B)
+----------+          Client Identifier     +---------------+
|         ------(A)-- & Redirection URI ---&gt;+               |
|  User-   |                                | Authorization |
|  Agent  ------(B)-- User authenticates --&gt;+     Server    |
|          |                                |               |
|          +&lt;---(C)--- Redirection URI ----&lt;+               |
|          |          with Access Token     +---------------+
|          |            in Fragment
|          |                                +---------------+
|          +----(D)--- Redirection URI ----&gt;+   Web-Hosted  |
|          |          without Fragment      |     Client    |
|          |                                |    Resource   |
|     (F)  +&lt;---(E)------- Script ---------&lt;+               |
|          |                                +---------------+
+-+----+---+
  |    |
 (A)  (G) Access Token
  |    |
  ^    v
+-+----+---+                                   +----------+
|          |                                   |  Remote  |
|   WoT    +&gt;---------(H)--Access WoT---------&gt;+  Device  |
| Consumer |               Affordance          |          |
|          |                                   +----------+
+----------+

</pre>

            <p>resource owner password</p>
            <b>Deprecated</b> From <a
              href="https://tools.ietf.org/html/draft-ietf-oauth-security-topics-15#section-2.1.2">OAuth 2.0 Security
              Best Current Practice</a>:
            <ul>
              <li>The resource owner password credentials grant MUST NOT be used. This
                grant type insecurely exposes the credentials of the resource owner
                to the client. Even if the client is benign, this results in an
                increased attack surface (credentials can leak in more places than
                just the AS) and users are trained to enter their credentials in
                places other than the AS.</li>
            </ul>

            For completeness the diagram flow is reported below.

            <pre>
 +----------+
 | Resource |
 |  Owner   |
 |          |
 +----+-----+
      v
      |    Resource Owner
     (A) Password Credentials
      |
      v
+-----+----+                                  +---------------+
|          +&gt;--(B)---- Resource Owner -------&gt;+               |
|          |         Password Credentials     | Authorization |
|   WoT    |                                  |     Server    |
| Consumer +&lt;--(C)---- Access Token ---------&lt;+               |
|          |    (w/ Optional Refresh Token)   |               |
+-----+----+                                  +---------------+
      |
      | (D) Access WoT Affordance
      |
 +----v-----+
 |  Remote  |
 |  Device  |
 |          |
 +----------+
</pre>


          </dd>
          <dt>Security Considerations</dt>
          <dd>

            See OAuth 2.0 security considerations in <a
              href="https://tools.ietf.org/html/rfc6749#section-10">RFC6749</a>.
            See also <a href="https://tools.ietf.org/html/rfc8628#section-5">RFC 8628 section 5</a> for `device` flow.

          </dd>
          <dt>Privacy Considerations</dt>
          <dd>
            <section id="todo--privacy-x" class="ednote">TODO:
              Describe any issues related to privacy; if there are none, say "none" and justify
            </section>

          </dd>
          <dt>Gaps</dt>
          <dd>

            <section id="todo-gaps-x" class="ednote" TODO:>TODO: Describe any gaps that are
              not addressed in the current WoT standards and building blocks

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <section id="todo-references-x" class="ednote" TODO:>TODO: Provide links to relevant standards that are
              relevant for this use case</section>

          </dd>
          <dt>Comments</dt>
          <dd>
            Notice that the OAuth 2.0 protocol is not an authentication protocol, however <a
              href="https://openid.net/connect/">OpenID</a> defines an authentication layer on top of OAuth 2.0.
          </dd>
        </dl>
      </section>
    </section>

    <section id="lifecycle">
      <h2>Lifecycle</h2>
      <section id="device-lifecycle">
        <h2>Device Lifecycle</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            All WoT Arch participants

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>



          </dd>
          <dt>Category</dt>
          <dd>



          </dd>
          <dt>Class</dt>
          <dd>



          </dd>
          <dt>Status</dt>
          <dd>



          </dd>
          <dt>Target Users</dt>
          <dd>

            device manufacturer, gateway manufacturer, cloud provider

          </dd>
          <dt>Motivation</dt>
          <dd>

            Current spec does not address lifecycle.

          </dd>
          <dt>Expected Devices</dt>
          <dd>


          </dd>
          <dt>Expected Data</dt>
          <dd>


          </dd>
          <dt>Dependencies</dt>
          <dd>



          </dd>
          <dt>Description</dt>
          <dd>

            Handle the entire device lifecycle:
            Define terminology for lifecycle states and transitions.

            <p>Actors (represent a physical person or group of persons (company))</p>
            Manufacturer
            Service Provider
            Network Provider (potentially transparent for WoT use cases)
            Device Owner (User)
            Others?

            <p>Roles:</p>
            Depending on the use case, an actor can have multiple roles,
            e.g. security maintainer.
            Roles can be delegated.

          </dd>
          <dt>Variants</dt>
          <dd>

            There are (at least) two different entities to consider:
            <ul>
              <li>Things / Devices</li>
              <li>Consumers, e.g. cloud services or gateways</li>
            </ul>

            In more complex use cases there are additional entities:
            <ul>
              <li>Intermediates</li>
              <li>Directories</li>
            </ul>

          </dd>
          <dt>Gaps</dt>
          <dd>

            The current architecture spec does not describe device lifecycle in detail.
            A common lifecycle model helps to clarify terminology and structures the discussion
            in different groups.
            Interaction of a device with other entities such as directories may introduce
            additional states and transitions.

          </dd>
          <dt>Existing standards</dt>
          <dd>
            <ul>
              <li>WoT Security</li>
              <li>ETSI OneM2M</li>
              <li>OMA LwM2M</li>
              <li>OCF</li>
              <li>IEEE</li>
              <li>SIM cards / GSMA</li>
              <li>IETF</li>
              <li>Application Lifecycle (W3C Multimodal Interaction WG)</li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>
            All lifecycle contributions and discussion documents are available at:
            <a
              href="https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle">https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle</a>
            <br>
            <br>
            documents that were created / discussed in the architecture TF.
            <ul>
              <li>Lifecycle comparisons:
                <a
                  href="https://github.com/w3c/wot-architecture/blob/master/proposals/Device-lifecycle-comparisons.pdf">https://github.com/w3c/wot-architecture/blob/master/proposals/Device-lifecycle-comparisons.pdf</a>
              </li>
              <li>Lifecycle states:
                <a
                  href="https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/lifecycle-states.md">https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/lifecycle-states.md</a>
              </li>
              <li>Draft lifecycle diagram:
                <a
                  href="https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20lifecycle%20diagram-WoT%20new%20lifecycle.svg">https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20lifecycle%20diagram-WoT%20new%20lifecycle.svg</a>
              </li>
              <li>Layered lifecycle:
                <a
                  href="https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20layered%20%20lifecycle%20diagram-WoT%20new%20lifecycle.svg">https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/WoT%20layered%20%20lifecycle%20diagram-WoT%20new%20lifecycle.svg</a>
              </li>
              <li>System lifecycle:
                <a
                  href="https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/unified%20device%20lifecycle.svg">https://github.com/w3c/wot-architecture/blob/master/proposals/lifecycle/unified%20device%20lifecycle.svg</a>
              </li>
              <li>IoT Security Bootstrapping:
                <a
                  href="https://github.com/w3c/wot-security/blob/master/presentations/2020-03-16-Bootstrapping%20IoT%20Security%20-%20The%20IETF%20Anima%20and%20OPC-UA%20Recipes.pdf">https://github.com/w3c/wot-security/blob/master/presentations/2020-03-16-Bootstrapping%20IoT%20Security%20-%20The%20IETF%20Anima%20and%20OPC-UA%20Recipes.pdf</a>
              </li>
          </dd>
        </dl>
      </section>
    </section>

    <section id="Discovery">
      <h2>Discovery</h2>
      <section id="discovery">
        <h2>Discovery</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>


          </dd>
          <dt>Category</dt>
          <dd>


          </dd>
          <dt>Class</dt>
          <dd>


          </dd>
          <dt>Status</dt>
          <dd>


          </dd>
          <dt>Target Users</dt>
          <dd>

            All stakeholders:
            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>cloud provider</li>
              <li>service provider</li>
              <li>device manufacturer</li>
              <li>gateway manufacturer</li>
              <li>network operator (potentially transparent for WoT use cases)</li>
              <li>identity provider</li>
              <li>directory service operator</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            Discovery defines a distribution mechanism for the metadata contained in WoT Things Descriptions,
            and allows Things to advertise their capabilities and for potential consumers to find Things that
            match their needs. A standardized discovery mechanism is an enabler for convenient and ad-hoc
            orchestration of combinations of Things from different vendors while supporting appropriate security
            and privacy controls.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>Thing - any device or service that wishes to distribute (advertise) its metadata.</li>
              <li>Consumer - any device or service that wishes to find Things whose location and metadata satisfies
                specified constraints.</li>
              <li>Discovery Service - Mechanism by which metadata is distributed, which can involve a variety of
                services to handle spatial and semantic queries, register Thing Descriptions, provide access controls,
                etc.</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>Thing Descriptions - metadata describing a Thing</li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            <ul>
              <li>WoT Discovery</li>
            </ul>

            Note: this is a "horizontal" use case, and is driven by requirements in multiple verticals.

          </dd>
          <dt>Description</dt>
          <dd>

            A user wishing to build or instantiate an IoT service needs access to Thing Descriptions of installed and
            running
            devices satisfying specific requirements. These requirements can include being in or near a certain
            location,
            accessible using particular protocols or on a certain network,
            satisfying certain semantic categories, having certain capabilities, or having specific sub-APIs
            (interfaces).
            Discovery is the general process whereby WoT Thing Descriptions satisfying a specific set of such
            constraints are retreived by a running system.

          </dd>

          <dt>Variants</dt>
          <dd>
            <ul>
              <li>Run-time discovery allows late binding of orchestration services to particular devices and requires
                that
                consumers be able to adapt to Thing Descriptions discovered when a service is deployed.</li>
              <li>Development-time discovery may be useful during system development to build services that can
                interface to
                a particular class of Thing Descriptions. In this case what actually needs to be discovered Thing
                Models,
                not specific Thing Descriptions.</li>
            </ul>

          </dd>
          <dt>Security Considerations</dt>
          <dd>

            <ul>
              <li>The distribution mechanism needs to be able to clearly authenticate potential users.</li>
              <li>The distribution mechanism for metadata should only provide metadata to authorized users.</li>
              <li>The distribution mechanism should be able to resist denial-of-service attacks seeking to overwhelm it
                withi spurious requests.</li>
              <li>The distribution mechanism should be able to preserve the integrity of metadata.</li>
            </ul>

          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            <ul>
              <li>Metadata should only be distributed to appropriate sets of requesters, with the definition of
                "appropriate" configurable by the source of the metadata.</li>
              <li>Unauthorized users should not be able to access or infer information that they do not have access
                rights to.</li>
              <li>Providers of metadata should be able to withdraw metadata from distribution at any time.</li>
              <li>Metadata should not be retained indefinitely.</li>
            </ul>

          </dd>
          <dt>Gaps</dt>
          <dd>

            <ul>
              <li>The current WoT standards define a metadata format (the Thing Description) but not a means of
                distributing it.</li>
            </ul>

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>CoreRD</li>
              <li>DID</li>
            </ul>

          </dd>
          <dt>Comments</dt>
          <dd>

            <ul>
              <li>Many discovery mechansims already exist but many do not satisfy all the requirements above, e.g. they
                may have insufficient
                privacy controls. A standards solution that builds upon prior work in this area is desirable.</li>
          </dd>
        </dl>
      </section>
    </section>

    <section id="VR/AR">
      <h2>VR/AR</h2>

      <section id="ar-guide">
        <h2>AR Virtual Guide</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            <ul>
              <li>Rob Smith</li>
              <li>Kaz Ashimura</li>
            </ul>

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>
            <ul>
              <li>Michael McCool</li>
              <li>Christine Perey</li>
            </ul>
          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>
          </dd>
          <dt>Category</dt>
          <dd>

            vertical ( but quasi-horizontal from accessibility viewpoint)

          </dd>
          <dt>Class</dt>
          <dd>
          </dd>
          <dt>Status</dt>
          <dd>
          </dd>
          <dt>Target Users</dt>
          <dd>
            <ul>
              <li>device owners</li>
              <li>device user</li>
              <li>cloud provider</li>
              <li>service provider</li>
              <li>device manufacturer</li>
              <li>network operator (potentially transparent for WoT use cases)</li>
              <li>identity provider</li>
              <li>directory service operator</li>
            </ul>

          </dd>
          <dt>Motivation</dt>
          <dd>

            Using a wearable semi-transparent display,
            users can be guided by a virtual assistant through a physical area of interest with
            a rendered overlay to visualize events, annotate structures and other physical features,
            or visualize live and historical data associated with features of interest (which may or
            may not be at the same physical location as the sensor generating the data).
            An annotated map may provide additional geospatial guidance, including
            identification of landmarks, locations of devices. The system may also guide the
            user along a specific trajectory.

          </dd>
          <dt>Expected Devices</dt>
          <dd>

            <ul>
              <li>Wearable, semi-transparent head-mounted display</li>
              <li>Headphones for speakers for audio output</li>
              <li>Geopose and motion estimator (various technologies can be used)</li>
              <!-- Do we need this?  Its function is handled by the data processor
            <li>Recorder and Player to store and reproduce the scenes</li>
-->
              <li>Data processor to integrate all data (including live an historical data and geopose),
                generate annotations for the display, and record/play scenes</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>3D Position, orientation, velocity, and acceleration of the user</li>
              <li>Corresponding geolocation information (latitude, longitude, altitude) for all features of interest,
                including
                but not limited to physical landmarks, roads and paths, and locations of sensor's measurement points.
              </li>
              <li>Timestamps to allow synchronization between the annotations and data streams and the user's movement
              </li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>

            <ul>
              <li>WoT Thing Description</li>
              <li>WoT Binding Templates</li>
              <li>WoT Discovery</li>
              <li>Optional: WoT Scripting API accessible from application for interacting with devices.</li>
            </ul>

          </dd>
          <dt>Description</dt>
          <dd>

            <ul>
              <li>The user can travel around a real space with guidance from virtually defined geospatial
                data projected on a head-mounted wearable display synchronized with the view of the physical
                environment.</li>
              <li>The wearable display can generate position and orientation (geopose) data so that the user's
                movement will be traced through the physical environment and can synchronized with virtual features.
              </li>
              <li>The user can control the video images provided by the system, based
                sensors attached the display system or other means of control (gestures, voice input, etc.)</li>
              <li>The technology should include synchronization of playback of stored video media and related
                sensors, displays, and devices as well as the display of geolocation information from the virtual map.
              </li>
              <li>Discovery of sensors should take into account the position and field of view of the user
                so that data can be retreived only for the relevant features of interest.</li>
              <li>Discovery may additionally want to consider the motion (e.g. velocity) of the user to that
                data soon to come into view can be prefetched.</li>
              <li>Metadata for sensors needs to distinguish between the location of the device itself and the
                feature of interest it is measuring. For example, a camera might monitor traffic on a highway.
                The feature of interest is the location on the highway being monitored, while the location of the
                camera might be quite far away (e.g. mounted on top of a building).</li>
            </ul>

            See also the <a href="https://w3c.github.io/sdw/proposals/geotagging/webvmt/#virtualguide">Use Case
              description from the WebVMT Editor's draft</a>

          </dd>
          <dt>Variants</dt>
          <dd>
            <ul>
              <li>Two synchronized displays (for example, a phone and a headset) can offer greater insight and provide
                clearer
                guidance to the user by showing different views of the same location, e.g. a top-view map on the phone.
              </li>
              <li>A VR (virtual-only) implementation may also be used, with a rendered scene replacing the real scene.
                This may be applicable to contexts such as a Smart City dashboard where sensor information from data
                needs to be viewed in context without having to actually visit the site.</li>
              <li>The head-mounted semi-transparent display might be replaced in some contexts with a handheld display
                e.g. a phone or tablet. To be useful for AR however, such a device needs a back camera to simulate
                transparency
                and capture images of the real environment (optional for VR), and a way to determine its geolocation and
                orientation (geopose) relative to the environment.</li>
              <li>The head-mounted display may use a camera rather than being physically transparent.</li>
              <li>A microphone may be added for voice input, including voice commands. This avoid having to clutter the
                view with controls.</li>
              <li>A 3D camera (e.g. LIDAR) may be used to capture a view of the environment, which can be helpful to
                establish geopose and align annotations with real features of the environment.</li>
              <li>A virtual guide for a particular geographic location, e.g. a historical site, which visualises past
                events and buildings in AR, or allows remote users to explore in VR.</li>
              <li>A medical tool which allows a patient to describe their symptoms using AR, e.g. identify a painful
                area
                on their own body, which is also modelled as a 'map' to show internal features and display a treatment
                guide, including any WoT medical devices.</li>
              <li>A virtual controller for a city engineer to visualise utilities, e.g. electrical cables or water
                pipes,
                and control them. For example, a maintenance engineer could switch off an individual street lamp in
                order
                to replace the bulb using an AR menu displayed on that WoT-enabled lamppost.</li>
              <li>These mechanisms can also be used for video overlay in general. The technologies are related
                to the recording, playing, and distribution of video content when the data is stored.
                Playback of stored data and movements would be useful for simulation and debugging.</li>
            </ul>

          </dd>
          <dt>Security Considerations</dt>
          <dd>

            <ul>
              <li>If an AR systems is compromised it could be used to guide a user into a dangerous
                situation while hiding that fact from them, e.g. encouraging them to step over a drop.</li>
              <li>For the above reason the system should "fail gracefully" if there is any sign its integrity is
                compromised, and should implement mechanisms (e.g. signing) to detect tampering.
                Standards should be similar to other systems than can cause physical harm, e.g. automobiles.</li>
              <li>For a "simulated" transparent head-mounted display using a camera, the system should
                have a fail-safe supporting an unfiltered view, which should be automatic even if the processor
                crashes.</li>
              <li>For all systems the user should have a simple
                way (e.g. a single button push) of viewing "baseline reality".</li>
            </ul>

          </dd>
          <dt>Privacy Considerations</dt>
          <dd>

            <ul>
              <li>Systems that handle or display private data, e.g. medical applications, should respect the
                relevant regulations.</li>
              <li>Private data should not be retained by the device or used for purposes other than which it
                was provided. This includes the location of personal devices. To display information from another's
                personal device, permission needs to be explicity granted by that person and this permission should
                be time and possibly space-limited.
              </li>
            </ul>

          </dd>
          <dt>Requirements</dt>
          <dd>
            <ul>
              <li>Geospatially aware discovery mechanisms that can discover features of interest close to the
                user.</li>
              <li>Geospatial filters for discovery that include a pyramid-shaped region representing the field of view
                of the user.
                Note: a basic cylindrical, spherical, or rectangular filter region can be used instead and then the
                irrelevant results filtered out, but this is less efficient than the filter itself supporting
                field-of-view
                queries.
              </li>
              <li>Geospatial data associate with the metadata for devices. Note that mobile devices may update their
                position more rapidly than a discovery service may be able to support. In this case the discovery
                service
                needs to take the velocity and last known position of the data source into account and compute
                a zone of uncertainty and return the metadata for sources that might possibly be in the field of view.
                For sources such as this with dynamic positions, the AR system may also communicate with data sources
                directly to determine their most recent geolocation.
              </li>
            </ul>

          </dd>
          <dt>Gaps</dt>
          <dd>
            <ul>
              <li>Geospatial queries for discovery.</li>
              <li>Standardized encodings of geospatial metadata in TDs.</li>
            </ul>

          </dd>
          <dt>Existing standards</dt>
          <dd>

          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>
    </section>
    <section id="edge-computing">
      <h2>Edge Computing</h2>
      <section id="edge-computing">
        <h2>Edge Computing</h2>
        <dl>

          <dt>Submitter(s)</dt>
          <dd>

            Michael McCool

          </dd>
          <dt>Reviewer(s)</dt>
          <dd>

            Michael Lagally

          </dd>
          <dt>Tracker Issue ID</dt>
          <dd>
          </dd>
          <dt>Category</dt>
          <dd>
          </dd>
          <dt>Class</dt>
          <dd>
          </dd>
          <dt>Status</dt>
          <dd>
          </dd>
          <dt>Target Users</dt>
          <dd>
            Note: User should be "Stakeholder"
            <ul>
              <li>device owners - may benefit from using edge computing for iot orchestration and compute offload</li>
              <li>device user - may benefit from reduced cost of devices that can use compute offload</li>
              <li>cloud provider - may provide fallback for local edge compute services</li>
              <li>service provider - may provide edge computing service</li>
              <li>device manufacturer - may lower cost of device by depending on compute offload</li>
              <li>gateway manufacturer - may provide edge computing host hardware</li>
              <li>network operator - may provide edge computing nodes</li>
              <li>directory service operator - provides means to discover edge computing nodes</li>
            </ul>
          </dd>
          <dt>Motivation</dt>
          <dd>

            <ul>
              <li>IoT devices are often designed to be inexpensive (so they can be used at scale),
                small (for ease of installation) and are often power-limited, for example needing to
                run off a battery. For all these reasons, they usually have severely limited on-board
                computational capabilities.</li>
              <li>For applications that require significant computation and/or memory, for example
                computer vision, machine learning, or autonomous navigation, offloading work to
                another computer on the network may be advantageous.</li>
              <li>Offloading to the cloud typically involves relatively long latencies and may also
                have privacy implications. Edge computing implies offloading to a more "local" compute
                node with lower latency and optionally under more direct control of the user (improving
                privacy). This can be important for control applications (eg in robotics), computer
                graphics (eg gaming) and for applications processing imagery (eg facial recognition).</li>
              <li>An edge computer is also a convenient place to run persistent computations such as
                IoT orchestration rules that need to be "always on". Such an IoT orchestration system,
                in addition to needing to read from sensors and send commands to actuators over the
                network, may also invoke computationally-intensive services (eg image recognition). An
                example would be a security system that when a motion sensor is tripped, runs a person
                detection computation, and if a person is detected when and where they should not be,
                sounds an alarm. The motion sensor and alarm can be IoT devices while the person
                detection is a computationally-intensive service.</li>
            </ul>

          </dd>
          <dt>Expected Devices</dt>
          <dd>
            <ul>
              <li>IoT devices with Thing Descriptions for use in IoT orchestrations.</li>
              <li>An edge computer providing one or more fixed or generic compute services.</li>
              <li>A directory or other discovery mechanism that allows IoT devices and edge computers to advertize their
                availability.</li>
            </ul>

          </dd>
          <dt>Expected Data</dt>
          <dd>

            <ul>
              <li>Thing descriptions for IoT devices</li>
              <li>Thing descriptions for compute services</li>
              <li>Compute service configurations, e.g container images, WASM code, scripts, ONNX files, etc.</li>
            </ul>

          </dd>
          <dt>Dependencies - Affected WoT deliverables and/or work items</dt>
          <dd>
            <ul>
              <li>WoT Discovery - needs to be designed to support services, not just physical devices.</li>
              <li>WoT Architecture - concept of Thing needs to be expanded to include computational services.</li>
              <li>WoT Scripting API - essential for programming IoT orchestrations.</li>
            </ul>
          </dd>
          <dt>Description</dt>
          <dd>

            The WoT architecture can provide an interesting approach to edge computing:
            <ul>
              <li>An IoT orchestration running in an edge computer can consume WoT Thing Descriptions
                in order to determine how to connect to IoT devices.</li>
              <li>Fixed services (e.g. person detection) and generic compute nodes (a service that would
                allow an arbitrary computation to be loaded onto it) can also advertise themselves using
                Thing Descriptions, allowing an IoT orchestrator to interface to devices and services
                in a uniform way. This also facilitates support for "virtual devices", e.g. using
                computer vision, audio recognition, or other forms of analytics in place of a physical sensor.</li>
              <li>WoT discovery can be used to find appropriate compute services for IoT devices to offload
                computationally demanding tasks to, assuming those services describe themselves with
                TDs and advertise their availability via WoT discovery mechanisms.</li>
            </ul>
          </dd>

          <dt>Variants</dt>

          <dd>
            <ul>
              <li>An edge computer can provide facilities either for general-purpose computation (e.g.
                loading and running a container image, script, etc.) or special-purpose fixed computations
                (e.g. object detection and tracking, person detection, etc.). General-purpose computation
                is more powerful but also is more difficult to make fully secure.</li>
              <li>An edge computation can be stateless (function as a service, FaaS) or stateful. It is easier
                to migrate stateless computations transparently to new compute hardware but state then
                needs to be provided by a separate service, e.g. a database, and it is harder to program.</li>
              <li>Edge computers may provide just IoT orchestration without significant computational
                ability, just compute offload, or both. Many more use cases can be unlocked by providing both.</li>
              <li>Persistent computation can be provided in various ways. Rather than actually running
                continuously, an edge computation might be event-driven, for example.</li>
              <li>Under discussion are various ways to integrate edge computation with the web execution environment,
                for example by extending web and service workers.</li>
            </ul>
          </dd>
          <dt>Security Considerations</dt>

          <dd>

            Edge compute services supporting the specification of generic computation has many security
            challenges. In addition to the challenges common to cloud computing, e.g. protecting "tenants"
            from seeing each other's activity, additional challenges arise if the edge computer is offering
            computation as an ad-hoc service. For example, there needs to be a way to project the edge
            computer from denial-of-service attacks.

            An edge computer may also need to be protected from physical attacks. There is also the
            possibility that an edge computer might be physically compromised so approaches such as
            isolated containers (protecting the contents from the edge computer's hypervisor), and/or
            validated boot, might be necessary in some circumstances.

          </dd>
          <dt>Privacy Considerations</dt>

          <dd>

            Edge computers can theoretically improve privacy since sensitive data can be processed "locally"
            without having to be transmitted to a remote site. This however is tempered by edge computer's
            greater vulnerability to physical attacks. To avoid offloading work to a malicious edge computer,
            some means of evaluating the trustworthiness of edge computers is needed.
          </dd>
          <dt>Gaps</dt>

          <dd>

            <ul>
              <li>Explicit support for WoT Things that are services.</li>
              <li>Sufficient abstraction capability (eg "interfaces") to support virtual devices.</li>
              <li>A mechanism to package and install edge computations that can use the WoT scripting API for
                orchestration.
              </li>
              <li>A general means to manage compute nodes to provide offload targets (eg a standardized
                TD template for compute services).</li>
            </ul>

          </dd>
          <dt>Existing standards</dt>
          <dd>

            <ul>
              <li><a href="https://tools.ietf.org/html/draft-hong-t2trg-iot-edge-computing-04">IoT Edge Challenges and
                  Functions</a>, IETF Internet-Draft, Network Working Group, version 04, expires 26 November 2020.</li>
              <li><a href="https://www.iiconsortium.org/fog-and-edge-white-papers.htm">IIC/OpenFog</a>, Fog and Edge
                Computing White Papers</li>
              <li><a href="https://www.etsi.org/technologies/multi-access-edge-computing">ETSI MEC</a>, ETSI standards
                for
                Multiaccess Edge Computing </li>
            </ul>
          </dd>
          <dt>Comments</dt>
          <dd>
          </dd>
        </dl>
      </section>
    </section>

  </section>

  <!-- REQUIREMENTS -->



  <section id="sec-requirements">
    <h1>Requirements</h1>

    <section id="sec-functional-requirement">
      <h2>Functional Requirements</h2>
      <p>This section defines the properties required in an
        abstract Web of Things (WoT) architecture.</p>

      <section class="ednote">
        <h2>TODO: New requirements from new use cases need to be added here.
          Add a link to the requirements section in the github repo.
        </h2>
      </section>
      <section id="sec-requirements-principles">
        <h3>Common Principles</h3>
        <ul>
          <li>WoT architecture should enable mutual
            interworking of different eco-systems using web
            technology.</li>
          <li>WoT architecture should be based on the web
            architecture using RESTful APIs.</li>
          <li>WoT architecture should allow to use
            multiple payload formats which are commonly used
            in the web.</li>
          <li>WoT architecture must enable different device
            architectures and must not force a client or server
            implementation of system components.</li>
          <li>Flexibility
            <p>There are a wide variety of physical
              device configurations for WoT
              implementations. The WoT abstract
              architecture should be able to be mapped to
              and cover all of the variations.</p>
          </li>
          <li>Compatibility
            <p>There are already many existing IoT
              solutions and ongoing IoT standardization
              activities in many business fields. The WoT
              should provide a bridge between these
              existing and developing IoT solutions and
              Web technology based on WoT concepts. The
              WoT should be upwards compatible with
              existing IoT solutions and current
              standards.</p>
          </li>
          <li>Scalability
            <p>WoT must be able to scale for IoT
              solutions that incorporate thousands to
              millions of devices. These devices may offer
              the same capabilities even though they are
              created by different manufacturers.</p>
          </li>
          <li>Interoperability
            <p>WoT must provide interoperability across
              device and cloud manufacturers. It must be
              possible to take a WoT enabled device and
              connect it with a cloud service from
              different manufacturers out of the box.</p>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-thing-functionalities">
        <h3>Thing Functionalities</h3>
        <ul>
          <li>WoT architecture should allow things to
            have functionalities such as
            <ul>
              <li>reading thing's status information</li>
              <li>updating thing's status information
                which might cause actuation</li>
              <li>subscribing to, receiving and
                unsubscribing to notifications of
                changes of the thing's status
                information.</li>
              <li>invoking functions with input and
                output parameters which would cause
                certain actuation or calculation.</li>
              <li>subscribing to, receiving and
                unsubscribing to event notifications
                that are more general than just reports
                of state transitions.
              </li>
            </ul>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-search-and-discovery">
        <h3>Search and Discovery</h3>
        <ul>
          <li>WoT architecture should allow clients to
            know thing's attributes, functionalities and
            their access points, prior to access to the
            thing itself.</li>
          <li>WoT architecture should allow clients to
            search things by its attributes and
            functionalities.</li>
          <li>WoT architecture should allow semantic
            search of things providing required
            functionalities based on a unified vocabulary,
            regardless of naming of the functionalities.</li>
        </ul>
      </section>
      <section id="sec-requirements-description-mechanism">
        <h3>Description Mechanism</h3>
        <ul>
          <li>WoT architecture should support a common
            description mechanism which enables describing
            things and their functions.</li>
          <li>Such descriptions should be not only
            human-readable, but also machine-readable.</li>
          <li>Such descriptions should allow semantic
            annotation of its structure and described
            contents.</li>
          <li>Such description should be able to be
            exchanged using multiple formats which are
            commonly used in the web.</li>
        </ul>
      </section>
      <section id="sec-requirements-description-of-attributes">
        <h3>Description of Attributes</h3>
        <ul>
          <li>WoT architecture should allow describing
            thing's attributes such as
            <ul>
              <li>name</li>
              <li>explanation</li>
              <li>version of spec, format and
                description itself</li>
              <li>links to other related things and
                metadata information</li>
            </ul>
          </li>
          <li>Such descriptions should support
            internationalization.</li>
        </ul>
      </section>
      <section id="sec-requirements-description-of-functionalities">
        <h3>Description of Functionalities</h3>
        <ul>
          <li>WoT architecture should allow describing
            thing's functionalities which is shown in <a href="#sec-requirements-thing-functionalities"></a>
          </li>
        </ul>
      </section>
      <section id="sec-requirements-network">
        <h3>Network</h3>
        <ul>
          <li>WoT architecture should support multiple
            web protocols which are commonly used.</li>
          <li>Such protocols include
            <ol>
              <li>protocols commonly used in the
                internet and</li>
              <li>protocols commonly used in the
                local area network</li>
            </ol>
          </li>
          <li>WoT architecture should allow using
            multiple web protocols to access to the same
            functionality.</li>
          <li>WoT architecture should allow using a
            combination of multiple protocols to the
            functionalities of the same thing (e.g., HTTP and
            WebSocket).</li>
        </ul>
      </section>
      <section id="sec-requirements-deployment">
        <h3>Deployment</h3>
        <ul>
          <li>WoT architecture should support a wide
            variety of thing capabilities such as edge
            devices with resource restrictions and virtual
            things on the cloud, based on the same model.</li>
          <li>WoT architecture should support multiple
            levels of thing hierarchy with intermediate
            entities such as gateways and proxies.</li>
          <li>WoT architecture should support accessing
            things in the local network from the outside of
            the local network (the internet or another local
            network), considering network address
            translation.</li>
        </ul>
      </section>
      <section id="sec-requirements-application">
        <h3>Application</h3>
        <ul>
          <li>WoT architecture should allow describing
            applications for a wide variety of things such
            as edge device, gateway, cloud and UI/UX device,
            using web standard technology based on the same
            model.</li>
        </ul>
      </section>
      <section id="sec-requirements-legacy-adoption">
        <h3>Legacy Adoption</h3>
        <ul>
          <li>WoT architecture should allow mapping of
            legacy IP and non-IP protocols to web protocols,
            supporting various topologies, where
            such legacy protocols are terminated and
            translated.
          </li>
          <li>WoT architecture should allow transparent
            use of existing IP protocols without
            translation, which follow RESTful architecture.</li>

          <li>WoT architecture must not enforce client or server
            roles on devices and services.
            An IoT device can be either a client or a server,
            or both, depending on the system architecture;
            the same is true of edge and cloud services.
          </li>
        </ul>
      </section>
    </section>

    <section id="non-functionals">
      <h2>Non-Functional Requirements</h2>
      <section id="security-privacy">
        <h2>Security and Privacy</h2>
      </section>
    </section>

    <section id="sec-technical-requirements">
      <h2>Technical Requirements</h2>
      <section class="ednote">
        <h2>TODO: New requirements from new use cases need to be added here.</h2>
      </section>
      <p>
        The W3C WoT Thing Architecture [[wot-architecture]] defines the
        abstract architecture of Web of Things and illustrates it with various
        system topologies. This section describes
        technical requirements derived from the abstract
        architecture.
      </p>

      <section>
        <h3>Components in the Web of Things and the Web
          of Things Architecture</h3>
        <p>The use cases help to identify basic components
          such as devices and applications, that access and
          control those devices, proxies (i.e., gateways and
          edge devices) that are located between devices.
          An additional component useful in some use cases
          is the directory, which assists with discovery.</p>
        <p>Those components are connected to the internet or
          field networks in offices, factories or other
          facilities. Note that all components involved may be
          connected to a single network in some cases,
          however, in general components can be deployed
          across multiple networks.</p>
      </section>
      <section>
        <h3>Devices</h3>
        <p>
          Access to devices is made using a description of
          their functions and interfaces. This description is
          called <em>Thing Description (TD)</em>. A <em>Thing
            Description</em> includes a general metadata about
          the device, information models representing
          functions, transport protocol description for
          operating on information models, and security
          information.
        </p>
        <p>General metadata contains device identifiers
          (URI), device information such as serial number,
          production date, location and other human readable
          information.</p>
        <p>Information models defines device attributes, and
          represent device‚Äôs internal settings, control
          functionality and notification functionality.
          Devices that have the same functionality have the
          same information model regardless of the transport
          protocols used.</p>
        <p>Because many systems based on Web of Things
          architecture are crossing system Domains,
          vocabularies and meta data (e.g., ontologies) used in
          information models should be commonly understood by
          involved parties. In addition to REST transports,
          PubSub transports are also supported.</p>
        <p>Security information includes descriptions about
          authentication, authorization and secure
          communications. Devices are required to put TDs
          either inside them or at locations external to the
          devices, and to make TDs accessible so that other
          components can find and access them.</p>
      </section>
      <section>
        <h3>Applications</h3>
        <p>Applications need to be able to generate and use network
          and program interfaces based on metadata (descriptions).</p>
        <p>Applications have to be able to obtain these
          descriptions through the network,
          therefore, need to be able to conduct search
          operations and acquire the necessary descriptions over the
          network.</p>
      </section>
      <section>
        <h3>Digital Twins</h3>
        <p>Digital Twins need to generate program interfaces
          internally based on metadata (descriptions),
          and to represent virtual devices by using those
          program interfaces. A twin has to produce a description for
          the virtual device and make it externally available.</p>
        <p>Identifiers of virtual devices need to be newly
          assigned, therefore, are different from the original
          devices. This makes sure that virtual devices and
          the original devices are clearly recognized as
          separate entities. Transport and security mechanisms
          and settings of the virtual devices can be different
          from original devices if necessary. Virtual devices
          are required to have descriptions provided either directly by
          the twin or to have them available at external
          locations. In either case it is required to make the
          descriptions available so that other components can find and
          use the devices associated with them.</p>
      </section>
      <section>
        <h3>Discovery</h3>
        <p>For TDs of devices and virtual devices to be
          accessible from devices, applications and twins,
          there needs to be a common way to share TDs.
          Directories can serve this requirement by providing
          functionalities to allow devices and twins
          themselves automatically or the users to manually
          register the descriptions.</p>
        <p>Descriptions of the devices and virtual devices
          need to be searchable by external entities.
          Directories have to be able to process search
          operations with search keys such as keywords from
          the general description in the device description or
          information models.</p>
      </section>
      <section>
        <h3>Security</h3>
        <p>Security information related to devices and
          virtual devices needs to be described in device
          descriptions. This includes information for
          authentication/authorization and payload
          encryption.</p>
        <p>WoT architecture should support multiple security
          mechanism commonly used in the web, such as Basic,
          Digest, Bearer and OAuth2.0.</p>
      </section>
      <section>
        <h3>Accessibility</h3>
        <p>The Web of Things primarily targets
          machine-to-machine communication. The humans
          involved are usually developers that integrate
          Things into applications. End-users will be faced
          with the front-ends of the applications or the
          physical user interfaces provided by devices
          themselves. Both are out of scope of the W3C WoT
          specifications. Given the focus on IoT instead of
          users, accessibility is not a direct requirement,
          and hence is not addressed within this specification.</p>
        <p>There is, however, an interesting aspect on
          accessibility: Fulfilling the requirements above
          enables machines to understand the network-facing
          API of devices. This can be utilized by
          accessibility tools to provide user interfaces of
          different modality, thereby removing barriers to
          using physical devices and IoT-related applications.</p>
      </section>
    </section>

    <section id="sec-building-block-requirements">
      <h2>Requirements for individual WoT Building Blocks</h2>
      <section id="arch">
        <h2>Architecture</h2>
      </section>
      <section id="td">
        <h2>Thing Description</h2>
      </section>
      <section id="profiles">
        <h2>Profile</h2>
      </section>
      <section id="binding-templates">
        <h2>Binding Templates</h2>
      </section>
      <section id="scripting">
        <h2>Scripting</h2>
      </section>
      <section id="discovery">
        <h2>Discovery</h2>
      </section>
      <section id="new-building-blocks">
        <h2>New Building Blocks</h2>
      </section>
    </section>
  </section>

  <!-- Acknowledgements -->

  <section id="acknowledgements" class="appendix normative">
    <h1>Acknowledgments</h1>

    <section class="ednote" TODO:>
      List all authors of contributions.
    </section>
    <p>Special thanks to all authors of use case descriptions
      <!-- Michael McCool, Takuki Kamiya, Kazuyuki Ashimura,
        Sebastian K√§bisch, Zoltan Kis, ... --> for their contributions to this document.
    </p>
    <p>
      Many thanks to the W3C staff and all other active Participants of the W3C Web
      of Things Interest Group (WoT IG) and Working Group (WoT WG) for their
      support, technical input and suggestions that led to improvements to
      this document.
    </p>
  </section>

  <!--
    <section id="references" class="appendix">
      <h2 id="a-references"><bdi class="secno">A.</bdi>
        References<a class="self-link" aria-label="¬ß" href="#references"></a></h2>
      <section id="informative-references">
        <h3 id="a-1-informative-references"><bdi class="secno">A.1</bdi> Informative references<a class="self-link"
            aria-label="¬ß" href="#informative-references"></a></h3>
        <dl class="bibliography">
          <dt id="bib-hybridcast">[Hybridcast]</dt>
          <dd>
            Hybridcast and Hybridcast Connect: a Japanese Integrated Broadcast-Broadband system, HbbTV, ATSC 3.0,
            ...etc.
          </dd>
          <dt id="bib-nmea">[NMEA]</dt>
          <dd>
            defines sentences from GPS devices
          </dd>
          <dt id="bib-wgs84">[WGS84]</dt>
          <dd>
            https://en.wikipedia.org/wiki/World_Geodetic_System
            * World Geodetic System * Defines lat/long/alt coordinate system used by most other geolocation standards
            * More complicated than you would think (need to deal with deviations of Earth from a true sphere,
            gravitational irregularities, position of centroid, etc. etc.)
          </dd>
          <dt id="bib-geovoc">[Basic Geo Vocabulary]</dt>
          <dd>
            https://www.w3.org/2003/01/geo/
            * Very basic RDF definitions for lat, long, and alt
            * Does not define heading or speed
            * Does not define accuracy
            * Does not define timestamps
            * Uses string as a data model (rather than a number)
          </dd>
          <dt id="bib-geoapi">[W3C Geolocalization API]</dt>
          <dd>
            https://www.w3.org/TR/geolocation-API/
            * W3C Devices and Sensors WG is now handling
            * There is an updated proposal: https://w3c.github.io/geolocation-sensor/#geolocationsensor-interface
            * Data schema of updated proposal is similar to existing API, but all elements are now optional
            * Data includes latitude, longitude, altitude, heading, and speed
            * Accuracy is included for latitude/longitude (single number in meters, 95% confidence,
            interpretation a little ambiguous, but probably intended to be a radius) and altitude, but not for heading
            or speed.
          </dd>
          <dt id="bib-geocons">[Open Geospatial Consortium]</dt>
          <dd>
            * See http://docs.opengeospatial.org/as/18-005r4/18-005r4.html * Referring to locations by coordinates
            * Has standards defining semantics for identifying locations * Useful for mapping
          </dd>
          <dt id="bib-iso19111">[ISO19111]</dt>
          <dd>
            * ISO19111: https://www.iso.org/standard/74039.html * Standard for referring to locations by coordinates
            * Related to OGS standard above and WGS84 * Various other standards that relate to remote sensing,
            geolocation, etc.
            * Here is an example (see references): https://www.iso.org/obp/ui/fr/#iso:std:iso:ts:19159:-2:ed-1:v1:en
          </dd>
          <dt id="bib-ssn">[SSN]</dt>
          <dd>
            https://www.w3.org/TR/vocab-ssn/
            * Defines "accuracy": https://www.w3.org/TR/vocab-ssn/#SSNSYSTEMAccuracy
            * Definition of accuracy is consistent with how it is used in Web Geolocation API
            * Also defines related terms Precision, Resolution, Latency, Drift, etc.
          </dd>
          <dt id="bib-timestamp">[Timestamps]</dt>
          <dd>
            * W3C standard in proposed new web geolocation API: https://w3c.github.io/hr-time/#dom-domhighrestimestamp
            * See also related issues such as latency defined in SSN
          </dd>
          <dt id="bib-mmiuc31">[MMI UC3.1]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-mmiuc32">[MMI UC3.2]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-ice-f2761">[ICE F2761-09(2013)]</dt>
          <dd>
            Medical Devices and Medical Systems - Essential safety requirements for equipment comprising the
            patient-centric
            integrated clinical environment (ICE) - Part 1: General requirements and conceptual model.
            The idea behind ICE is to allow medical devices that conform to the ICE standard, either natively or using
            an adapter,
            to interoperate with other ICE-compliant devices regardless of manufacturer.
          </dd>
          <dt id="bib-openice">[OpenICE]</dt>
          <dd>
            OpenICE is an initiative to create a community implementation of F2761-09
            (ICE - Integrated Clinical Environment) based on DDS (https://www.omg.org/spec/DDS/About-DDS/).
          </dd>
          <dt id="bib-mdira">[MDIRA]</dt>
          <dd>
            https://secwww.jhuapl.edu/mdira/documents
            MDIRA Version 1.0 provides requirements and implementation guidance for MDIRA-compliant systems focused on
            trauma
            and critical care in austere environments. Johns Hopkins University Applied Physics Laboratory (JHU-APL)
            lead a research project
            in collaboration with US military to develop a framework of autonomous / closed loop prototypes for
            military health care which
            are dual use for the civilian healthcare system.
          </dd>
          <dt id="bib-mqtt">[MQTT]</dt>
          <dd>
            <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">
              <cite>MQTT Version 3.1.1 Plus Errata 01</cite></a>.
            Andrew Banks; Rahul Gupta. OASIS Standard. December 2015.
            URL: <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">
              http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html</a>
          </dd>
          <dt id="bib-opcua">[OPC UA]</dt>
          <dd>
            description opc ua
          </dd>
          <dt id="bib-bacnet">[BACnet]</dt>
          <dd>
            description BACnet
          </dd>
          <dt id="bib-coap">[CoAP]</dt>
          <dd>
            description CoAP
          </dd>
          <dt id="bib-mmiuc51">[MMI UC5.1]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-mmiuc52">[MMI UC5.2]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-mmiuc11">[MMI UC1.1]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-mmiuc12">[MMI UC1.2]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-mmiuc21">[MMI UC2.1]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-iec61850">[IEC 61850]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-ieee1574">[IEEE 1574]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-knx">[KNX]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-modbus">[Modbus]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-ogc">[OGC Sensor Things]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-onem2m">[OneM2M]</dt>
          <dd>
            no description
          </dd>
          <dt id="bib-lwm2m">[LWM2M]</dt>
          <dd>
            <a
              href="http://openmobilealliance.org/release/LightweightM2M/V1_1-20180710-A/OMA-TS-LightweightM2M_Core-V1_1-20180710-A.pdf">
              <cite>Lightweight Machine to Machine Technical
                Specification: Core</cite></a>. OMA SpecWorks. August
            2018. Approved Version: 1.1. URL: <a
              href="http://openmobilealliance.org/release/LightweightM2M/V1_1-20180710-A/OMA-TS-LightweightM2M_Core-V1_1-20180710-A.pdf">
              http://openmobilealliance.org/release/LightweightM2M/V1_1-20180710-A/OMA-TS-LightweightM2M_Core-V1_1-20180710-A.pdf</a>
          </dd>
          <dt id="bib-ocf">[OCF]</dt>
          <dd>
            <a href="https://openconnectivity.org/developer/specifications"><cite>
                OCF Core Specification</cite></a>. Open Connectivity
            Foundation. April 2019. Version 2.0.2. URL: <a
              href="https://openconnectivity.org/developer/specifications">https://openconnectivity.org/developer/specifications</a>
          </dd>
        </dl>
      </section>
    </section>
  -->



  <!--
Domains from the 1.0 UC document

                <section id="domain-home_automation">
                        <h3>Domain: home automation</h3>
                        <section id="building-automation">
                        </section>
                        <section id="adaptive-building">
                        </section>
                        <section id="washing-machine">
                        </section>
                        <section id="integrated-home-automation">
                        </section>
                        <section id="Smart_home">
                        </section>
                        <section id="intelligent-hotel-room">
                        </section>
                </section>
                <section id="domain-logistics">
                        <h3>Domain: logistics</h3>
                </section>
                <section id="domain-manufacturing">
                        <h3>Domain: manufacturing</h3>
                        <section id="shift-of-function">
                        </section>
                        <section id="automation-factories">
                        </section>
                        <section id="smart-maintenance">
                        </section>
                        <section id="lifecycle-management">
                        </section>
                </section>
                <section id="domain-smart_grids">
                        <h3>Domain: smart grids</h3>
                        <section id="vpp">
                        </section>
                </section>
                <section id="domain-transportation">
                        <h3>Domain: transportation</h3>
                        <section id="emobility">
                        </section>
                </section>
                <section id="domain-smart_cities">
                        <h3>Domain: smart cities</h3>
                        <section id="flood-monitoring">
                        </section>
                </section>
                <section id="domain-healthcare_and_medical">
                        <h3>Domain: healthcare and medical</h3>
                        <section id="health-monitoring">
                        </section>
                        <section id="ecg">
                        </section>
                </section>
                <section id="cross-domain">
                        <h3>Domain: Cross Domain</h3>
                </section>
                <section id="domain-other">
                        <h3>Domain: other</h3>
                        <section id="device-management">
                        </section>

                        -->



  </section>
  </section>


</body>

</html>
